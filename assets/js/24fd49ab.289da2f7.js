"use strict";(self.webpackChunkdocusaurus=self.webpackChunkdocusaurus||[]).push([[187],{2533:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>o,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"module4/llm_cognitive_planning","title":"Cognitive Planning using Large Language Models (LLMs)","description":"Introduction","source":"@site/docs/module4/llm_cognitive_planning.md","sourceDirName":"module4","slug":"/module4/llm_cognitive_planning","permalink":"/humanoid-robotics-book/docs/module4/llm_cognitive_planning","draft":false,"unlisted":false,"editUrl":"https://github.com/Amnaahmed798/humanoid-robotics-book/edit/main/docs/module4/llm_cognitive_planning.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Module 4: Vision-Language-Action (VLA)","permalink":"/humanoid-robotics-book/docs/module4/introduction"},"next":{"title":"Multi-Modal Interaction: Speech, Gesture, Vision","permalink":"/humanoid-robotics-book/docs/module4/multi_modal_interaction"}}');var i=t(4848),s=t(8453);const o={},r="Cognitive Planning using Large Language Models (LLMs)",l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Understanding Cognitive Planning in Robotics",id:"understanding-cognitive-planning-in-robotics",level:2},{value:"Traditional vs. LLM-Based Planning",id:"traditional-vs-llm-based-planning",level:3},{value:"Cognitive Planning Architecture",id:"cognitive-planning-architecture",level:3},{value:"Integrating LLMs with ROS 2",id:"integrating-llms-with-ros-2",level:2},{value:"LLM Communication Architecture",id:"llm-communication-architecture",level:3},{value:"Context-Aware Planning",id:"context-aware-planning",level:2},{value:"Maintaining World State",id:"maintaining-world-state",level:3},{value:"Plan Monitoring and Adjustment",id:"plan-monitoring-and-adjustment",level:2},{value:"Adaptive Planning",id:"adaptive-planning",level:3},{value:"Safety and Validation",id:"safety-and-validation",level:2},{value:"Plan Safety Checker",id:"plan-safety-checker",level:3},{value:"Integration with Other Systems",id:"integration-with-other-systems",level:2},{value:"Multi-Agent Coordination",id:"multi-agent-coordination",level:3},{value:"Best Practices for LLM Integration",id:"best-practices-for-llm-integration",level:2},{value:"Performance and Reliability",id:"performance-and-reliability",level:3},{value:"Testing and Validation",id:"testing-and-validation",level:3}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"cognitive-planning-using-large-language-models-llms",children:"Cognitive Planning using Large Language Models (LLMs)"})}),"\n",(0,i.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsx)(e.p,{children:"Large Language Models (LLMs) have revolutionized how we approach cognitive planning in robotics. Unlike traditional symbolic planners that rely on predefined rules and state transitions, LLMs can understand natural language, reason about complex tasks, and generate sophisticated action sequences. This chapter explores how to integrate LLMs into robotic systems for cognitive planning, enabling robots to perform complex, multi-step tasks with human-like reasoning."}),"\n",(0,i.jsx)(e.h2,{id:"understanding-cognitive-planning-in-robotics",children:"Understanding Cognitive Planning in Robotics"}),"\n",(0,i.jsx)(e.h3,{id:"traditional-vs-llm-based-planning",children:"Traditional vs. LLM-Based Planning"}),"\n",(0,i.jsx)(e.p,{children:"Traditional planning in robotics involves:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Symbolic Representation"}),": States and actions represented symbolically"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Predefined Rules"}),": Explicit rules for action selection"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Deterministic Execution"}),": Predictable action sequences"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Limited Flexibility"}),": Difficulty handling novel situations"]}),"\n"]}),"\n",(0,i.jsx)(e.p,{children:"LLM-based planning offers:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Natural Language Interface"}),": Direct command understanding"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Contextual Reasoning"}),": Ability to reason about context"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Adaptive Behavior"}),": Flexibility in handling novel situations"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Knowledge Integration"}),": Incorporation of world knowledge"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"cognitive-planning-architecture",children:"Cognitive Planning Architecture"}),"\n",(0,i.jsx)(e.p,{children:"The cognitive planning architecture with LLMs typically includes:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"Natural Language Input\n        \u2193\n    LLM Parser (Intent Recognition)\n        \u2193\n    Task Decomposition\n        \u2193\n    Action Sequence Generation\n        \u2193\n    ROS 2 Action Execution\n        \u2193\n    Feedback and Monitoring\n        \u2193\n    Plan Adjustment (if needed)\n"})}),"\n",(0,i.jsx)(e.h2,{id:"integrating-llms-with-ros-2",children:"Integrating LLMs with ROS 2"}),"\n",(0,i.jsx)(e.h3,{id:"llm-communication-architecture",children:"LLM Communication Architecture"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"import openai\nimport asyncio\nimport json\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import PoseStamped\nfrom action_msgs.msg import GoalStatus\nimport time\n\n\nclass LLMCognitivePlanner(Node):\n    \"\"\"\n    Cognitive planning node using Large Language Models.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__('llm_cognitive_planner')\n\n        # Initialize LLM client\n        self.llm_client = self.initialize_llm_client()\n\n        # Publishers\n        self.plan_pub = self.create_publisher(String, '/planning/plan', 10)\n        self.action_pub = self.create_publisher(String, '/planning/action', 10)\n        self.status_pub = self.create_publisher(String, '/planning/status', 10)\n\n        # Subscribers\n        self.command_sub = self.create_subscription(\n            String,\n            '/voice_command',\n            self.command_callback,\n            10\n        )\n\n        self.task_sub = self.create_subscription(\n            String,\n            '/high_level_task',\n            self.task_callback,\n            10\n        )\n\n        # Internal state\n        self.current_plan = []\n        self.executing_action = None\n        self.plan_execution_status = 'idle'\n        self.robot_capabilities = self.define_robot_capabilities()\n\n        self.get_logger().info('LLM Cognitive Planner initialized')\n\n    def initialize_llm_client(self):\n        \"\"\"\n        Initialize LLM client (using OpenAI as example).\n        \"\"\"\n        # Set your API key\n        openai.api_key = \"YOUR_API_KEY_HERE\"  # In practice, use secure configuration\n\n        return openai\n\n    def define_robot_capabilities(self):\n        \"\"\"\n        Define what the robot is capable of doing.\n        \"\"\"\n        return {\n            'navigation': {\n                'abilities': ['move_forward', 'move_backward', 'turn_left', 'turn_right', 'go_to_location'],\n                'constraints': ['max_speed: 0.5 m/s', 'max_rotation_speed: 1.0 rad/s']\n            },\n            'manipulation': {\n                'abilities': ['grasp_object', 'release_object', 'pick_up', 'place_down'],\n                'constraints': ['max_payload: 1.0 kg', 'reachable_area: 1.5m radius']\n            },\n            'perception': {\n                'abilities': ['detect_objects', 'identify_colors', 'measure_distances'],\n                'constraints': ['camera_fov: 60 degrees', 'depth_range: 0.1-3.0m']\n            },\n            'communication': {\n                'abilities': ['speak', 'listen', 'display_text'],\n                'constraints': ['language: English']\n            }\n        }\n\n    def command_callback(self, msg):\n        \"\"\"\n        Process high-level commands from voice or other sources.\n        \"\"\"\n        command = msg.data\n        self.get_logger().info(f'Received command: {command}')\n\n        # Plan and execute\n        asyncio.create_task(self.process_command_async(command))\n\n    def task_callback(self, msg):\n        \"\"\"\n        Process complex tasks that require cognitive planning.\n        \"\"\"\n        task = msg.data\n        self.get_logger().info(f'Received complex task: {task}')\n\n        # Plan and execute complex task\n        asyncio.create_task(self.plan_complex_task_async(task))\n\n    async def process_command_async(self, command):\n        \"\"\"\n        Process command asynchronously using LLM.\n        \"\"\"\n        try:\n            # Generate plan using LLM\n            plan = await self.generate_plan_with_llm(command)\n\n            # Execute plan\n            await self.execute_plan(plan)\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing command: {e}')\n\n    async def generate_plan_with_llm(self, command):\n        \"\"\"\n        Generate a plan using LLM based on the command.\n        \"\"\"\n        # Define the system prompt with robot capabilities\n        system_prompt = f\"\"\"\n        You are a cognitive planning assistant for a robot. The robot has the following capabilities:\n\n        {json.dumps(self.robot_capabilities, indent=2)}\n\n        Your task is to break down high-level commands into executable actions.\n        Respond with a JSON array of actions, where each action has:\n        - 'action': the specific action to take\n        - 'parameters': any required parameters\n        - 'description': human-readable description of the action\n\n        Actions should be atomic and executable by the robot.\n        \"\"\"\n\n        # User command\n        user_prompt = f\"Plan the following command: {command}\"\n\n        try:\n            response = await asyncio.get_event_loop().run_in_executor(\n                None,\n                lambda: self.llm_client.ChatCompletion.create(\n                    model=\"gpt-3.5-turbo\",  # or gpt-4 for more complex tasks\n                    messages=[\n                        {\"role\": \"system\", \"content\": system_prompt},\n                        {\"role\": \"user\", \"content\": user_prompt}\n                    ],\n                    temperature=0.1  # Low temperature for more deterministic output\n                )\n            )\n\n            # Extract the plan from response\n            plan_text = response.choices[0].message.content\n\n            # Parse JSON response\n            plan = self.parse_llm_response(plan_text)\n\n            # Validate plan\n            validated_plan = self.validate_plan(plan)\n\n            self.get_logger().info(f'Generated plan: {validated_plan}')\n\n            # Publish plan\n            plan_msg = String()\n            plan_msg.data = json.dumps(validated_plan)\n            self.plan_pub.publish(plan_msg)\n\n            return validated_plan\n\n        except Exception as e:\n            self.get_logger().error(f'Error generating plan with LLM: {e}')\n            return []\n\n    def parse_llm_response(self, response_text):\n        \"\"\"\n        Parse LLM response to extract plan.\n        \"\"\"\n        try:\n            # Look for JSON in the response\n            start_idx = response_text.find('[')\n            end_idx = response_text.rfind(']') + 1\n\n            if start_idx != -1 and end_idx != 0:\n                json_str = response_text[start_idx:end_idx]\n                plan = json.loads(json_str)\n                return plan\n            else:\n                # If no JSON found, try to extract as plain text\n                # This is a simplified fallback\n                return [{\"action\": \"unknown\", \"parameters\": {}, \"description\": response_text}]\n        except json.JSONDecodeError:\n            self.get_logger().error('Could not parse LLM response as JSON')\n            return []\n\n    def validate_plan(self, plan):\n        \"\"\"\n        Validate the plan against robot capabilities.\n        \"\"\"\n        validated_plan = []\n\n        for action in plan:\n            action_type = action.get('action', '')\n\n            # Check if action is supported by robot\n            is_supported = False\n            for capability_category in self.robot_capabilities.values():\n                if action_type in capability_category['abilities']:\n                    is_supported = True\n                    break\n\n            if is_supported:\n                validated_plan.append(action)\n            else:\n                self.get_logger().warn(f'Action not supported by robot: {action_type}')\n                # Could add fallback or error handling here\n\n        return validated_plan\n\n    async def execute_plan(self, plan):\n        \"\"\"\n        Execute the generated plan step by step.\n        \"\"\"\n        if not plan:\n            self.get_logger().warn('No plan to execute')\n            return\n\n        self.plan_execution_status = 'executing'\n\n        for i, action in enumerate(plan):\n            self.get_logger().info(f'Executing action {i+1}/{len(plan)}: {action[\"description\"]}')\n\n            # Update status\n            status_msg = String()\n            status_msg.data = f'Executing: {action[\"description\"]}'\n            self.status_pub.publish(status_msg)\n\n            # Execute action\n            success = await self.execute_single_action(action)\n\n            if not success:\n                self.get_logger().error(f'Action failed: {action}')\n                self.plan_execution_status = 'failed'\n                break\n\n        self.plan_execution_status = 'completed' if self.plan_execution_status == 'executing' else self.plan_execution_status\n\n    async def execute_single_action(self, action):\n        \"\"\"\n        Execute a single action from the plan.\n        \"\"\"\n        action_type = action.get('action', '')\n        parameters = action.get('parameters', {})\n\n        try:\n            if action_type == 'move_forward':\n                return await self.execute_move_forward(parameters)\n            elif action_type == 'move_backward':\n                return await self.execute_move_backward(parameters)\n            elif action_type == 'turn_left':\n                return await self.execute_turn_left(parameters)\n            elif action_type == 'turn_right':\n                return await self.execute_turn_right(parameters)\n            elif action_type == 'go_to_location':\n                return await self.execute_go_to_location(parameters)\n            elif action_type == 'grasp_object':\n                return await self.execute_grasp_object(parameters)\n            elif action_type == 'release_object':\n                return await self.execute_release_object(parameters)\n            elif action_type == 'detect_objects':\n                return await self.execute_detect_objects(parameters)\n            else:\n                self.get_logger().warn(f'Unknown action type: {action_type}')\n                return False\n\n        except Exception as e:\n            self.get_logger().error(f'Error executing action {action_type}: {e}')\n            return False\n\n    async def execute_move_forward(self, parameters):\n        \"\"\"\n        Execute move forward action.\n        \"\"\"\n        distance = parameters.get('distance', 1.0)  # default 1 meter\n        speed = parameters.get('speed', 0.3)  # default 0.3 m/s\n\n        # Publish action to ROS 2\n        action_msg = String()\n        action_msg.data = f\"MOVE_FORWARD:{distance}:{speed}\"\n        self.action_pub.publish(action_msg)\n\n        # Simulate execution time\n        await asyncio.sleep(distance / speed)\n\n        return True\n\n    async def execute_move_backward(self, parameters):\n        \"\"\"\n        Execute move backward action.\n        \"\"\"\n        distance = parameters.get('distance', 1.0)\n        speed = parameters.get('speed', 0.3)\n\n        action_msg = String()\n        action_msg.data = f\"MOVE_BACKWARD:{distance}:{speed}\"\n        self.action_pub.publish(action_msg)\n\n        await asyncio.sleep(distance / speed)\n\n        return True\n\n    async def execute_turn_left(self, parameters):\n        \"\"\"\n        Execute turn left action.\n        \"\"\"\n        angle = parameters.get('angle', 90.0)  # degrees\n        speed = parameters.get('speed', 0.5)  # rad/s\n\n        action_msg = String()\n        action_msg.data = f\"TURN_LEFT:{angle}:{speed}\"\n        self.action_pub.publish(action_msg)\n\n        await asyncio.sleep(angle / 180.0 * 3.14159 / speed)\n\n        return True\n\n    async def execute_turn_right(self, parameters):\n        \"\"\"\n        Execute turn right action.\n        \"\"\"\n        angle = parameters.get('angle', 90.0)\n        speed = parameters.get('speed', 0.5)\n\n        action_msg = String()\n        action_msg.data = f\"TURN_RIGHT:{angle}:{speed}\"\n        self.action_pub.publish(action_msg)\n\n        await asyncio.sleep(angle / 180.0 * 3.14159 / speed)\n\n        return True\n\n    async def execute_go_to_location(self, parameters):\n        \"\"\"\n        Execute go to location action.\n        \"\"\"\n        x = parameters.get('x', 0.0)\n        y = parameters.get('y', 0.0)\n        z = parameters.get('z', 0.0)\n\n        # Create navigation goal\n        goal = PoseStamped()\n        goal.header.stamp = self.get_clock().now().to_msg()\n        goal.header.frame_id = 'map'\n        goal.pose.position.x = x\n        goal.pose.position.y = y\n        goal.pose.position.z = z\n\n        # Publish navigation goal\n        # self.nav_goal_pub.publish(goal)  # Would need navigation publisher\n\n        action_msg = String()\n        action_msg.data = f\"GO_TO_LOCATION:{x},{y},{z}\"\n        self.action_pub.publish(action_msg)\n\n        # Wait for navigation to complete (simplified)\n        await asyncio.sleep(5.0)  # Simulated navigation time\n\n        return True\n\n    async def execute_grasp_object(self, parameters):\n        \"\"\"\n        Execute grasp object action.\n        \"\"\"\n        object_name = parameters.get('object', 'unknown')\n        approach_height = parameters.get('approach_height', 0.1)\n\n        action_msg = String()\n        action_msg.data = f\"GRASP_OBJECT:{object_name}:{approach_height}\"\n        self.action_pub.publish(action_msg)\n\n        await asyncio.sleep(2.0)  # Simulated grasping time\n\n        return True\n\n    async def execute_release_object(self, parameters):\n        \"\"\"\n        Execute release object action.\n        \"\"\"\n        object_name = parameters.get('object', 'unknown')\n        placement_height = parameters.get('placement_height', 0.0)\n\n        action_msg = String()\n        action_msg.data = f\"RELEASE_OBJECT:{object_name}:{placement_height}\"\n        self.action_pub.publish(action_msg)\n\n        await asyncio.sleep(2.0)  # Simulated releasing time\n\n        return True\n\n    async def execute_detect_objects(self, parameters):\n        \"\"\"\n        Execute detect objects action.\n        \"\"\"\n        detection_range = parameters.get('range', 2.0)\n        object_types = parameters.get('object_types', ['all'])\n\n        action_msg = String()\n        action_msg.data = f\"DETECT_OBJECTS:{detection_range}:{','.join(object_types)}\"\n        self.action_pub.publish(action_msg)\n\n        await asyncio.sleep(1.0)  # Simulated detection time\n\n        return True\n\n    async def plan_complex_task_async(self, task):\n        \"\"\"\n        Plan and execute a complex task requiring multiple steps.\n        \"\"\"\n        self.get_logger().info(f'Planning complex task: {task}')\n\n        # For complex tasks, we might need additional context\n        context = {\n            'robot_position': self.get_current_position(),\n            'environment_map': self.get_environment_map(),\n            'available_objects': self.get_available_objects(),\n            'robot_state': self.get_robot_state()\n        }\n\n        # Generate complex plan with context\n        plan = await self.generate_complex_plan_with_context(task, context)\n\n        # Execute the complex plan\n        await self.execute_plan(plan)\n\n    def get_current_position(self):\n        \"\"\"\n        Get current robot position (simplified).\n        \"\"\"\n        # In real implementation, this would get from odometry\n        return {'x': 0.0, 'y': 0.0, 'theta': 0.0}\n\n    def get_environment_map(self):\n        \"\"\"\n        Get environment map (simplified).\n        \"\"\"\n        # In real implementation, this would get from SLAM or map server\n        return {'known_locations': ['kitchen', 'living_room', 'bedroom']}\n\n    def get_available_objects(self):\n        \"\"\"\n        Get available objects in environment (simplified).\n        \"\"\"\n        # In real implementation, this would get from perception system\n        return ['red_ball', 'blue_cube', 'green_pyramid']\n\n    def get_robot_state(self):\n        \"\"\"\n        Get current robot state (simplified).\n        \"\"\"\n        return {\n            'battery_level': 85,\n            'gripper_status': 'open',\n            'navigation_status': 'ready'\n        }\n\n    async def generate_complex_plan_with_context(self, task, context):\n        \"\"\"\n        Generate a complex plan using LLM with additional context.\n        \"\"\"\n        system_prompt = f\"\"\"\n        You are a cognitive planning assistant for a robot. The robot has these capabilities:\n\n        {json.dumps(self.robot_capabilities, indent=2)}\n\n        The current context is:\n        - Robot Position: {context['robot_position']}\n        - Environment Map: {context['environment_map']}\n        - Available Objects: {context['available_objects']}\n        - Robot State: {context['robot_state']}\n\n        Plan the following complex task: {task}\n\n        Break it down into executable steps, considering the current context.\n        Respond with a JSON array of actions with 'action', 'parameters', and 'description'.\n        \"\"\"\n\n        try:\n            response = await asyncio.get_event_loop().run_in_executor(\n                None,\n                lambda: self.llm_client.ChatCompletion.create(\n                    model=\"gpt-4\",  # Use more capable model for complex tasks\n                    messages=[\n                        {\"role\": \"system\", \"content\": system_prompt},\n                        {\"role\": \"user\", \"content\": task}\n                    ],\n                    temperature=0.1\n                )\n            )\n\n            plan_text = response.choices[0].message.content\n            plan = self.parse_llm_response(plan_text)\n            validated_plan = self.validate_plan(plan)\n\n            return validated_plan\n\n        except Exception as e:\n            self.get_logger().error(f'Error generating complex plan: {e}')\n            return []\n"})}),"\n",(0,i.jsx)(e.h2,{id:"context-aware-planning",children:"Context-Aware Planning"}),"\n",(0,i.jsx)(e.h3,{id:"maintaining-world-state",children:"Maintaining World State"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'class ContextAwarePlanner(LLMCognitivePlanner):\n    """\n    Planner that maintains context and world state for better planning.\n    """\n\n    def __init__(self):\n        super().__init__()\n\n        # Initialize context and world state\n        self.world_state = {\n            \'locations\': {},\n            \'objects\': {},\n            \'robot_state\': {},\n            \'task_history\': [],\n            \'user_preferences\': {}\n        }\n\n        # Timer for updating world state\n        self.world_state_timer = self.create_timer(1.0, self.update_world_state)\n\n    def update_world_state(self):\n        """\n        Update world state with latest information.\n        """\n        # Update robot state\n        self.world_state[\'robot_state\'] = self.get_updated_robot_state()\n\n        # Update known locations from SLAM/map\n        self.world_state[\'locations\'] = self.get_updated_locations()\n\n        # Update objects from perception system\n        self.world_state[\'objects\'] = self.get_updated_objects()\n\n    def get_updated_robot_state(self):\n        """\n        Get latest robot state from sensors and systems.\n        """\n        # This would integrate with actual robot systems\n        return {\n            \'position\': self.get_current_position(),\n            \'battery\': self.get_battery_level(),\n            \'gripper\': self.get_gripper_status(),\n            \'navigation\': self.get_navigation_status()\n        }\n\n    def get_updated_locations(self):\n        """\n        Get latest known locations from map/SLAM system.\n        """\n        # This would integrate with actual mapping system\n        return self.get_environment_map()\n\n    def get_updated_objects(self):\n        """\n        Get latest known objects from perception system.\n        """\n        # This would integrate with actual perception system\n        return self.get_available_objects()\n\n    async def generate_plan_with_context(self, command):\n        """\n        Generate plan using full world state context.\n        """\n        # Include world state in prompt\n        context_str = json.dumps(self.world_state, indent=2)\n\n        system_prompt = f"""\n        You are a cognitive planning assistant for a robot. The robot has these capabilities:\n\n        {json.dumps(self.robot_capabilities, indent=2)}\n\n        Current world state:\n        {context_str}\n\n        Plan the following command: {command}\n\n        Consider the current world state when generating the plan.\n        Respond with a JSON array of actions with \'action\', \'parameters\', and \'description\'.\n        """\n\n        try:\n            response = await asyncio.get_event_loop().run_in_executor(\n                None,\n                lambda: self.llm_client.ChatCompletion.create(\n                    model="gpt-4",\n                    messages=[\n                        {"role": "system", "content": system_prompt},\n                        {"role": "user", "content": command}\n                    ],\n                    temperature=0.1\n                )\n            )\n\n            plan_text = response.choices[0].message.content\n            plan = self.parse_llm_response(plan_text)\n            validated_plan = self.validate_plan(plan)\n\n            # Update task history\n            self.world_state[\'task_history\'].append({\n                \'command\': command,\n                \'plan\': validated_plan,\n                \'timestamp\': time.time()\n            })\n\n            return validated_plan\n\n        except Exception as e:\n            self.get_logger().error(f\'Error generating contextual plan: {e}\')\n            return []\n'})}),"\n",(0,i.jsx)(e.h2,{id:"plan-monitoring-and-adjustment",children:"Plan Monitoring and Adjustment"}),"\n",(0,i.jsx)(e.h3,{id:"adaptive-planning",children:"Adaptive Planning"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'class AdaptivePlanner(ContextAwarePlanner):\n    """\n    Planner that can adapt and adjust plans based on execution feedback.\n    """\n\n    def __init__(self):\n        super().__init__()\n\n        # Subscribers for execution feedback\n        self.execution_feedback_sub = self.create_subscription(\n            String,\n            \'/execution_feedback\',\n            self.execution_feedback_callback,\n            10\n        )\n\n        # Plan adjustment parameters\n        self.adjustment_threshold = 0.7  # Confidence threshold for plan adjustments\n\n    def execution_feedback_callback(self, msg):\n        """\n        Process execution feedback and adjust plan if needed.\n        """\n        try:\n            feedback = json.loads(msg.data)\n            action_result = feedback.get(\'action_result\', \'success\')\n            action_id = feedback.get(\'action_id\', \'\')\n            confidence = feedback.get(\'confidence\', 1.0)\n\n            if action_result == \'failure\' or confidence < self.adjustment_threshold:\n                self.handle_execution_failure(action_id, feedback)\n\n        except json.JSONDecodeError:\n            self.get_logger().error(\'Could not parse execution feedback\')\n\n    def handle_execution_failure(self, action_id, feedback):\n        """\n        Handle action execution failure and adjust plan.\n        """\n        self.get_logger().warn(f\'Action {action_id} failed: {feedback}\')\n\n        # Generate alternative plan using LLM\n        async def adjust_plan():\n            # Get current state and failed action info\n            current_state = self.world_state.copy()\n            failed_action = next((a for a in self.current_plan if a.get(\'id\') == action_id), None)\n\n            if failed_action:\n                # Ask LLM for alternative approach\n                alternative_plan = await self.generate_alternative_plan(\n                    failed_action, current_state, feedback\n                )\n\n                if alternative_plan:\n                    # Replace failed action with alternative\n                    self.replace_failed_action(action_id, alternative_plan)\n\n        # Execute adjustment asynchronously\n        asyncio.create_task(adjust_plan())\n\n    async def generate_alternative_plan(self, failed_action, current_state, feedback):\n        """\n        Generate alternative plan when original action fails.\n        """\n        system_prompt = f"""\n        You are a cognitive planning assistant for a robot. The robot has these capabilities:\n\n        {json.dumps(self.robot_capabilities, indent=2)}\n\n        Current world state:\n        {json.dumps(current_state, indent=2)}\n\n        The following action failed:\n        {json.dumps(failed_action, indent=2)}\n\n        Failure feedback:\n        {json.dumps(feedback, indent=2)}\n\n        Generate an alternative plan to achieve the same goal, considering why the original action failed.\n        Respond with a JSON array of alternative actions.\n        """\n\n        try:\n            response = await asyncio.get_event_loop().run_in_executor(\n                None,\n                lambda: self.llm_client.ChatCompletion.create(\n                    model="gpt-4",\n                    messages=[\n                        {"role": "system", "content": system_prompt},\n                        {"role": "user", "content": "Generate alternative actions to achieve the goal"}\n                    ],\n                    temperature=0.2\n                )\n            )\n\n            plan_text = response.choices[0].message.content\n            alternative_plan = self.parse_llm_response(plan_text)\n            validated_plan = self.validate_plan(alternative_plan)\n\n            return validated_plan\n\n        except Exception as e:\n            self.get_logger().error(f\'Error generating alternative plan: {e}\')\n            return []\n\n    def replace_failed_action(self, action_id, alternative_plan):\n        """\n        Replace a failed action with an alternative plan.\n        """\n        # Find and replace the failed action in the current plan\n        for i, action in enumerate(self.current_plan):\n            if action.get(\'id\') == action_id:\n                # Replace with alternative actions\n                self.current_plan = (\n                    self.current_plan[:i] +\n                    alternative_plan +\n                    self.current_plan[i+1:]\n                )\n\n                self.get_logger().info(f\'Replaced failed action {action_id} with alternative plan\')\n                break\n'})}),"\n",(0,i.jsx)(e.h2,{id:"safety-and-validation",children:"Safety and Validation"}),"\n",(0,i.jsx)(e.h3,{id:"plan-safety-checker",children:"Plan Safety Checker"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"class SafeLLMPlanner(AdaptivePlanner):\n    \"\"\"\n    Planner with built-in safety checks and validation.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n        # Safety constraints\n        self.safety_constraints = {\n            'collision_avoidance': True,\n            'workspace_boundaries': True,\n            'payload_limits': True,\n            'energy_efficiency': True\n        }\n\n    def validate_plan(self, plan):\n        \"\"\"\n        Validate plan against safety constraints.\n        \"\"\"\n        validated_plan = []\n\n        for action in plan:\n            if self.is_action_safe(action):\n                validated_plan.append(action)\n            else:\n                self.get_logger().warn(f'Action deemed unsafe: {action}')\n                # Could implement safe alternatives here\n\n        return validated_plan\n\n    def is_action_safe(self, action):\n        \"\"\"\n        Check if an action is safe to execute.\n        \"\"\"\n        action_type = action.get('action', '')\n        parameters = action.get('parameters', {})\n\n        # Check various safety constraints\n        if not self.check_collision_risk(action_type, parameters):\n            return False\n\n        if not self.check_workspace_boundary(action_type, parameters):\n            return False\n\n        if not self.check_payload_limit(action_type, parameters):\n            return False\n\n        return True\n\n    def check_collision_risk(self, action_type, parameters):\n        \"\"\"\n        Check if action poses collision risk.\n        \"\"\"\n        # This would integrate with collision detection system\n        if action_type in ['move_forward', 'move_backward', 'go_to_location']:\n            # Check path for obstacles\n            target_location = self.calculate_target_location(action_type, parameters)\n            if self.has_obstacles_in_path(target_location):\n                return False\n\n        return True\n\n    def check_workspace_boundary(self, action_type, parameters):\n        \"\"\"\n        Check if action respects workspace boundaries.\n        \"\"\"\n        if action_type in ['go_to_location']:\n            target_x = parameters.get('x', 0.0)\n            target_y = parameters.get('y', 0.0)\n\n            # Check if target is within workspace boundaries\n            if (abs(target_x) > 10.0 or abs(target_y) > 10.0):  # Example boundaries\n                return False\n\n        return True\n\n    def check_payload_limit(self, action_type, parameters):\n        \"\"\"\n        Check if action respects payload limits.\n        \"\"\"\n        if action_type in ['grasp_object', 'pick_up']:\n            object_weight = parameters.get('weight', 0.0)\n            max_payload = 1.0  # kg\n\n            if object_weight > max_payload:\n                return False\n\n        return True\n\n    def calculate_target_location(self, action_type, parameters):\n        \"\"\"\n        Calculate target location for navigation actions.\n        \"\"\"\n        # Simplified calculation\n        if action_type == 'go_to_location':\n            return (parameters.get('x', 0.0), parameters.get('y', 0.0))\n        elif action_type == 'move_forward':\n            # Calculate based on current position and distance\n            current_pos = self.get_current_position()\n            distance = parameters.get('distance', 1.0)\n            # Simplified forward movement calculation\n            return (current_pos['x'] + distance, current_pos['y'])\n        else:\n            return (0.0, 0.0)\n\n    def has_obstacles_in_path(self, target_location):\n        \"\"\"\n        Check if there are obstacles in the path to target location.\n        \"\"\"\n        # This would integrate with perception/collision detection\n        # For now, return False (no obstacles detected)\n        return False\n"})}),"\n",(0,i.jsx)(e.h2,{id:"integration-with-other-systems",children:"Integration with Other Systems"}),"\n",(0,i.jsx)(e.h3,{id:"multi-agent-coordination",children:"Multi-Agent Coordination"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"class MultiAgentLLMPlanner(SafeLLMPlanner):\n    \"\"\"\n    Planner that can coordinate with other agents/robots.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n        # Publisher for coordination\n        self.coordination_pub = self.create_publisher(\n            String,\n            '/coordination_requests',\n            10\n        )\n\n        # Subscriber for coordination feedback\n        self.coordination_sub = self.create_subscription(\n            String,\n            '/coordination_responses',\n            self.coordination_callback,\n            10\n        )\n\n        # Track other agents\n        self.other_agents = {}\n\n    def coordination_callback(self, msg):\n        \"\"\"\n        Handle coordination messages from other agents.\n        \"\"\"\n        try:\n            coordination_data = json.loads(msg.data)\n            agent_id = coordination_data.get('agent_id')\n            request_type = coordination_data.get('request_type')\n            content = coordination_data.get('content')\n\n            if request_type == 'resource_request':\n                self.handle_resource_request(agent_id, content)\n            elif request_type == 'task_coordination':\n                self.handle_task_coordination(agent_id, content)\n\n        except json.JSONDecodeError:\n            self.get_logger().error('Could not parse coordination message')\n\n    def handle_resource_request(self, agent_id, content):\n        \"\"\"\n        Handle resource request from another agent.\n        \"\"\"\n        resource = content.get('resource')\n        priority = content.get('priority', 'normal')\n\n        # Decide whether to grant resource request\n        if self.can_grant_resource(resource, priority):\n            response = {\n                'agent_id': self.get_namespace(),  # This robot's ID\n                'request_id': content.get('request_id'),\n                'response': 'granted',\n                'resource': resource\n            }\n        else:\n            response = {\n                'agent_id': self.get_namespace(),\n                'request_id': content.get('request_id'),\n                'response': 'denied',\n                'reason': 'resource_not_available'\n            }\n\n        # Publish response\n        response_msg = String()\n        response_msg.data = json.dumps(response)\n        self.coordination_pub.publish(response_msg)\n\n    def can_grant_resource(self, resource, priority):\n        \"\"\"\n        Determine if we can grant a resource request.\n        \"\"\"\n        # Check if resource is currently needed by this agent\n        if resource == 'navigation_space' and self.plan_execution_status == 'executing':\n            return priority == 'emergency'  # Only grant in emergencies\n\n        return True  # Simplified - in practice, check resource availability\n\n    async def generate_coordinated_plan(self, task, other_agents_info):\n        \"\"\"\n        Generate plan considering other agents in the environment.\n        \"\"\"\n        system_prompt = f\"\"\"\n        You are a cognitive planning assistant for a robot coordinating with other agents.\n        The robot has these capabilities:\n\n        {json.dumps(self.robot_capabilities, indent=2)}\n\n        Other agents in the environment:\n        {json.dumps(other_agents_info, indent=2)}\n\n        Current world state:\n        {json.dumps(self.world_state, indent=2)}\n\n        Plan the following task while considering coordination with other agents: {task}\n\n        Be aware of potential resource conflicts and coordinate appropriately.\n        Respond with a JSON array of actions.\n        \"\"\"\n\n        try:\n            response = await asyncio.get_event_loop().run_in_executor(\n                None,\n                lambda: self.llm_client.ChatCompletion.create(\n                    model=\"gpt-4\",\n                    messages=[\n                        {\"role\": \"system\", \"content\": system_prompt},\n                        {\"role\": \"user\", \"content\": task}\n                    ],\n                    temperature=0.1\n                )\n            )\n\n            plan_text = response.choices[0].message.content\n            plan = self.parse_llm_response(plan_text)\n            validated_plan = self.validate_plan(plan)\n\n            return validated_plan\n\n        except Exception as e:\n            self.get_logger().error(f'Error generating coordinated plan: {e}')\n            return []\n"})}),"\n",(0,i.jsx)(e.h2,{id:"best-practices-for-llm-integration",children:"Best Practices for LLM Integration"}),"\n",(0,i.jsx)(e.h3,{id:"performance-and-reliability",children:"Performance and Reliability"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Caching"}),": Cache common plans and responses to reduce LLM calls"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Fallback Mechanisms"}),": Implement traditional planners as fallback"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Timeout Handling"}),": Set reasonable timeouts for LLM responses"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Error Recovery"}),": Plan for LLM failures gracefully"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Context Management"}),": Carefully manage context to avoid confusion"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Security"}),": Secure API keys and sensitive data"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Privacy"}),": Consider privacy implications of sending data to LLMs"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Cost Management"}),": Monitor and control API usage costs"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"testing-and-validation",children:"Testing and Validation"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Unit Tests"}),": Test individual planning components"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Integration Tests"}),": Test full planning-execution loop"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Simulation Testing"}),": Test in simulated environments first"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Safety Testing"}),": Verify safety constraints are enforced"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Edge Case Testing"}),": Test unusual or unexpected commands"]}),"\n"]}),"\n",(0,i.jsx)(e.p,{children:"LLM-based cognitive planning opens up new possibilities for flexible, adaptive robotic systems that can understand and execute complex, natural language commands while adapting to dynamic environments and unexpected situations."})]})}function p(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(d,{...n})}):d(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>o,x:()=>r});var a=t(6540);const i={},s=a.createContext(i);function o(n){const e=a.useContext(s);return a.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:o(n.components),a.createElement(s.Provider,{value:e},n.children)}}}]);