"use strict";(self.webpackChunkdocusaurus=self.webpackChunkdocusaurus||[]).push([[487],{2994:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>c,contentTitle:()=>i,default:()=>m,frontMatter:()=>s,metadata:()=>a,toc:()=>l});const a=JSON.parse('{"id":"module4/nl_to_ros2_actions","title":"Natural Language Command Translation to ROS 2 Actions","description":"Introduction","source":"@site/docs/module4/nl_to_ros2_actions.md","sourceDirName":"module4","slug":"/module4/nl_to_ros2_actions","permalink":"/humanoid-robotics-book/docs/module4/nl_to_ros2_actions","draft":false,"unlisted":false,"editUrl":"https://github.com/Amnaahmed798/humanoid-robotics-book/edit/main/docs/module4/nl_to_ros2_actions.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Multi-Modal Interaction: Speech, Gesture, Vision","permalink":"/humanoid-robotics-book/docs/module4/multi_modal_interaction"},"next":{"title":"Voice-to-Action with OpenAI Whisper Integration","permalink":"/humanoid-robotics-book/docs/module4/whisper_integration"}}');var o=t(4848),r=t(8453);const s={},i="Natural Language Command Translation to ROS 2 Actions",c={},l=[{value:"Introduction",id:"introduction",level:2},{value:"Understanding Natural Language Commands",id:"understanding-natural-language-commands",level:2},{value:"Command Structure Analysis",id:"command-structure-analysis",level:3},{value:"Command Categories",id:"command-categories",level:3},{value:"Rule-Based Command Parsing",id:"rule-based-command-parsing",level:2},{value:"Pattern Matching Approach",id:"pattern-matching-approach",level:3},{value:"Semantic Parsing with Named Entity Recognition",id:"semantic-parsing-with-named-entity-recognition",level:2},{value:"Advanced NER-based Parser",id:"advanced-ner-based-parser",level:3},{value:"Machine Learning Approach with Transformers",id:"machine-learning-approach-with-transformers",level:2},{value:"Transformer-Based Command Classifier",id:"transformer-based-command-classifier",level:3},{value:"ROS 2 Action Interface Integration",id:"ros-2-action-interface-integration",level:2},{value:"Action Translation System",id:"action-translation-system",level:3},{value:"Context-Aware Command Processing",id:"context-aware-command-processing",level:2},{value:"Context Manager for Command Understanding",id:"context-manager-for-command-understanding",level:3},{value:"Validation and Error Handling",id:"validation-and-error-handling",level:2},{value:"Command Validation System",id:"command-validation-system",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Caching and Optimization",id:"caching-and-optimization",level:3},{value:"Best Practices for NL to ROS 2 Translation",id:"best-practices-for-nl-to-ros-2-translation",level:2},{value:"Implementation Guidelines",id:"implementation-guidelines",level:3}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"natural-language-command-translation-to-ros-2-actions",children:"Natural Language Command Translation to ROS 2 Actions"})}),"\n",(0,o.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,o.jsx)(e.p,{children:"Translating natural language commands to ROS 2 actions is a critical component of cognitive robotic systems. This chapter explores how to convert human language into executable robot behaviors using a combination of natural language processing, semantic parsing, and ROS 2 action interfaces. We'll cover both rule-based and machine learning approaches to achieve robust command translation."}),"\n",(0,o.jsx)(e.h2,{id:"understanding-natural-language-commands",children:"Understanding Natural Language Commands"}),"\n",(0,o.jsx)(e.h3,{id:"command-structure-analysis",children:"Command Structure Analysis"}),"\n",(0,o.jsx)(e.p,{children:"Natural language commands typically follow patterns that can be categorized:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{children:"[Action] [Object] [Location] [Constraints]\n"})}),"\n",(0,o.jsx)(e.p,{children:"Examples:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:'"Go to the kitchen" \u2192 Action: navigate, Location: kitchen'}),"\n",(0,o.jsx)(e.li,{children:'"Pick up the red ball" \u2192 Action: grasp, Object: red ball'}),"\n",(0,o.jsx)(e.li,{children:'"Bring me the cup from the table" \u2192 Action: fetch, Object: cup, Location: table'}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"command-categories",children:"Command Categories"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Navigation Commands"}),": Move to locations, follow directions"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Manipulation Commands"}),": Pick up, place, grasp objects"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Perception Commands"}),": Look for, detect, identify objects"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Communication Commands"}),": Speak, listen, display information"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Complex Commands"}),": Multi-step tasks requiring planning"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"rule-based-command-parsing",children:"Rule-Based Command Parsing"}),"\n",(0,o.jsx)(e.h3,{id:"pattern-matching-approach",children:"Pattern Matching Approach"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import re\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional\nfrom enum import Enum\n\n\nclass ActionType(Enum):\n    NAVIGATE = "navigate"\n    GRASP = "grasp"\n    RELEASE = "release"\n    DETECT = "detect"\n    FOLLOW = "follow"\n    SPEAK = "speak"\n    LISTEN = "listen"\n\n\n@dataclass\nclass ParsedCommand:\n    action: ActionType\n    object_name: Optional[str] = None\n    location: Optional[str] = None\n    quantity: Optional[int] = None\n    attributes: List[str] = None\n    original_text: str = ""\n\n\nclass RuleBasedParser:\n    """\n    Rule-based parser for translating natural language to robot commands.\n    """\n\n    def __init__(self):\n        # Navigation patterns\n        self.nav_patterns = [\n            (r\'go\\s+to\\s+(?:the\\s+)?(\\w+)\', ActionType.NAVIGATE),\n            (r\'move\\s+to\\s+(?:the\\s+)?(\\w+)\', ActionType.NAVIGATE),\n            (r\'navigate\\s+to\\s+(?:the\\s+)?(\\w+)\', ActionType.NAVIGATE),\n            (r\'go\\s+(forward|backward|left|right)\', ActionType.NAVIGATE),\n            (r\'move\\s+(forward|backward|left|right)\', ActionType.NAVIGATE),\n        ]\n\n        # Manipulation patterns\n        self.manipulation_patterns = [\n            (r\'(?:pick\\s+up|grasp|grab|take|lift)\\s+(?:the\\s+)?(.+)\', ActionType.GRASP),\n            (r\'(?:place|put|set|drop|release)\\s+(?:down\\s+)?(?:the\\s+)?(.+)\', ActionType.RELEASE),\n            (r\'pick\\s+(?:up\\s+)?(.+)\', ActionType.GRASP),\n        ]\n\n        # Detection patterns\n        self.detection_patterns = [\n            (r\'(?:find|locate|look\\s+for|detect|search\\s+for)\\s+(?:the\\s+)?(.+)\', ActionType.DETECT),\n            (r\'where\\s+is\\s+(?:the\\s+)?(.+)\', ActionType.DETECT),\n        ]\n\n        # Follow patterns\n        self.follow_patterns = [\n            (r\'follow\\s+(me|him|her|the\\s+\\w+)\', ActionType.FOLLOW),\n            (r\'come\\s+with\\s+(me|him|her)\', ActionType.FOLLOW),\n        ]\n\n        # Speak patterns\n        self.speak_patterns = [\n            (r\'say\\s+(.+)\', ActionType.SPEAK),\n            (r\'speak\\s+(.+)\', ActionType.SPEAK),\n        ]\n\n        # Object attributes\n        self.attribute_patterns = [\n            (r\'(red|blue|green|yellow|purple|orange|pink|black|white|gray)\', \'color\'),\n            (r\'(small|large|big|tiny|huge|medium)\', \'size\'),\n            (r\'(square|round|rectangular|circular|triangular)\', \'shape\'),\n        ]\n\n    def parse_command(self, text: str) -> Optional[ParsedCommand]:\n        """\n        Parse natural language command into structured format.\n        """\n        text_lower = text.lower().strip()\n\n        # Extract attributes first\n        attributes = self.extract_attributes(text_lower)\n\n        # Try navigation patterns\n        for pattern, action_type in self.nav_patterns:\n            match = re.search(pattern, text_lower)\n            if match:\n                location = match.group(1) if match.lastindex else None\n                return ParsedCommand(\n                    action=action_type,\n                    location=location,\n                    attributes=attributes,\n                    original_text=text\n                )\n\n        # Try manipulation patterns\n        for pattern, action_type in self.manipulation_patterns:\n            match = re.search(pattern, text_lower)\n            if match:\n                object_name = match.group(1) if match.lastindex else None\n                return ParsedCommand(\n                    action=action_type,\n                    object_name=object_name,\n                    attributes=attributes,\n                    original_text=text\n                )\n\n        # Try detection patterns\n        for pattern, action_type in self.detection_patterns:\n            match = re.search(pattern, text_lower)\n            if match:\n                object_name = match.group(1) if match.lastindex else None\n                return ParsedCommand(\n                    action=action_type,\n                    object_name=object_name,\n                    attributes=attributes,\n                    original_text=text\n                )\n\n        # Try follow patterns\n        for pattern, action_type in self.follow_patterns:\n            match = re.search(pattern, text_lower)\n            if match:\n                target = match.group(1) if match.lastindex else None\n                return ParsedCommand(\n                    action=action_type,\n                    object_name=target,\n                    attributes=attributes,\n                    original_text=text\n                )\n\n        # Try speak patterns\n        for pattern, action_type in self.speak_patterns:\n            match = re.search(pattern, text_lower)\n            if match:\n                message = match.group(1) if match.lastindex else None\n                return ParsedCommand(\n                    action=action_type,\n                    object_name=message,  # Use object_name for message\n                    attributes=attributes,\n                    original_text=text\n                )\n\n        # If no pattern matched, return None\n        return None\n\n    def extract_attributes(self, text: str) -> List[str]:\n        """\n        Extract object attributes from text.\n        """\n        attributes = []\n\n        for pattern, attr_type in self.attribute_patterns:\n            matches = re.findall(pattern, text)\n            attributes.extend(matches)\n\n        return attributes\n\n    def validate_parsed_command(self, parsed: ParsedCommand) -> bool:\n        """\n        Validate the parsed command for semantic correctness.\n        """\n        if not parsed.action:\n            return False\n\n        # Check if required fields are present based on action type\n        if parsed.action in [ActionType.NAVIGATE, ActionType.FOLLOW]:\n            if not parsed.location and not parsed.object_name:\n                return False\n\n        elif parsed.action in [ActionType.GRASP, ActionType.RELEASE, ActionType.DETECT]:\n            if not parsed.object_name:\n                return False\n\n        elif parsed.action == ActionType.SPEAK:\n            if not parsed.object_name:  # Using object_name for message\n                return False\n\n        return True\n\n\n# Example usage\nparser = RuleBasedParser()\ncommand = parser.parse_command("Go to the kitchen and pick up the red ball")\nif command:\n    print(f"Action: {command.action}")\n    print(f"Location: {command.location}")\n    print(f"Object: {command.object_name}")\n    print(f"Attributes: {command.attributes}")\n'})}),"\n",(0,o.jsx)(e.h2,{id:"semantic-parsing-with-named-entity-recognition",children:"Semantic Parsing with Named Entity Recognition"}),"\n",(0,o.jsx)(e.h3,{id:"advanced-ner-based-parser",children:"Advanced NER-based Parser"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import spacy\nfrom typing import Tuple, Dict, Any\nimport json\n\n\nclass SemanticParser:\n    """\n    Semantic parser using spaCy for named entity recognition and dependency parsing.\n    """\n\n    def __init__(self):\n        # Load spaCy model (install with: python -m spacy download en_core_web_sm)\n        try:\n            self.nlp = spacy.load("en_core_web_sm")\n        except OSError:\n            print("spaCy model not found. Install with: python -m spacy download en_core_web_sm")\n            # Fallback to basic regex if model not available\n            self.nlp = None\n\n    def parse_with_spacy(self, text: str) -> Optional[ParsedCommand]:\n        """\n        Parse command using spaCy NLP.\n        """\n        if not self.nlp:\n            return None\n\n        doc = self.nlp(text)\n\n        # Extract action (verb)\n        action = self.extract_action(doc)\n        if not action:\n            return None\n\n        # Extract object\n        obj = self.extract_object(doc)\n\n        # Extract location\n        location = self.extract_location(doc)\n\n        # Extract attributes\n        attributes = self.extract_attributes_from_doc(doc)\n\n        return ParsedCommand(\n            action=action,\n            object_name=obj,\n            location=location,\n            attributes=attributes,\n            original_text=text\n        )\n\n    def extract_action(self, doc) -> Optional[ActionType]:\n        """\n        Extract action from parsed document.\n        """\n        # Look for main verbs that indicate actions\n        for token in doc:\n            if token.pos_ == "VERB":\n                lemma = token.lemma_.lower()\n\n                # Map verb lemmas to action types\n                if lemma in ["go", "move", "navigate", "walk", "run", "drive"]:\n                    return ActionType.NAVIGATE\n                elif lemma in ["pick", "grasp", "grab", "take", "lift", "hold"]:\n                    return ActionType.GRASP\n                elif lemma in ["place", "put", "set", "drop", "release", "lay"]:\n                    return ActionType.RELEASE\n                elif lemma in ["find", "locate", "look", "see", "detect", "search"]:\n                    return ActionType.DETECT\n                elif lemma in ["follow", "come", "track", "accompany"]:\n                    return ActionType.FOLLOW\n                elif lemma in ["say", "speak", "tell", "talk", "announce"]:\n                    return ActionType.SPEAK\n\n        return None\n\n    def extract_object(self, doc) -> Optional[str]:\n        """\n        Extract object from parsed document.\n        """\n        for token in doc:\n            # Look for direct objects\n            if token.dep_ == "dobj":\n                # Get the full noun phrase\n                return self.get_full_noun_phrase(token)\n\n            # Look for prepositional objects\n            if token.dep_ == "pobj" and token.head.text in ["for", "up", "down", "at"]:\n                return self.get_full_noun_phrase(token)\n\n        # If no direct object, look for compound nouns\n        for chunk in doc.noun_chunks:\n            if chunk.root.dep_ == "dobj" or chunk.root.head.pos_ == "VERB":\n                return chunk.text\n\n        return None\n\n    def extract_location(self, doc) -> Optional[str]:\n        """\n        Extract location from parsed document.\n        """\n        for token in doc:\n            # Look for prepositional phrases indicating location\n            if token.dep_ == "prep" and token.head.pos_ == "VERB":\n                # Get the prepositional phrase\n                prep_obj = [child for child in token.children if child.dep_ == "pobj"]\n                if prep_obj:\n                    return self.get_full_noun_phrase(prep_obj[0])\n\n        # Look for noun chunks that might be locations\n        for chunk in doc.noun_chunks:\n            if chunk.text.lower() in ["kitchen", "living room", "bedroom", "office", "hallway", "door", "window"]:\n                return chunk.text\n\n        return None\n\n    def extract_attributes_from_doc(self, doc) -> List[str]:\n        """\n        Extract attributes from parsed document.\n        """\n        attributes = []\n\n        for token in doc:\n            # Look for adjectives modifying nouns\n            if token.pos_ == "ADJ":\n                # Check if this adjective modifies a nearby noun\n                for child in token.head.children:\n                    if child.dep_ == "amod" and child.text == token.text:\n                        attributes.append(token.text)\n\n        return attributes\n\n    def get_full_noun_phrase(self, token) -> str:\n        """\n        Get the full noun phrase starting from the token.\n        """\n        # Get all tokens in the subtree\n        tokens = [t.text for t in token.subtree if t.pos_ in ["NOUN", "PROPN", "ADJ", "DET"]]\n        return " ".join(tokens)\n'})}),"\n",(0,o.jsx)(e.h2,{id:"machine-learning-approach-with-transformers",children:"Machine Learning Approach with Transformers"}),"\n",(0,o.jsx)(e.h3,{id:"transformer-based-command-classifier",children:"Transformer-Based Command Classifier"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom typing import List\nimport numpy as np\n\n\nclass TransformerCommandClassifier:\n    """\n    Transformer-based classifier for command categorization.\n    """\n\n    def __init__(self, model_name="bert-base-uncased"):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n        # Define number of action classes\n        self.num_actions = len(ActionType)\n\n        # Initialize model\n        self.model = AutoModelForSequenceClassification.from_pretrained(\n            model_name,\n            num_labels=self.num_actions\n        )\n\n        # Action to index mapping\n        self.action_to_idx = {action.value: idx for idx, action in enumerate(ActionType)}\n        self.idx_to_action = {idx: action for action, idx in self.action_to_idx.items()}\n\n    def preprocess_command(self, text: str) -> Dict[str, torch.Tensor]:\n        """\n        Preprocess command text for transformer model.\n        """\n        encoded = self.tokenizer(\n            text,\n            padding=True,\n            truncation=True,\n            max_length=128,\n            return_tensors="pt"\n        )\n        return encoded\n\n    def classify_command(self, text: str) -> Tuple[ActionType, float]:\n        """\n        Classify command text into action type with confidence score.\n        """\n        inputs = self.preprocess_command(text)\n\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n            predictions = torch.softmax(outputs.logits, dim=-1)\n            predicted_idx = torch.argmax(predictions, dim=-1).item()\n            confidence = predictions[0][predicted_idx].item()\n\n        action = self.idx_to_action[predicted_idx]\n        return action, confidence\n\n    def extract_entities(self, text: str) -> Dict[str, str]:\n        """\n        Extract entities using a sequence labeling approach.\n        """\n        # This would typically use a NER model like BERT for NER\n        # For simplicity, we\'ll use the rule-based approach here\n        # In practice, you\'d use a dedicated NER model\n\n        parser = RuleBasedParser()\n        parsed = parser.parse_command(text)\n\n        if parsed:\n            return {\n                \'object\': parsed.object_name or \'\',\n                \'location\': parsed.location or \'\',\n                \'attributes\': \', \'.join(parsed.attributes or [])\n            }\n        else:\n            return {\'object\': \'\', \'location\': \'\', \'attributes\': \'\'}\n\n\nclass HybridCommandProcessor:\n    """\n    Hybrid processor combining rule-based and ML approaches.\n    """\n\n    def __init__(self):\n        self.rule_parser = RuleBasedParser()\n        self.ml_classifier = TransformerCommandClassifier()\n        self.semantic_parser = SemanticParser() if self.is_spacy_available() else None\n\n    def is_spacy_available(self) -> bool:\n        """\n        Check if spaCy is available.\n        """\n        try:\n            import spacy\n            return True\n        except ImportError:\n            return False\n\n    def process_command(self, text: str) -> ParsedCommand:\n        """\n        Process command using hybrid approach.\n        """\n        # Try rule-based parsing first (fast and reliable for common patterns)\n        rule_parsed = self.rule_parser.parse_command(text)\n        if rule_parsed and self.rule_parser.validate_parsed_command(rule_parsed):\n            return rule_parsed\n\n        # Try semantic parsing if spaCy is available\n        if self.semantic_parser:\n            semantic_parsed = self.semantic_parser.parse_with_spacy(text)\n            if semantic_parsed and self.rule_parser.validate_parsed_command(semantic_parsed):\n                return semantic_parsed\n\n        # Fall back to ML classification\n        ml_action, confidence = self.ml_classifier.classify_command(text)\n\n        if confidence > 0.7:  # Only use if confidence is high enough\n            entities = self.ml_classifier.extract_entities(text)\n\n            return ParsedCommand(\n                action=ml_action,\n                object_name=entities.get(\'object\'),\n                location=entities.get(\'location\'),\n                attributes=entities.get(\'attributes\', \'\').split(\', \') if entities.get(\'attributes\') else [],\n                original_text=text\n            )\n\n        # If all methods fail, return a default command\n        return ParsedCommand(\n            action=ActionType.SPEAK,\n            object_name=f"I don\'t understand the command: {text}",\n            original_text=text\n        )\n'})}),"\n",(0,o.jsx)(e.h2,{id:"ros-2-action-interface-integration",children:"ROS 2 Action Interface Integration"}),"\n",(0,o.jsx)(e.h3,{id:"action-translation-system",children:"Action Translation System"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom rclpy.action import ActionClient\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import PoseStamped\nfrom geometry_msgs.msg import Twist\nfrom sensor_msgs.msg import JointState\nfrom action_msgs.msg import GoalStatus\n\n# Import common action interfaces\nfrom nav2_msgs.action import NavigateToPose\nfrom control_msgs.action import GripperCommand\nfrom std_srvs.srv import Trigger\n\n\nclass NLCommandTranslator(Node):\n    """\n    Node that translates natural language commands to ROS 2 actions.\n    """\n\n    def __init__(self):\n        super().__init__(\'nl_command_translator\')\n\n        # Initialize command processor\n        self.command_processor = HybridCommandProcessor()\n\n        # Publishers\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n        self.status_pub = self.create_publisher(String, \'/command_status\', 10)\n\n        # Subscribers\n        self.command_sub = self.create_subscription(\n            String,\n            \'/natural_language_command\',\n            self.command_callback,\n            10\n        )\n\n        # Action clients\n        self.nav_client = ActionClient(self, NavigateToPose, \'navigate_to_pose\')\n        self.gripper_client = ActionClient(self, GripperCommand, \'gripper_command\')\n\n        # Service clients\n        self.speech_client = self.create_client(Trigger, \'speak_text\')\n\n        # Wait for action servers\n        self.nav_client.wait_for_server()\n        self.gripper_client.wait_for_server()\n\n        self.get_logger().info(\'NL Command Translator initialized\')\n\n    def command_callback(self, msg):\n        """\n        Process natural language command.\n        """\n        command_text = msg.data\n        self.get_logger().info(f\'Received command: {command_text}\')\n\n        # Parse command\n        parsed_command = self.command_processor.process_command(command_text)\n\n        if parsed_command:\n            self.get_logger().info(f\'Parsed command: {parsed_command.action.value}\')\n            self.execute_parsed_command(parsed_command)\n        else:\n            self.get_logger().error(f\'Could not parse command: {command_text}\')\n            self.send_error_response(f"Sorry, I couldn\'t understand: {command_text}")\n\n    def execute_parsed_command(self, parsed_command: ParsedCommand):\n        """\n        Execute the parsed command based on its action type.\n        """\n        action_type = parsed_command.action\n\n        if action_type == ActionType.NAVIGATE:\n            self.execute_navigation_command(parsed_command)\n        elif action_type == ActionType.GRASP:\n            self.execute_grasp_command(parsed_command)\n        elif action_type == ActionType.RELEASE:\n            self.execute_release_command(parsed_command)\n        elif action_type == ActionType.DETECT:\n            self.execute_detection_command(parsed_command)\n        elif action_type == ActionType.FOLLOW:\n            self.execute_follow_command(parsed_command)\n        elif action_type == ActionType.SPEAK:\n            self.execute_speak_command(parsed_command)\n        else:\n            self.send_error_response(f"Unknown action type: {action_type}")\n\n    def execute_navigation_command(self, parsed_command: ParsedCommand):\n        """\n        Execute navigation command.\n        """\n        location = parsed_command.location\n\n        if not location:\n            self.send_error_response("No destination specified for navigation")\n            return\n\n        # Map location names to coordinates (would come from map server in practice)\n        location_coords = self.get_location_coordinates(location)\n\n        if location_coords:\n            goal_msg = NavigateToPose.Goal()\n            goal_msg.pose.header.frame_id = \'map\'\n            goal_msg.pose.header.stamp = self.get_clock().now().to_msg()\n            goal_msg.pose.pose.position.x = location_coords[0]\n            goal_msg.pose.pose.position.y = location_coords[1]\n            goal_msg.pose.pose.position.z = 0.0\n\n            # Set orientation (facing forward)\n            from math import pi\n            goal_msg.pose.pose.orientation.z = 0.0\n            goal_msg.pose.pose.orientation.w = 1.0\n\n            self.get_logger().info(f\'Navigating to {location} at ({location_coords[0]}, {location_coords[1]})\')\n\n            # Send navigation goal\n            future = self.nav_client.send_goal_async(goal_msg)\n            future.add_done_callback(self.navigation_done_callback)\n        else:\n            self.send_error_response(f"Unknown location: {location}")\n\n    def get_location_coordinates(self, location_name: str) -> Optional[Tuple[float, float]]:\n        """\n        Get coordinates for a named location.\n        """\n        # This would normally come from a map server or location database\n        location_map = {\n            \'kitchen\': (-2.0, 1.0),\n            \'living room\': (1.0, -1.0),\n            \'bedroom\': (2.0, 2.0),\n            \'office\': (-1.0, -2.0),\n            \'entrance\': (0.0, 0.0)\n        }\n\n        return location_map.get(location_name.lower())\n\n    def navigation_done_callback(self, future):\n        """\n        Callback when navigation goal is completed.\n        """\n        goal_handle = future.result()\n        if goal_handle.accepted:\n            self.get_logger().info(\'Navigation goal accepted\')\n        else:\n            self.get_logger().error(\'Navigation goal rejected\')\n\n    def execute_grasp_command(self, parsed_command: ParsedCommand):\n        """\n        Execute grasp command.\n        """\n        object_name = parsed_command.object_name\n\n        if not object_name:\n            self.send_error_response("No object specified for grasping")\n            return\n\n        self.get_logger().info(f\'Attempting to grasp: {object_name}\')\n\n        # First, navigate to object location\n        # This would involve object detection and localization\n        # For now, we\'ll simulate the process\n\n        # Send gripper command\n        goal_msg = GripperCommand.Goal()\n        goal_msg.command.position = 0.0  # Closed position\n        goal_msg.command.max_effort = 100.0\n\n        self.get_logger().info(\'Sending grasp command\')\n\n        future = self.gripper_client.send_goal_async(goal_msg)\n        future.add_done_callback(self.gripper_done_callback)\n\n    def execute_release_command(self, parsed_command: ParsedCommand):\n        """\n        Execute release command.\n        """\n        object_name = parsed_command.object_name\n\n        if not object_name:\n            self.send_error_response("No object specified for releasing")\n            return\n\n        self.get_logger().info(f\'Attempting to release: {object_name}\')\n\n        # Send gripper command to open\n        goal_msg = GripperCommand.Goal()\n        goal_msg.command.position = 1.0  # Open position\n        goal_msg.command.max_effort = 100.0\n\n        self.get_logger().info(\'Sending release command\')\n\n        future = self.gripper_client.send_goal_async(goal_msg)\n        future.add_done_callback(self.gripper_done_callback)\n\n    def gripper_done_callback(self, future):\n        """\n        Callback when gripper action is completed.\n        """\n        goal_handle = future.result()\n        if goal_handle.accepted:\n            self.get_logger().info(\'Gripper action accepted\')\n        else:\n            self.get_logger().error(\'Gripper action rejected\')\n\n    def execute_detection_command(self, parsed_command: ParsedCommand):\n        """\n        Execute detection command.\n        """\n        object_name = parsed_command.object_name\n\n        if not object_name:\n            self.send_error_response("No object specified for detection")\n            return\n\n        self.get_logger().info(f\'Looking for: {object_name}\')\n\n        # This would trigger object detection system\n        # For now, we\'ll simulate the process\n        detected = self.simulate_object_detection(object_name)\n\n        if detected:\n            response = f"I found the {object_name}!"\n        else:\n            response = f"I couldn\'t find the {object_name}."\n\n        self.send_speech_response(response)\n\n    def simulate_object_detection(self, object_name: str) -> bool:\n        """\n        Simulate object detection process.\n        """\n        # In real implementation, this would interface with perception system\n        # For simulation, return True 70% of the time\n        import random\n        return random.random() < 0.7\n\n    def execute_follow_command(self, parsed_command: ParsedCommand):\n        """\n        Execute follow command.\n        """\n        target = parsed_command.object_name or "person"\n\n        self.get_logger().info(f\'Following: {target}\')\n\n        # This would activate follow behavior\n        # For now, send a simple command\n        cmd = Twist()\n        cmd.linear.x = 0.3  # Follow at 0.3 m/s\n        cmd.angular.z = 0.0  # Don\'t turn\n\n        # Publish follow command\n        self.cmd_vel_pub.publish(cmd)\n\n        self.send_speech_response(f"Now following {target}")\n\n    def execute_speak_command(self, parsed_command: ParsedCommand):\n        """\n        Execute speak command.\n        """\n        message = parsed_command.object_name  # Using object_name for message\n\n        if not message:\n            self.send_error_response("No message to speak")\n            return\n\n        self.get_logger().info(f\'Speaking: {message}\')\n\n        # Call speech service\n        if self.speech_client.wait_for_service(timeout_sec=1.0):\n            request = Trigger.Request()\n            request.data = message\n            future = self.speech_client.call_async(request)\n            future.add_done_callback(self.speech_done_callback)\n        else:\n            self.get_logger().error(\'Speech service not available\')\n\n    def speech_done_callback(self, future):\n        """\n        Callback when speech is completed.\n        """\n        try:\n            response = future.result()\n            if response.success:\n                self.get_logger().info(\'Speech completed successfully\')\n            else:\n                self.get_logger().error(f\'Speech failed: {response.message}\')\n        except Exception as e:\n            self.get_logger().error(f\'Speech service call failed: {e}\')\n\n    def send_error_response(self, error_msg: str):\n        """\n        Send error response.\n        """\n        self.get_logger().error(error_msg)\n\n        # Send error status\n        status_msg = String()\n        status_msg.data = f"ERROR: {error_msg}"\n        self.status_pub.publish(status_msg)\n\n        # Try to speak the error\n        self.send_speech_response(f"Error: {error_msg}")\n\n    def send_speech_response(self, message: str):\n        """\n        Send speech response.\n        """\n        if self.speech_client.wait_for_service(timeout_sec=1.0):\n            request = Trigger.Request()\n            request.data = message\n            self.speech_client.call_async(request)\n        else:\n            self.get_logger().warn(f\'Cannot speak: {message}\')\n'})}),"\n",(0,o.jsx)(e.h2,{id:"context-aware-command-processing",children:"Context-Aware Command Processing"}),"\n",(0,o.jsx)(e.h3,{id:"context-manager-for-command-understanding",children:"Context Manager for Command Understanding"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'class ContextManager:\n    """\n    Manages context for improved command understanding.\n    """\n\n    def __init__(self):\n        self.conversation_history = []\n        self.current_task = None\n        self.robot_state = {}\n        self.environment_state = {}\n        self.user_preferences = {}\n\n    def update_context(self, parsed_command: ParsedCommand, execution_result: str = ""):\n        """\n        Update context based on command and execution result.\n        """\n        context_entry = {\n            \'command\': parsed_command,\n            \'timestamp\': time.time(),\n            \'execution_result\': execution_result\n        }\n\n        self.conversation_history.append(context_entry)\n\n        # Keep only recent history (last 10 entries)\n        if len(self.conversation_history) > 10:\n            self.conversation_history = self.conversation_history[-10:]\n\n    def resolve_pronouns(self, text: str) -> str:\n        """\n        Resolve pronouns based on context.\n        """\n        # Simple pronoun resolution\n        if \'it\' in text.lower():\n            # Find the last mentioned object\n            for entry in reversed(self.conversation_history):\n                if entry[\'command\'].object_name:\n                    resolved = text.lower().replace(\'it\', entry[\'command\'].object_name)\n                    return resolved\n\n        if \'there\' in text.lower():\n            # Find the last mentioned location\n            for entry in reversed(self.conversation_history):\n                if entry[\'command\'].location:\n                    resolved = text.lower().replace(\'there\', entry[\'command\'].location)\n                    return resolved\n\n        return text\n\n    def infer_missing_information(self, parsed_command: ParsedCommand) -> ParsedCommand:\n        """\n        Infer missing information from context.\n        """\n        # If location is missing but context suggests one\n        if not parsed_command.location and self.current_task:\n            # For example, if current task is in kitchen, assume kitchen location\n            if \'kitchen\' in self.current_task:\n                parsed_command.location = \'kitchen\'\n\n        # If object is generic, use context\n        if parsed_command.object_name == \'it\':\n            for entry in reversed(self.conversation_history):\n                if entry[\'command\'].object_name:\n                    parsed_command.object_name = entry[\'command\'].object_name\n                    break\n\n        return parsed_command\n\n\nclass ContextAwareTranslator(NLCommandTranslator):\n    """\n    Command translator with context awareness.\n    """\n\n    def __init__(self):\n        super().__init__()\n        self.context_manager = ContextManager()\n\n    def command_callback(self, msg):\n        """\n        Process natural language command with context awareness.\n        """\n        command_text = msg.data\n\n        # Resolve pronouns and references\n        resolved_text = self.context_manager.resolve_pronouns(command_text)\n        self.get_logger().info(f\'Resolved command: {resolved_text}\')\n\n        # Parse command\n        parsed_command = self.command_processor.process_command(resolved_text)\n\n        if parsed_command:\n            # Infer missing information from context\n            parsed_command = self.context_manager.infer_missing_information(parsed_command)\n\n            self.get_logger().info(f\'Parsed command: {parsed_command.action.value}\')\n            self.execute_parsed_command(parsed_command)\n\n            # Update context\n            self.context_manager.update_context(parsed_command, "executed")\n        else:\n            self.get_logger().error(f\'Could not parse command: {resolved_text}\')\n            self.send_error_response(f"Sorry, I couldn\'t understand: {resolved_text}")\n            self.context_manager.update_context(\n                ParsedCommand(action=ActionType.SPEAK, object_name=f"Sorry, I couldn\'t understand: {resolved_text}"),\n                "failed"\n            )\n'})}),"\n",(0,o.jsx)(e.h2,{id:"validation-and-error-handling",children:"Validation and Error Handling"}),"\n",(0,o.jsx)(e.h3,{id:"command-validation-system",children:"Command Validation System"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'class CommandValidator:\n    """\n    Validates commands for safety and feasibility.\n    """\n\n    def __init__(self):\n        self.workspace_limits = {\n            \'x_min\': -10.0, \'x_max\': 10.0,\n            \'y_min\': -10.0, \'y_max\': 10.0,\n            \'z_min\': 0.0, \'z_max\': 2.0\n        }\n\n        self.payload_limits = {\n            \'max_weight\': 1.0,  # kg\n            \'max_volume\': 0.01   # m^3\n        }\n\n    def validate_command(self, parsed_command: ParsedCommand) -> Tuple[bool, str]:\n        """\n        Validate command for safety and feasibility.\n        """\n        action_type = parsed_command.action\n\n        if action_type == ActionType.NAVIGATE:\n            return self.validate_navigation_command(parsed_command)\n        elif action_type == ActionType.GRASP:\n            return self.validate_grasp_command(parsed_command)\n        elif action_type == ActionType.RELEASE:\n            return self.validate_release_command(parsed_command)\n        else:\n            # Other actions are generally safe\n            return True, "Command is valid"\n\n    def validate_navigation_command(self, parsed_command: ParsedCommand) -> Tuple[bool, str]:\n        """\n        Validate navigation command.\n        """\n        location = parsed_command.location\n\n        if not location:\n            return False, "No destination specified"\n\n        # Check if location is within workspace limits\n        coords = self.get_location_coordinates(location)\n        if coords:\n            x, y = coords\n            if (x < self.workspace_limits[\'x_min\'] or x > self.workspace_limits[\'x_max\'] or\n                y < self.workspace_limits[\'y_min\'] or y > self.workspace_limits[\'y_max\']):\n                return False, f"Destination {location} is outside workspace limits"\n\n        return True, "Navigation command is valid"\n\n    def validate_grasp_command(self, parsed_command: ParsedCommand) -> Tuple[bool, str]:\n        """\n        Validate grasp command.\n        """\n        object_name = parsed_command.object_name\n\n        if not object_name:\n            return False, "No object specified for grasping"\n\n        # In real implementation, check object properties (weight, size, etc.)\n        # For now, assume all objects are graspable\n        return True, "Grasp command is valid"\n\n    def validate_release_command(self, parsed_command: ParsedCommand) -> Tuple[bool, str]:\n        """\n        Validate release command.\n        """\n        # Release commands are generally safe\n        return True, "Release command is valid"\n\n    def get_location_coordinates(self, location_name: str) -> Optional[Tuple[float, float]]:\n        """\n        Get coordinates for a named location (copied from NLCommandTranslator).\n        """\n        location_map = {\n            \'kitchen\': (-2.0, 1.0),\n            \'living room\': (1.0, -1.0),\n            \'bedroom\': (2.0, 2.0),\n            \'office\': (-1.0, -2.0),\n            \'entrance\': (0.0, 0.0)\n        }\n\n        return location_map.get(location_name.lower())\n'})}),"\n",(0,o.jsx)(e.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,o.jsx)(e.h3,{id:"caching-and-optimization",children:"Caching and Optimization"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'from functools import lru_cache\nimport hashlib\n\n\nclass OptimizedCommandTranslator(ContextAwareTranslator):\n    """\n    Optimized command translator with caching and performance enhancements.\n    """\n\n    def __init__(self):\n        super().__init__()\n        self.command_cache = {}\n        self.cache_size_limit = 100\n\n    @lru_cache(maxsize=1000)\n    def cached_parse_command(self, text: str) -> Optional[ParsedCommand]:\n        """\n        Cached command parsing for frequently used commands.\n        """\n        return self.command_processor.process_command(text)\n\n    def command_callback(self, msg):\n        """\n        Process command with caching.\n        """\n        command_text = msg.data\n\n        # Create cache key\n        cache_key = hashlib.md5(command_text.encode()).hexdigest()\n\n        # Check cache first\n        if cache_key in self.command_cache:\n            parsed_command = self.command_cache[cache_key]\n            self.get_logger().info(f\'Command found in cache: {command_text}\')\n        else:\n            # Resolve pronouns and references\n            resolved_text = self.context_manager.resolve_pronouns(command_text)\n\n            # Parse command\n            parsed_command = self.cached_parse_command(resolved_text)\n\n            # Add to cache if successful\n            if parsed_command:\n                self.command_cache[cache_key] = parsed_command\n                # Limit cache size\n                if len(self.command_cache) > self.cache_size_limit:\n                    # Remove oldest entry\n                    oldest_key = next(iter(self.command_cache))\n                    del self.command_cache[oldest_key]\n\n        if parsed_command:\n            # Validate command\n            validator = CommandValidator()\n            is_valid, validation_msg = validator.validate_command(parsed_command)\n\n            if is_valid:\n                # Infer missing information from context\n                parsed_command = self.context_manager.infer_missing_information(parsed_command)\n\n                self.get_logger().info(f\'Parsed command: {parsed_command.action.value}\')\n                self.execute_parsed_command(parsed_command)\n\n                # Update context\n                self.context_manager.update_context(parsed_command, "executed")\n            else:\n                self.get_logger().error(f\'Invalid command: {validation_msg}\')\n                self.send_error_response(f"Command not valid: {validation_msg}")\n        else:\n            self.get_logger().error(f\'Could not parse command: {resolved_text}\')\n            self.send_error_response(f"Sorry, I couldn\'t understand: {resolved_text}")\n            self.context_manager.update_context(\n                ParsedCommand(action=ActionType.SPEAK, object_name=f"Sorry, I couldn\'t understand: {resolved_text}"),\n                "failed"\n            )\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    node = OptimizedCommandTranslator()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info(\'Shutting down command translator...\')\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,o.jsx)(e.h2,{id:"best-practices-for-nl-to-ros-2-translation",children:"Best Practices for NL to ROS 2 Translation"}),"\n",(0,o.jsx)(e.h3,{id:"implementation-guidelines",children:"Implementation Guidelines"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Layered Approach"}),": Use multiple parsing methods (rule-based, ML, semantic)"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Context Awareness"}),": Maintain conversation and task context"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Validation"}),": Always validate commands for safety and feasibility"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Error Handling"}),": Provide graceful fallbacks for unrecognized commands"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Caching"}),": Cache frequent commands to improve performance"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Feedback"}),": Provide clear feedback to users about command execution"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Security"}),": Sanitize inputs to prevent command injection"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Testing"}),": Extensively test with various command formulations"]}),"\n"]}),"\n",(0,o.jsx)(e.p,{children:"The natural language to ROS 2 action translation system enables robots to understand and respond to human commands in natural language, bridging the gap between human communication and robot execution capabilities."})]})}function m(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(d,{...n})}):d(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>s,x:()=>i});var a=t(6540);const o={},r=a.createContext(o);function s(n){const e=a.useContext(r);return a.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function i(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:s(n.components),a.createElement(r.Provider,{value:e},n.children)}}}]);