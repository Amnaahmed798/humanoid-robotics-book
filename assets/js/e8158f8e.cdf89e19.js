"use strict";(self.webpackChunkdocusaurus=self.webpackChunkdocusaurus||[]).push([[597],{6625:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>p,frontMatter:()=>a,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module4/whisper_integration","title":"Voice-to-Action with OpenAI Whisper Integration","description":"Introduction","source":"@site/docs/module4/whisper_integration.md","sourceDirName":"module4","slug":"/module4/whisper_integration","permalink":"/humanoid-robotics-book/docs/module4/whisper_integration","draft":false,"unlisted":false,"editUrl":"https://github.com/Amnaahmed798/humanoid-robotics-book/edit/main/docs/module4/whisper_integration.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Natural Language Command Translation to ROS 2 Actions","permalink":"/humanoid-robotics-book/docs/module4/nl_to_ros2_actions"},"next":{"title":"Appendices","permalink":"/humanoid-robotics-book/docs/appendices/overview"}}');var o=t(4848),r=t(8453);const a={},s="Voice-to-Action with OpenAI Whisper Integration",l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Understanding Speech Recognition in Robotics",id:"understanding-speech-recognition-in-robotics",level:2},{value:"Challenges in Robotic Speech Recognition",id:"challenges-in-robotic-speech-recognition",level:3},{value:"Whisper Model Overview",id:"whisper-model-overview",level:3},{value:"Installing and Setting up Whisper",id:"installing-and-setting-up-whisper",level:2},{value:"Installation Requirements",id:"installation-requirements",level:3},{value:"Basic Whisper Usage",id:"basic-whisper-usage",level:3},{value:"Integrating Whisper with ROS 2",id:"integrating-whisper-with-ros-2",level:2},{value:"Audio Capture and Processing Node",id:"audio-capture-and-processing-node",level:3},{value:"Advanced Whisper Integration with Context",id:"advanced-whisper-integration-with-context",level:2},{value:"Context-Aware Command Processing",id:"context-aware-command-processing",level:3},{value:"Optimizing Whisper for Real-time Robotics",id:"optimizing-whisper-for-real-time-robotics",level:2},{value:"Performance Optimization Techniques",id:"performance-optimization-techniques",level:3},{value:"Voice Command Grammar and Validation",id:"voice-command-grammar-and-validation",level:2},{value:"Command Validation System",id:"command-validation-system",level:3},{value:"Integration with Robot Systems",id:"integration-with-robot-systems",level:2},{value:"Complete Integration Example",id:"complete-integration-example",level:3},{value:"Best Practices for Whisper Integration",id:"best-practices-for-whisper-integration",level:2},{value:"Performance Considerations",id:"performance-considerations",level:3}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"voice-to-action-with-openai-whisper-integration",children:"Voice-to-Action with OpenAI Whisper Integration"})}),"\n",(0,o.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,o.jsx)(e.p,{children:"Voice-to-action capabilities enable robots to understand and respond to spoken commands, making human-robot interaction more natural and intuitive. OpenAI's Whisper model provides state-of-the-art speech recognition that can be integrated into robotic systems to enable voice-controlled operation. This chapter covers the integration of Whisper with robotic systems for voice-to-action capabilities."}),"\n",(0,o.jsx)(e.h2,{id:"understanding-speech-recognition-in-robotics",children:"Understanding Speech Recognition in Robotics"}),"\n",(0,o.jsx)(e.h3,{id:"challenges-in-robotic-speech-recognition",children:"Challenges in Robotic Speech Recognition"}),"\n",(0,o.jsx)(e.p,{children:"Speech recognition in robotics faces unique challenges compared to traditional applications:"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Acoustic Environment"}),": Robots operate in noisy environments with mechanical sounds"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Real-time Processing"}),": Commands need to be processed with minimal latency"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Domain Specificity"}),": Commands are often specific to robot capabilities"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Robustness"}),": Systems must handle various accents, speaking styles, and environmental conditions"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"whisper-model-overview",children:"Whisper Model Overview"}),"\n",(0,o.jsx)(e.p,{children:"Whisper is a general-purpose speech recognition model that:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Supports multiple languages"}),"\n",(0,o.jsx)(e.li,{children:"Handles various accents and speaking styles"}),"\n",(0,o.jsx)(e.li,{children:"Performs well in noisy environments"}),"\n",(0,o.jsx)(e.li,{children:"Can be fine-tuned for specific domains"}),"\n",(0,o.jsx)(e.li,{children:"Provides both speech-to-text and speech-to-speech capabilities"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"installing-and-setting-up-whisper",children:"Installing and Setting up Whisper"}),"\n",(0,o.jsx)(e.h3,{id:"installation-requirements",children:"Installation Requirements"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-bash",children:"# Install Whisper and related dependencies\npip install openai-whisper\npip install torch torchvision torchaudio  # PyTorch\npip install pyaudio  # For audio capture\npip install soundfile  # For audio file processing\npip install transformers  # For additional NLP processing\n"})}),"\n",(0,o.jsx)(e.h3,{id:"basic-whisper-usage",children:"Basic Whisper Usage"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import whisper\nimport torch\n\n# Load the model\nmodel = whisper.load_model("base")  # Options: tiny, base, small, medium, large\n\n# Transcribe audio file\nresult = model.transcribe("path/to/audio.wav")\nprint(result["text"])\n'})}),"\n",(0,o.jsx)(e.h2,{id:"integrating-whisper-with-ros-2",children:"Integrating Whisper with ROS 2"}),"\n",(0,o.jsx)(e.h3,{id:"audio-capture-and-processing-node",children:"Audio Capture and Processing Node"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import AudioData\nimport pyaudio\nimport wave\nimport numpy as np\nimport whisper\nimport torch\nimport tempfile\nimport os\nfrom threading import Thread, Lock\nimport queue\n\n\nclass WhisperAudioNode(Node):\n    \"\"\"\n    ROS 2 node that captures audio and processes it with Whisper.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__('whisper_audio_node')\n\n        # Initialize Whisper model\n        self.get_logger().info('Loading Whisper model...')\n        self.model = whisper.load_model(\"base\")\n        self.get_logger().info('Whisper model loaded')\n\n        # Audio parameters\n        self.rate = 16000  # Sample rate\n        self.chunk = 1024  # Buffer size\n        self.format = pyaudio.paInt16\n        self.channels = 1\n        self.record_seconds = 3  # Duration of each recording\n\n        # Publishers and subscribers\n        self.transcript_pub = self.create_publisher(\n            String,\n            '/voice_transcript',\n            10\n        )\n\n        self.command_pub = self.create_publisher(\n            String,\n            '/voice_command',\n            10\n        )\n\n        # Audio processing\n        self.audio = pyaudio.PyAudio()\n        self.is_listening = False\n        self.audio_queue = queue.Queue()\n        self.processing_lock = Lock()\n\n        # Start audio capture thread\n        self.capture_thread = Thread(target=self.capture_audio, daemon=True)\n        self.capture_thread.start()\n\n        # Timer for processing audio\n        self.process_timer = self.create_timer(0.1, self.process_audio_queue)\n\n        self.get_logger().info('Whisper Audio Node initialized')\n\n    def capture_audio(self):\n        \"\"\"\n        Capture audio from microphone in a separate thread.\n        \"\"\"\n        stream = self.audio.open(\n            format=self.format,\n            channels=self.channels,\n            rate=self.rate,\n            input=True,\n            frames_per_buffer=self.chunk\n        )\n\n        self.get_logger().info('Audio capture started')\n\n        while rclpy.ok():\n            frames = []\n\n            # Record for specified duration\n            for _ in range(0, int(self.rate / self.chunk * self.record_seconds)):\n                data = stream.read(self.chunk)\n                frames.append(data)\n\n            # Add to processing queue\n            self.audio_queue.put(frames)\n\n        stream.stop_stream()\n        stream.close()\n\n    def process_audio_queue(self):\n        \"\"\"\n        Process audio from queue using Whisper.\n        \"\"\"\n        try:\n            frames = self.audio_queue.get_nowait()\n            self.process_audio_chunk(frames)\n        except queue.Empty:\n            pass  # No audio to process\n\n    def process_audio_chunk(self, frames):\n        \"\"\"\n        Process a chunk of audio frames with Whisper.\n        \"\"\"\n        if self.processing_lock.locked():\n            # Skip if already processing\n            return\n\n        with self.processing_lock:\n            try:\n                # Convert frames to numpy array\n                audio_data = b''.join(frames)\n                audio_np = np.frombuffer(audio_data, dtype=np.int16)\n\n                # Normalize audio\n                audio_float = audio_np.astype(np.float32) / 32768.0\n\n                # Save to temporary file for Whisper processing\n                with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:\n                    # Write WAV file\n                    wf = wave.open(temp_file.name, 'wb')\n                    wf.setnchannels(self.channels)\n                    wf.setsampwidth(2)  # 16-bit\n                    wf.setframerate(self.rate)\n                    wf.writeframes(audio_data)\n                    wf.close()\n\n                    # Transcribe with Whisper\n                    result = self.model.transcribe(temp_file.name)\n                    transcript = result[\"text\"].strip()\n\n                    # Clean up temp file\n                    os.unlink(temp_file.name)\n\n                if transcript:\n                    # Publish transcript\n                    transcript_msg = String()\n                    transcript_msg.data = transcript\n                    self.transcript_pub.publish(transcript_msg)\n\n                    self.get_logger().info(f'Heard: {transcript}')\n\n                    # Process command if it's a robot command\n                    self.process_command(transcript)\n\n            except Exception as e:\n                self.get_logger().error(f'Error processing audio: {e}')\n\n    def process_command(self, transcript):\n        \"\"\"\n        Process the transcript to extract robot commands.\n        \"\"\"\n        # Convert to lowercase for easier processing\n        text = transcript.lower()\n\n        # Define command patterns\n        command_patterns = {\n            'move_forward': ['move forward', 'go forward', 'forward', 'move ahead', 'go ahead'],\n            'move_backward': ['move backward', 'go backward', 'backward', 'back', 'reverse'],\n            'turn_left': ['turn left', 'left', 'rotate left'],\n            'turn_right': ['turn right', 'right', 'rotate right'],\n            'stop': ['stop', 'halt', 'pause', 'freeze'],\n            'pick_up': ['pick up', 'grasp', 'grab', 'take', 'lift'],\n            'place_down': ['place down', 'put down', 'place', 'put', 'release'],\n            'follow_me': ['follow me', 'follow', 'come with me'],\n            'go_to': ['go to', 'move to', 'navigate to', 'go to location']\n        }\n\n        # Find matching command\n        detected_command = None\n        for command, patterns in command_patterns.items():\n            for pattern in patterns:\n                if pattern in text:\n                    detected_command = command\n                    break\n            if detected_command:\n                break\n\n        if detected_command:\n            # Publish command\n            cmd_msg = String()\n            cmd_msg.data = detected_command\n            self.command_pub.publish(cmd_msg)\n            self.get_logger().info(f'Command detected: {detected_command}')\n        else:\n            self.get_logger().info('No command detected')\n\n    def destroy_node(self):\n        \"\"\"\n        Clean up audio resources.\n        \"\"\"\n        self.audio.terminate()\n        super().destroy_node()\n\n\nclass WhisperCommandInterpreter(Node):\n    \"\"\"\n    Node to interpret Whisper commands and convert to robot actions.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__('whisper_command_interpreter')\n\n        # Subscribers\n        self.transcript_sub = self.create_subscription(\n            String,\n            '/voice_transcript',\n            self.transcript_callback,\n            10\n        )\n\n        self.command_sub = self.create_subscription(\n            String,\n            '/voice_command',\n            self.command_callback,\n            10\n        )\n\n        # Publishers for robot control\n        self.cmd_vel_pub = self.create_publisher(\n            Twist,\n            '/cmd_vel',\n            10\n        )\n\n        self.gripper_cmd_pub = self.create_publisher(\n            String,\n            '/gripper_command',\n            10\n        )\n\n        self.navigation_goal_pub = self.create_publisher(\n            PoseStamped,\n            '/goal_pose',\n            10\n        )\n\n        # Command mapping\n        self.command_mapping = {\n            'move_forward': self.move_forward,\n            'move_backward': self.move_backward,\n            'turn_left': self.turn_left,\n            'turn_right': self.turn_right,\n            'stop': self.stop_robot,\n            'pick_up': self.pick_up_object,\n            'place_down': self.place_down_object,\n            'follow_me': self.follow_mode,\n            'go_to': self.go_to_location\n        }\n\n        self.get_logger().info('Whisper Command Interpreter initialized')\n\n    def transcript_callback(self, msg):\n        \"\"\"\n        Process transcript for context and intent analysis.\n        \"\"\"\n        transcript = msg.data.lower()\n\n        # Extract locations, objects, and other context from transcript\n        self.extract_context(transcript)\n\n    def command_callback(self, msg):\n        \"\"\"\n        Execute the detected command.\n        \"\"\"\n        command = msg.data\n\n        if command in self.command_mapping:\n            self.command_mapping[command]()\n        else:\n            self.get_logger().warn(f'Unknown command: {command}')\n\n    def extract_context(self, transcript):\n        \"\"\"\n        Extract context like locations, objects from transcript.\n        \"\"\"\n        # This would use NLP techniques to extract entities\n        # For example: \"go to the kitchen\" -> location: \"kitchen\"\n        # \"pick up the red ball\" -> object: \"red ball\"\n        pass\n\n    def move_forward(self):\n        \"\"\"\n        Move robot forward.\n        \"\"\"\n        cmd = Twist()\n        cmd.linear.x = 0.3  # m/s\n        self.cmd_vel_pub.publish(cmd)\n        self.get_logger().info('Moving forward')\n\n    def move_backward(self):\n        \"\"\"\n        Move robot backward.\n        \"\"\"\n        cmd = Twist()\n        cmd.linear.x = -0.3  # m/s\n        self.cmd_vel_pub.publish(cmd)\n        self.get_logger().info('Moving backward')\n\n    def turn_left(self):\n        \"\"\"\n        Turn robot left.\n        \"\"\"\n        cmd = Twist()\n        cmd.angular.z = 0.5  # rad/s\n        self.cmd_vel_pub.publish(cmd)\n        self.get_logger().info('Turning left')\n\n    def turn_right(self):\n        \"\"\"\n        Turn robot right.\n        \"\"\"\n        cmd = Twist()\n        cmd.angular.z = -0.5  # rad/s\n        self.cmd_vel_pub.publish(cmd)\n        self.get_logger().info('Turning right')\n\n    def stop_robot(self):\n        \"\"\"\n        Stop robot movement.\n        \"\"\"\n        cmd = Twist()\n        self.cmd_vel_pub.publish(cmd)\n        self.get_logger().info('Stopping robot')\n\n    def pick_up_object(self):\n        \"\"\"\n        Command robot to pick up an object.\n        \"\"\"\n        cmd = String()\n        cmd.data = 'grasp'\n        self.gripper_cmd_pub.publish(cmd)\n        self.get_logger().info('Attempting to pick up object')\n\n    def place_down_object(self):\n        \"\"\"\n        Command robot to place down an object.\n        \"\"\"\n        cmd = String()\n        cmd.data = 'release'\n        self.gripper_cmd_pub.publish(cmd)\n        self.get_logger().info('Releasing object')\n\n    def follow_mode(self):\n        \"\"\"\n        Activate follow mode.\n        \"\"\"\n        # This would activate a follow behavior\n        self.get_logger().info('Activating follow mode')\n\n    def go_to_location(self):\n        \"\"\"\n        Navigate to a specific location.\n        \"\"\"\n        # This would require location extraction from transcript\n        self.get_logger().info('Navigating to location')\n"})}),"\n",(0,o.jsx)(e.h2,{id:"advanced-whisper-integration-with-context",children:"Advanced Whisper Integration with Context"}),"\n",(0,o.jsx)(e.h3,{id:"context-aware-command-processing",children:"Context-Aware Command Processing"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"import re\nfrom typing import Dict, List, Tuple\n\n\nclass ContextAwareWhisperProcessor:\n    \"\"\"\n    Advanced Whisper processor that maintains context and state.\n    \"\"\"\n\n    def __init__(self):\n        self.context = {\n            'current_location': 'unknown',\n            'target_object': 'unknown',\n            'task_sequence': [],\n            'conversation_history': []\n        }\n\n    def process_transcript_with_context(self, transcript: str, current_context: Dict) -> Dict:\n        \"\"\"\n        Process transcript with awareness of current context.\n        \"\"\"\n        self.context.update(current_context)\n\n        # Analyze the transcript\n        analysis = self.analyze_transcript(transcript)\n\n        # Determine intent based on context\n        intent = self.determine_intent(transcript, analysis)\n\n        # Extract entities considering context\n        entities = self.extract_entities(transcript, intent)\n\n        # Generate response/action\n        action = self.generate_action(intent, entities)\n\n        # Update conversation history\n        self.context['conversation_history'].append({\n            'transcript': transcript,\n            'intent': intent,\n            'entities': entities,\n            'action': action\n        })\n\n        return {\n            'intent': intent,\n            'entities': entities,\n            'action': action,\n            'context': self.context.copy()\n        }\n\n    def analyze_transcript(self, transcript: str) -> Dict:\n        \"\"\"\n        Analyze transcript for various linguistic features.\n        \"\"\"\n        analysis = {\n            'tokens': transcript.lower().split(),\n            'entities': self.extract_named_entities(transcript),\n            'action_verbs': self.extract_action_verbs(transcript),\n            'spatial_prepositions': self.extract_spatial_prepositions(transcript),\n            'quantifiers': self.extract_quantifiers(transcript)\n        }\n\n        return analysis\n\n    def extract_named_entities(self, transcript: str) -> List[str]:\n        \"\"\"\n        Extract named entities from transcript.\n        \"\"\"\n        # Simple pattern matching for common robot entities\n        entities = []\n\n        # Object patterns\n        object_patterns = [\n            r'\\b(red|blue|green|yellow|big|small|large|tiny)\\s+(\\w+)\\b',\n            r'\\b(ball|cube|box|cup|bottle|chair|table)\\b'\n        ]\n\n        for pattern in object_patterns:\n            matches = re.findall(pattern, transcript, re.IGNORECASE)\n            for match in matches:\n                if isinstance(match, tuple):\n                    entities.append(' '.join(match))\n                else:\n                    entities.append(match)\n\n        return entities\n\n    def extract_action_verbs(self, transcript: str) -> List[str]:\n        \"\"\"\n        Extract action verbs from transcript.\n        \"\"\"\n        action_verbs = [\n            'go', 'move', 'navigate', 'walk', 'run', 'turn', 'rotate',\n            'pick', 'grasp', 'grab', 'take', 'lift', 'place', 'put', 'release',\n            'find', 'locate', 'look', 'see', 'follow', 'come', 'bring'\n        ]\n\n        found_verbs = []\n        tokens = transcript.lower().split()\n\n        for token in tokens:\n            for verb in action_verbs:\n                if verb in token:\n                    found_verbs.append(verb)\n\n        return found_verbs\n\n    def extract_spatial_prepositions(self, transcript: str) -> List[str]:\n        \"\"\"\n        Extract spatial prepositions from transcript.\n        \"\"\"\n        prepositions = [\n            'to', 'at', 'in', 'on', 'under', 'over', 'next to', 'near', 'by',\n            'left', 'right', 'front', 'back', 'behind', 'ahead'\n        ]\n\n        found_prep = []\n        lower_transcript = transcript.lower()\n\n        for prep in prepositions:\n            if prep in lower_transcript:\n                found_prep.append(prep)\n\n        return found_prep\n\n    def determine_intent(self, transcript: str, analysis: Dict) -> str:\n        \"\"\"\n        Determine the intent of the command based on analysis.\n        \"\"\"\n        transcript_lower = transcript.lower()\n\n        # Intent classification based on keywords and context\n        if any(verb in transcript_lower for verb in ['go', 'move', 'navigate', 'walk']):\n            if any(prep in transcript_lower for prep in ['to', 'at', 'in']):\n                return 'navigation_to_location'\n            else:\n                return 'simple_navigation'\n\n        elif any(verb in transcript_lower for verb in ['pick', 'grasp', 'grab', 'take', 'lift']):\n            return 'object_manipulation_pick'\n\n        elif any(verb in transcript_lower for verb in ['place', 'put', 'release']):\n            return 'object_manipulation_place'\n\n        elif any(verb in transcript_lower for verb in ['find', 'locate', 'look', 'see']):\n            return 'object_search'\n\n        elif any(verb in transcript_lower for verb in ['follow', 'come']):\n            return 'following'\n\n        else:\n            return 'unknown'\n\n    def extract_entities(self, transcript: str, intent: str) -> Dict:\n        \"\"\"\n        Extract entities relevant to the determined intent.\n        \"\"\"\n        entities = {}\n\n        if intent in ['navigation_to_location', 'object_search']:\n            # Extract location names\n            location_patterns = [\n                r'to the (\\w+)',\n                r'at the (\\w+)',\n                r'in the (\\w+)',\n                r'to (\\w+)',\n                r'go (?:to|at|in) the? (\\w+)'\n            ]\n\n            for pattern in location_patterns:\n                match = re.search(pattern, transcript, re.IGNORECASE)\n                if match:\n                    entities['location'] = match.group(1)\n                    break\n\n        if intent in ['object_manipulation_pick', 'object_manipulation_place', 'object_search']:\n            # Extract object descriptions\n            object_patterns = [\n                r'(?:the|a|an) ((?:red|blue|green|yellow|big|small|large|tiny)\\s+\\w+)',\n                r'(?:the|a|an) (\\w+)',\n                r'(\\w+) (?:on|at|near) the'\n            ]\n\n            for pattern in object_patterns:\n                match = re.search(pattern, transcript, re.IGNORECASE)\n                if match:\n                    entities['object'] = match.group(1)\n                    break\n\n        return entities\n\n    def generate_action(self, intent: str, entities: Dict) -> Dict:\n        \"\"\"\n        Generate robot action based on intent and entities.\n        \"\"\"\n        action = {\n            'type': intent,\n            'parameters': entities,\n            'confidence': 0.9  # Default high confidence\n        }\n\n        # Add specific parameters based on intent\n        if intent == 'navigation_to_location':\n            action['ros_action'] = 'nav2_msgs.action.NavigateToPose'\n            action['topic'] = '/navigate_to_pose'\n\n        elif intent == 'object_manipulation_pick':\n            action['ros_action'] = 'control_msgs.action.GripperCommand'\n            action['topic'] = '/gripper_command'\n\n        elif intent == 'object_manipulation_place':\n            action['ros_action'] = 'control_msgs.action.GripperCommand'\n            action['topic'] = '/gripper_command'\n\n        elif intent == 'object_search':\n            action['ros_action'] = 'object_search_action'\n            action['topic'] = '/object_search'\n\n        elif intent == 'following':\n            action['ros_action'] = 'follow_action'\n            action['topic'] = '/follow_person'\n\n        return action\n"})}),"\n",(0,o.jsx)(e.h2,{id:"optimizing-whisper-for-real-time-robotics",children:"Optimizing Whisper for Real-time Robotics"}),"\n",(0,o.jsx)(e.h3,{id:"performance-optimization-techniques",children:"Performance Optimization Techniques"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import asyncio\nimport concurrent.futures\nfrom functools import partial\nimport time\n\n\nclass OptimizedWhisperProcessor:\n    """\n    Optimized Whisper processor for real-time robotics applications.\n    """\n\n    def __init__(self, model_size="base"):\n        self.model_size = model_size\n        self.model = None\n        self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=2)\n        self.load_model()\n\n    def load_model(self):\n        """\n        Load Whisper model with optimizations.\n        """\n        import whisper\n        import torch\n\n        # Use GPU if available\n        device = "cuda" if torch.cuda.is_available() else "cpu"\n\n        self.get_logger().info(f\'Loading Whisper model on {device}...\')\n        self.model = whisper.load_model(self.model_size).to(device)\n\n        # Set to evaluation mode\n        self.model.eval()\n\n    def transcribe_audio_optimized(self, audio_path: str) -> str:\n        """\n        Optimized transcription with reduced latency.\n        """\n        # Use faster processing options\n        result = self.model.transcribe(\n            audio_path,\n            fp16=torch.cuda.is_available(),  # Use float16 on GPU\n            language=\'en\',\n            task=\'transcribe\'\n        )\n\n        return result["text"]\n\n    async def process_audio_async(self, audio_data):\n        """\n        Process audio asynchronously to avoid blocking.\n        """\n        loop = asyncio.get_event_loop()\n\n        # Run transcription in thread pool to avoid blocking\n        result = await loop.run_in_executor(\n            self.executor,\n            partial(self.transcribe_audio_optimized, audio_data)\n        )\n\n        return result\n\n    def process_with_timeout(self, audio_path: str, timeout: float = 5.0) -> str:\n        """\n        Process audio with timeout to ensure real-time performance.\n        """\n        import signal\n\n        def timeout_handler(signum, frame):\n            raise TimeoutError("Whisper processing timed out")\n\n        # Set up timeout\n        old_handler = signal.signal(signal.SIGALRM, timeout_handler)\n        signal.alarm(int(timeout))\n\n        try:\n            result = self.transcribe_audio_optimized(audio_path)\n            signal.alarm(0)  # Cancel alarm\n            return result\n        except TimeoutError:\n            self.get_logger().warn(\'Whisper processing timed out\')\n            return ""\n        finally:\n            signal.signal(signal.SIGALRM, old_handler)  # Restore old handler\n'})}),"\n",(0,o.jsx)(e.h2,{id:"voice-command-grammar-and-validation",children:"Voice Command Grammar and Validation"}),"\n",(0,o.jsx)(e.h3,{id:"command-validation-system",children:"Command Validation System"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import json\nimport re\nfrom dataclasses import dataclass\nfrom typing import Optional, List\n\n\n@dataclass\nclass VoiceCommand:\n    """\n    Data structure for validated voice commands.\n    """\n    intent: str\n    entities: Dict[str, str]\n    confidence: float\n    raw_transcript: str\n    timestamp: float\n\n\nclass VoiceCommandValidator:\n    """\n    Validates voice commands against a defined grammar.\n    """\n\n    def __init__(self):\n        self.command_grammar = self.load_command_grammar()\n\n    def load_command_grammar(self) -> Dict:\n        """\n        Load the command grammar definition.\n        """\n        return {\n            "navigation": {\n                "patterns": [\n                    r"^(?:go|move|navigate|walk)\\s+(?:to|toward|towards)\\s+(?:the\\s+)?(\\w+)$",\n                    r"^(?:go|move|navigate|walk)\\s+(forward|backward|left|right|ahead)$",\n                    r"^(?:turn|rotate)\\s+(left|right)$"\n                ],\n                "required_entities": ["destination"],\n                "optional_entities": ["direction", "speed"]\n            },\n            "manipulation": {\n                "patterns": [\n                    r"^(?:pick|grasp|grab|take|lift)\\s+(?:up\\s+)?(?:the\\s+)?(.+)$",\n                    r"^(?:place|put|set|release)\\s+(?:down\\s+)?(?:the\\s+)?(.+)(?:\\s+on\\s+(.+))?$"\n                ],\n                "required_entities": ["object"],\n                "optional_entities": ["location"]\n            },\n            "query": {\n                "patterns": [\n                    r"^(?:where|what)\\s+(?:is|are)\\s+(?:the\\s+)?(.+)$",\n                    r"^(?:can\\s+you|could\\s+you)\\s+(.+)\\\\?$"\n                ],\n                "required_entities": ["query_subject"],\n                "optional_entities": []\n            }\n        }\n\n    def validate_command(self, transcript: str) -> Optional[VoiceCommand]:\n        """\n        Validate transcript against command grammar.\n        """\n        transcript_lower = transcript.lower().strip()\n\n        for intent, grammar in self.command_grammar.items():\n            for pattern in grammar["patterns"]:\n                match = re.match(pattern, transcript_lower)\n                if match:\n                    # Extract entities\n                    entities = {}\n                    if match.groups():\n                        # This is a simplified entity extraction\n                        # In practice, you\'d map group indices to entity types\n                        entities = {"extracted": match.group(1)}\n\n                    return VoiceCommand(\n                        intent=intent,\n                        entities=entities,\n                        confidence=0.85,  # Assume high confidence for grammatically correct commands\n                        raw_transcript=transcript,\n                        timestamp=time.time()\n                    )\n\n        # If no pattern matches, return None (invalid command)\n        return None\n\n    def suggest_corrections(self, invalid_transcript: str) -> List[str]:\n        """\n        Suggest possible corrections for invalid commands.\n        """\n        suggestions = []\n\n        # Simple suggestions based on common mistakes\n        if "g" in invalid_transcript.lower():\n            # Suggest "go" if "g" is detected\n            corrected = invalid_transcript.lower().replace("g ", "go ")\n            if corrected != invalid_transcript.lower():\n                suggestions.append(f"Did you mean: {corrected}?")\n\n        # Add more sophisticated suggestion logic here\n\n        return suggestions\n'})}),"\n",(0,o.jsx)(e.h2,{id:"integration-with-robot-systems",children:"Integration with Robot Systems"}),"\n",(0,o.jsx)(e.h3,{id:"complete-integration-example",children:"Complete Integration Example"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n\n"""\nComplete integration of Whisper with robot control system.\n"""\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist, PoseStamped\nfrom sensor_msgs.msg import AudioData\nimport pyaudio\nimport wave\nimport numpy as np\nimport whisper\nimport torch\nimport tempfile\nimport os\nimport queue\nfrom threading import Thread, Lock\nimport time\n\n\nclass IntegratedWhisperRobot(Node):\n    """\n    Complete integration of Whisper with robot control.\n    """\n\n    def __init__(self):\n        super().__init__(\'integrated_whisper_robot\')\n\n        # Initialize Whisper model\n        self.get_logger().info(\'Loading Whisper model...\')\n        self.model = whisper.load_model("base")\n        self.get_logger().info(\'Whisper model loaded\')\n\n        # Audio parameters\n        self.rate = 16000\n        self.chunk = 1024\n        self.format = pyaudio.paInt16\n        self.channels = 1\n        self.record_seconds = 2\n\n        # Publishers\n        self.voice_cmd_pub = self.create_publisher(String, \'/voice_command\', 10)\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n        self.nav_goal_pub = self.create_publisher(PoseStamped, \'/goal_pose\', 10)\n\n        # Audio processing\n        self.audio = pyaudio.PyAudio()\n        self.audio_queue = queue.Queue()\n        self.processing_lock = Lock()\n\n        # Robot state\n        self.robot_state = {\n            \'current_location\': \'unknown\',\n            \'battery_level\': 100,\n            \'gripper_status\': \'open\'  # or \'closed\'\n        }\n\n        # Start audio capture\n        self.capture_thread = Thread(target=self.capture_audio, daemon=True)\n        self.capture_thread.start()\n\n        # Timer for processing\n        self.process_timer = self.create_timer(0.1, self.process_audio)\n\n        self.get_logger().info(\'Integrated Whisper Robot initialized\')\n\n    def capture_audio(self):\n        """\n        Capture audio from microphone.\n        """\n        stream = self.audio.open(\n            format=self.format,\n            channels=self.channels,\n            rate=self.rate,\n            input=True,\n            frames_per_buffer=self.chunk\n        )\n\n        self.get_logger().info(\'Audio capture started\')\n\n        while rclpy.ok():\n            frames = []\n            for _ in range(0, int(self.rate / self.chunk * self.record_seconds)):\n                data = stream.read(self.chunk)\n                frames.append(data)\n\n            self.audio_queue.put(frames)\n\n        stream.stop_stream()\n        stream.close()\n\n    def process_audio(self):\n        """\n        Process audio from queue.\n        """\n        try:\n            frames = self.audio_queue.get_nowait()\n            self.process_audio_chunk(frames)\n        except queue.Empty:\n            pass\n\n    def process_audio_chunk(self, frames):\n        """\n        Process audio chunk with Whisper.\n        """\n        if self.processing_lock.locked():\n            return\n\n        with self.processing_lock:\n            try:\n                # Convert to audio file for Whisper\n                audio_data = b\'\'.join(frames)\n\n                with tempfile.NamedTemporaryFile(suffix=\'.wav\', delete=False) as temp_file:\n                    wf = wave.open(temp_file.name, \'wb\')\n                    wf.setnchannels(self.channels)\n                    wf.setsampwidth(2)\n                    wf.setframerate(self.rate)\n                    wf.writeframes(audio_data)\n                    wf.close()\n\n                    # Transcribe\n                    result = self.model.transcribe(temp_file.name)\n                    transcript = result["text"].strip()\n\n                    # Clean up\n                    os.unlink(temp_file.name)\n\n                if transcript:\n                    self.get_logger().info(f\'Heard: {transcript}\')\n\n                    # Process command\n                    self.execute_voice_command(transcript)\n\n            except Exception as e:\n                self.get_logger().error(f\'Error processing audio: {e}\')\n\n    def execute_voice_command(self, transcript):\n        """\n        Execute voice command based on transcript.\n        """\n        # Convert to lowercase for processing\n        cmd_lower = transcript.lower()\n\n        # Navigation commands\n        if any(word in cmd_lower for word in [\'go to\', \'move to\', \'navigate to\']):\n            self.navigate_to_location(transcript)\n        elif \'move forward\' in cmd_lower or \'go forward\' in cmd_lower:\n            self.move_forward()\n        elif \'move backward\' in cmd_lower or \'go backward\' in cmd_lower:\n            self.move_backward()\n        elif \'turn left\' in cmd_lower:\n            self.turn_left()\n        elif \'turn right\' in cmd_lower:\n            self.turn_right()\n        elif \'stop\' in cmd_lower:\n            self.stop_robot()\n        # Manipulation commands\n        elif any(word in cmd_lower for word in [\'pick up\', \'grasp\', \'grab\']):\n            self.grasp_object()\n        elif any(word in cmd_lower for word in [\'place\', \'put down\', \'release\']):\n            self.release_object()\n        # Query commands\n        elif \'where are you\' in cmd_lower:\n            self.tell_location()\n        elif \'battery\' in cmd_lower:\n            self.report_battery()\n\n        # Publish command for other nodes\n        cmd_msg = String()\n        cmd_msg.data = transcript\n        self.voice_cmd_pub.publish(cmd_msg)\n\n    def navigate_to_location(self, command):\n        """\n        Navigate to a specific location.\n        """\n        # Extract location from command (simplified)\n        # In practice, use NLP to extract location entities\n        if \'kitchen\' in command.lower():\n            self.send_navigation_goal(-2.0, 1.0, 0.0)  # Example coordinates\n        elif \'living room\' in command.lower():\n            self.send_navigation_goal(1.0, -1.0, 0.0)\n        elif \'bedroom\' in command.lower():\n            self.send_navigation_goal(2.0, 2.0, 0.0)\n\n    def move_forward(self):\n        """Move robot forward."""\n        cmd = Twist()\n        cmd.linear.x = 0.3\n        self.cmd_vel_pub.publish(cmd)\n\n    def move_backward(self):\n        """Move robot backward."""\n        cmd = Twist()\n        cmd.linear.x = -0.3\n        self.cmd_vel_pub.publish(cmd)\n\n    def turn_left(self):\n        """Turn robot left."""\n        cmd = Twist()\n        cmd.angular.z = 0.5\n        self.cmd_vel_pub.publish(cmd)\n\n    def turn_right(self):\n        """Turn robot right."""\n        cmd = Twist()\n        cmd.angular.z = -0.5\n        self.cmd_vel_pub.publish(cmd)\n\n    def stop_robot(self):\n        """Stop robot movement."""\n        cmd = Twist()\n        self.cmd_vel_pub.publish(cmd)\n\n    def grasp_object(self):\n        """Command robot to grasp an object."""\n        self.get_logger().info(\'Grasping object\')\n        # Publish gripper command\n        gripper_cmd = String()\n        gripper_cmd.data = \'close\'\n        # self.gripper_pub.publish(gripper_cmd)  # Assuming gripper publisher exists\n\n    def release_object(self):\n        """Command robot to release an object."""\n        self.get_logger().info(\'Releasing object\')\n        # Publish gripper command\n        gripper_cmd = String()\n        gripper_cmd.data = \'open\'\n        # self.gripper_pub.publish(gripper_cmd)  # Assuming gripper publisher exists\n\n    def tell_location(self):\n        """Report robot\'s current location."""\n        self.get_logger().info(f\'Current location: {self.robot_state["current_location"]}\')\n\n    def report_battery(self):\n        """Report battery level."""\n        self.get_logger().info(f\'Battery level: {self.robot_state["battery_level"]}%\')\n\n    def send_navigation_goal(self, x, y, theta):\n        """Send navigation goal to navigation system."""\n        goal = PoseStamped()\n        goal.header.stamp = self.get_clock().now().to_msg()\n        goal.header.frame_id = \'map\'\n        goal.pose.position.x = x\n        goal.pose.position.y = y\n        goal.pose.position.z = 0.0\n\n        # Convert theta to quaternion\n        from math import sin, cos\n        cy = cos(theta * 0.5)\n        sy = sin(theta * 0.5)\n        goal.pose.orientation.z = sy\n        goal.pose.orientation.w = cy\n\n        self.nav_goal_pub.publish(goal.pose)\n        self.get_logger().info(f\'Navigating to ({x}, {y})\')\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    node = IntegratedWhisperRobot()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info(\'Shutting down...\')\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,o.jsx)(e.h2,{id:"best-practices-for-whisper-integration",children:"Best Practices for Whisper Integration"}),"\n",(0,o.jsx)(e.h3,{id:"performance-considerations",children:"Performance Considerations"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Model Selection"}),": Choose the appropriate model size based on your computational resources"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Audio Quality"}),": Use good quality microphones and audio preprocessing"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Real-time Processing"}),": Implement threading and queuing for non-blocking operation"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Error Handling"}),": Implement robust error handling for various failure modes"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Context Awareness"}),": Maintain conversation context for better command interpretation"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Security"}),": Consider privacy implications of voice data processing"]}),"\n"]}),"\n",(0,o.jsx)(e.p,{children:"Whisper integration with robotics opens up new possibilities for natural human-robot interaction, enabling robots to understand and respond to spoken commands in real-world environments."})]})}function p(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(d,{...n})}):d(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>a,x:()=>s});var i=t(6540);const o={},r=i.createContext(o);function a(n){const e=i.useContext(r);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:a(n.components),i.createElement(r.Provider,{value:e},n.children)}}}]);