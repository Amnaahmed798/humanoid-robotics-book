"use strict";(self.webpackChunkdocusaurus=self.webpackChunkdocusaurus||[]).push([[131],{6570:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>u,frontMatter:()=>r,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module4/multi_modal_interaction","title":"Multi-Modal Interaction: Speech, Gesture, Vision","description":"Introduction","source":"@site/docs/module4/multi_modal_interaction.md","sourceDirName":"module4","slug":"/module4/multi_modal_interaction","permalink":"/humanoid-robotics-book/docs/module4/multi_modal_interaction","draft":false,"unlisted":false,"editUrl":"https://github.com/Amnaahmed798/humanoid-robotics-book/edit/main/docs/module4/multi_modal_interaction.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Cognitive Planning using Large Language Models (LLMs)","permalink":"/humanoid-robotics-book/docs/module4/llm_cognitive_planning"},"next":{"title":"Natural Language Command Translation to ROS 2 Actions","permalink":"/humanoid-robotics-book/docs/module4/nl_to_ros2_actions"}}');var i=t(4848),o=t(8453);const r={},a="Multi-Modal Interaction: Speech, Gesture, Vision",l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Understanding Multi-Modal Interaction",id:"understanding-multi-modal-interaction",level:2},{value:"The Need for Multi-Modal Systems",id:"the-need-for-multi-modal-systems",level:3},{value:"Multi-Modal Architecture",id:"multi-modal-architecture",level:3},{value:"Speech Processing for Multi-Modal Systems",id:"speech-processing-for-multi-modal-systems",level:2},{value:"Speech Recognition and Context Integration",id:"speech-recognition-and-context-integration",level:3},{value:"Vision Processing for Multi-Modal Systems",id:"vision-processing-for-multi-modal-systems",level:2},{value:"Object Detection and Attention",id:"object-detection-and-attention",level:3},{value:"Gesture Recognition and Processing",id:"gesture-recognition-and-processing",level:2},{value:"Hand and Body Gesture Recognition",id:"hand-and-body-gesture-recognition",level:3},{value:"Multi-Modal Fusion and Intent Recognition",id:"multi-modal-fusion-and-intent-recognition",level:2},{value:"Fusion Engine",id:"fusion-engine",level:3},{value:"Integration with ROS 2",id:"integration-with-ros-2",level:2},{value:"ROS 2 Multi-Modal Interface",id:"ros-2-multi-modal-interface",level:3},{value:"Best Practices for Multi-Modal Systems",id:"best-practices-for-multi-modal-systems",level:2},{value:"Design Principles",id:"design-principles",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"multi-modal-interaction-speech-gesture-vision",children:"Multi-Modal Interaction: Speech, Gesture, Vision"})}),"\n",(0,i.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsx)(n.p,{children:"Multi-modal interaction in robotics combines multiple sensory modalities\u2014speech, gesture, and vision\u2014to create more natural and intuitive human-robot interfaces. This chapter explores how to integrate these modalities to enable robots to understand complex human intentions and respond appropriately. By combining multiple input channels, robots can achieve a more human-like understanding of their environment and user commands."}),"\n",(0,i.jsx)(n.h2,{id:"understanding-multi-modal-interaction",children:"Understanding Multi-Modal Interaction"}),"\n",(0,i.jsx)(n.h3,{id:"the-need-for-multi-modal-systems",children:"The Need for Multi-Modal Systems"}),"\n",(0,i.jsx)(n.p,{children:"Single-modality interfaces have limitations:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Speech alone"}),": Can be ambiguous without visual context"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Vision alone"}),": May miss verbal instructions or preferences"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Gesture alone"}),": Limited expressiveness without context"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Multi-modal systems overcome these limitations by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Disambiguation"}),": Resolving ambiguities across modalities"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Context Enhancement"}),": Providing richer situational understanding"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Natural Interaction"}),": Mirroring human communication patterns"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Robustness"}),": Providing redundancy when one modality fails"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"multi-modal-architecture",children:"Multi-Modal Architecture"}),"\n",(0,i.jsx)(n.p,{children:"The multi-modal interaction architecture typically includes:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Speech        \u2502    \u2502   Vision        \u2502    \u2502   Gesture       \u2502\n\u2502   Input         \u2502    \u2502   Input         \u2502    \u2502   Input         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2502                      \u2502                      \u2502\n          \u25bc                      \u25bc                      \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 Speech      \u2502      \u2502 Vision      \u2502      \u2502 Gesture     \u2502\n    \u2502 Processing  \u2502      \u2502 Processing  \u2502      \u2502 Processing  \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2502                      \u2502                      \u2502\n          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                     \u25bc           \u25bc\n            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n            \u2502   Multi-Modal Fusion    \u2502\n            \u2502   and Intent Analysis   \u2502\n            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502\n                           \u25bc\n                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                   \u2502   Action    \u2502\n                   \u2502   Planning  \u2502\n                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502\n                           \u25bc\n                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                   \u2502   Response  \u2502\n                   \u2502   Generation\u2502\n                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,i.jsx)(n.h2,{id:"speech-processing-for-multi-modal-systems",children:"Speech Processing for Multi-Modal Systems"}),"\n",(0,i.jsx)(n.h3,{id:"speech-recognition-and-context-integration",children:"Speech Recognition and Context Integration"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import speech_recognition as sr\nimport asyncio\nimport queue\nfrom dataclasses import dataclass\nfrom typing import Optional, Dict, Any\n\n\n@dataclass\nclass SpeechEvent:\n    """\n    Data structure for speech events with timing and context.\n    """\n    text: str\n    confidence: float\n    timestamp: float\n    speaker_id: Optional[str] = None\n    context: Dict[str, Any] = None\n\n\nclass MultiModalSpeechProcessor:\n    """\n    Speech processor designed for multi-modal interaction.\n    """\n\n    def __init__(self):\n        self.recognizer = sr.Recognizer()\n        self.microphone = sr.Microphone()\n\n        # Adjust for ambient noise\n        with self.microphone as source:\n            self.recognizer.adjust_for_ambient_noise(source)\n\n        # Speech event queue\n        self.speech_queue = queue.Queue()\n\n        # Parameters\n        self.energy_threshold = 400  # Minimum audio energy to consider for recording\n        self.pause_threshold = 0.8   # Seconds of non-speaking audio before a phrase is considered complete\n        self.phrase_threshold = 0.3  # Minimum seconds of speaking audio before we consider the audio a phrase\n\n        # Context information\n        self.current_context = {}\n        self.speaker_tracking = {}\n\n    def start_listening(self):\n        """\n        Start listening for speech with callback.\n        """\n        self.stop_listening = False\n\n        def audio_callback(recognizer, audio):\n            if not self.stop_listening:\n                try:\n                    # Use Google Speech Recognition (alternatively, use offline models)\n                    text = recognizer.recognize_google(audio)\n\n                    # Create speech event with current context\n                    speech_event = SpeechEvent(\n                        text=text,\n                        confidence=0.9,  # Would be obtained from recognizer if supported\n                        timestamp=time.time(),\n                        context=self.current_context.copy()\n                    )\n\n                    self.speech_queue.put(speech_event)\n\n                except sr.UnknownValueError:\n                    # Speech was detected but not understood\n                    pass\n                except sr.RequestError as e:\n                    print(f"Could not request results from speech recognition service; {e}")\n\n        # Start listening in background\n        self.stopper = self.recognizer.listen_in_background(self.microphone, audio_callback)\n\n        print("Listening for speech... Press Ctrl+C to stop.")\n\n    def stop_listening(self):\n        """\n        Stop speech recognition.\n        """\n        if hasattr(self, \'stopper\'):\n            self.stopper()\n        self.stop_listening = True\n\n    def get_speech_events(self):\n        """\n        Get speech events from queue.\n        """\n        events = []\n        try:\n            while True:\n                event = self.speech_queue.get_nowait()\n                events.append(event)\n        except queue.Empty:\n            pass\n        return events\n\n    def update_context(self, context_updates: Dict[str, Any]):\n        """\n        Update context information for speech processing.\n        """\n        self.current_context.update(context_updates)\n\n\nclass AdvancedSpeechProcessor(MultiModalSpeechProcessor):\n    """\n    Advanced speech processor with speaker diarization and emotion detection.\n    """\n\n    def __init__(self):\n        super().__init__()\n\n        # Initialize additional components\n        self.speaker_model = self.load_speaker_model()\n        self.emotion_model = self.load_emotion_model()\n        self.language_model = self.load_language_model()\n\n    def load_speaker_model(self):\n        """\n        Load speaker identification model.\n        """\n        # In practice, this would load a pre-trained speaker identification model\n        # For example, using scikit-learn, pyannote.audio, or similar\n        return None  # Placeholder\n\n    def load_emotion_model(self):\n        """\n        Load emotion detection model.\n        """\n        # In practice, this would load a pre-trained emotion detection model\n        # For example, using librosa for audio features + sklearn/pytorch model\n        return None  # Placeholder\n\n    def load_language_model(self):\n        """\n        Load language model for better understanding.\n        """\n        # This could be a transformer-based model for context understanding\n        return None  # Placeholder\n\n    def process_speech_with_context(self, audio_data, visual_context=None, gesture_context=None):\n        """\n        Process speech with additional context from other modalities.\n        """\n        try:\n            # Recognize speech\n            text = self.recognizer.recognize_google(audio_data)\n\n            # Analyze speaker (if multiple speakers)\n            speaker_id = self.identify_speaker(audio_data)\n\n            # Detect emotion from speech\n            emotion = self.detect_emotion(audio_data)\n\n            # Analyze with language model using context\n            analyzed_text = self.analyze_with_context(\n                text,\n                visual_context=visual_context,\n                gesture_context=gesture_context\n            )\n\n            return SpeechEvent(\n                text=analyzed_text,\n                confidence=0.9,\n                timestamp=time.time(),\n                speaker_id=speaker_id,\n                context={\n                    \'emotion\': emotion,\n                    \'visual_context\': visual_context,\n                    \'gesture_context\': gesture_context\n                }\n            )\n\n        except Exception as e:\n            print(f"Error processing speech: {e}")\n            return None\n\n    def identify_speaker(self, audio_data):\n        """\n        Identify speaker from audio data.\n        """\n        # Implementation would use speaker identification model\n        return "unknown_speaker"  # Placeholder\n\n    def detect_emotion(self, audio_data):\n        """\n        Detect emotion from audio data.\n        """\n        # Implementation would use emotion detection model\n        return "neutral"  # Placeholder\n\n    def analyze_with_context(self, text, visual_context=None, gesture_context=None):\n        """\n        Analyze text with additional context from other modalities.\n        """\n        # This would integrate visual and gesture context to disambiguate speech\n        # For example: "Pick that up" with visual context pointing to specific object\n        return text  # Placeholder\n'})}),"\n",(0,i.jsx)(n.h2,{id:"vision-processing-for-multi-modal-systems",children:"Vision Processing for Multi-Modal Systems"}),"\n",(0,i.jsx)(n.h3,{id:"object-detection-and-attention",children:"Object Detection and Attention"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import cv2\nimport numpy as np\nimport torch\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport time\nfrom dataclasses import dataclass\nfrom typing import List, Tuple, Optional\n\n\n@dataclass\nclass DetectedObject:\n    \"\"\"\n    Data structure for detected objects.\n    \"\"\"\n    id: str\n    class_name: str\n    confidence: float\n    bbox: Tuple[int, int, int, int]  # x1, y1, x2, y2\n    center: Tuple[int, int]  # x, y center\n    color: Optional[str] = None\n    size: Optional[str] = None\n\n\n@dataclass\nclass VisualEvent:\n    \"\"\"\n    Data structure for visual events.\n    \"\"\"\n    objects: List[DetectedObject]\n    attention_regions: List[Tuple[int, int, int, int]]  # x1, y1, x2, y2\n    gaze_direction: Optional[Tuple[float, float]] = None  # x, y direction vector\n    timestamp: float = 0.0\n\n\nclass MultiModalVisionProcessor:\n    \"\"\"\n    Vision processor designed for multi-modal interaction.\n    \"\"\"\n\n    def __init__(self):\n        # Initialize models\n        self.object_detector = self.load_object_detector()\n        self.pose_estimator = self.load_pose_estimator()\n        self.gaze_estimator = self.load_gaze_estimator()\n\n        # Video capture\n        self.cap = cv2.VideoCapture(0)\n        self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n        self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n\n        # Processing parameters\n        self.detection_confidence = 0.5\n        self.max_objects = 10\n\n        # Attention tracking\n        self.attention_history = []\n        self.focus_objects = set()\n\n    def load_object_detector(self):\n        \"\"\"\n        Load object detection model (e.g., YOLO, SSD, etc.).\n        \"\"\"\n        # This would load a pre-trained object detection model\n        # For example, using torchvision.models or YOLOv5\n        try:\n            import torchvision.models.detection as det_models\n            model = det_models.fasterrcnn_resnet50_fpn(pretrained=True)\n            model.eval()\n            return model\n        except ImportError:\n            print(\"Torchvision not available, using placeholder\")\n            return None\n\n    def load_pose_estimator(self):\n        \"\"\"\n        Load human pose estimation model.\n        \"\"\"\n        # This would load a pose estimation model like OpenPose or MediaPipe\n        return None  # Placeholder\n\n    def load_gaze_estimator(self):\n        \"\"\"\n        Load gaze estimation model.\n        \"\"\"\n        # This would load a gaze estimation model\n        return None  # Placeholder\n\n    def capture_frame(self) -> Optional[np.ndarray]:\n        \"\"\"\n        Capture a frame from the camera.\n        \"\"\"\n        ret, frame = self.cap.read()\n        if ret:\n            return frame\n        return None\n\n    def detect_objects(self, frame: np.ndarray) -> List[DetectedObject]:\n        \"\"\"\n        Detect objects in the frame.\n        \"\"\"\n        if self.object_detector is None:\n            # Placeholder implementation\n            return self.placeholder_object_detection(frame)\n\n        # Convert frame to PIL image\n        pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n\n        # Preprocess image\n        transform = transforms.Compose([\n            transforms.ToTensor(),\n        ])\n        input_tensor = transform(pil_image).unsqueeze(0)  # Add batch dimension\n\n        # Perform inference\n        with torch.no_grad():\n            outputs = self.object_detector(input_tensor)\n\n        # Process outputs\n        objects = []\n        if len(outputs) > 0:\n            output = outputs[0]\n            boxes = output['boxes'].cpu().numpy()\n            labels = output['labels'].cpu().numpy()\n            scores = output['scores'].cpu().numpy()\n\n            for i in range(len(scores)):\n                if scores[i] > self.detection_confidence:\n                    bbox = boxes[i].astype(int)\n                    class_name = self.coco_class_names.get(labels[i], f'unknown_{labels[i]}')\n\n                    obj = DetectedObject(\n                        id=f'obj_{i}',\n                        class_name=class_name,\n                        confidence=float(scores[i]),\n                        bbox=(int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3])),\n                        center=((bbox[0] + bbox[2]) // 2, (bbox[1] + bbox[3]) // 2)\n                    )\n                    objects.append(obj)\n\n                    # Limit number of objects\n                    if len(objects) >= self.max_objects:\n                        break\n\n        return objects\n\n    def placeholder_object_detection(self, frame: np.ndarray) -> List[DetectedObject]:\n        \"\"\"\n        Placeholder object detection implementation.\n        \"\"\"\n        # This is a simple color-based detection for demonstration\n        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n\n        # Define color ranges for common objects\n        color_ranges = {\n            'red': ([0, 50, 50], [10, 255, 255]),\n            'blue': ([100, 50, 50], [130, 255, 255]),\n            'green': ([40, 50, 50], [80, 255, 255]),\n        }\n\n        objects = []\n        for color_name, (lower, upper) in color_ranges.items():\n            lower = np.array(lower, dtype=\"uint8\")\n            upper = np.array(upper, dtype=\"uint8\")\n\n            mask = cv2.inRange(hsv, lower, upper)\n            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n            for contour in contours:\n                if cv2.contourArea(contour) > 500:  # Minimum area threshold\n                    x, y, w, h = cv2.boundingRect(contour)\n                    center_x, center_y = x + w // 2, y + h // 2\n\n                    obj = DetectedObject(\n                        id=f'{color_name}_{len(objects)}',\n                        class_name=color_name,\n                        confidence=0.8,\n                        bbox=(x, y, x + w, y + h),\n                        center=(center_x, center_y),\n                        color=color_name\n                    )\n                    objects.append(obj)\n\n        return objects\n\n    @property\n    def coco_class_names(self):\n        \"\"\"\n        COCO dataset class names.\n        \"\"\"\n        return {\n            1: 'person', 2: 'bicycle', 3: 'car', 4: 'motorcycle', 5: 'airplane',\n            6: 'bus', 7: 'train', 8: 'truck', 9: 'boat', 10: 'traffic light',\n            11: 'fire hydrant', 13: 'stop sign', 14: 'parking meter', 15: 'bench',\n            16: 'bird', 17: 'cat', 18: 'dog', 19: 'horse', 20: 'sheep', 21: 'cow',\n            22: 'elephant', 23: 'bear', 24: 'zebra', 25: 'giraffe', 27: 'backpack',\n            28: 'umbrella', 31: 'handbag', 32: 'tie', 33: 'suitcase', 34: 'frisbee',\n            35: 'skis', 36: 'snowboard', 37: 'sports ball', 38: 'kite', 39: 'baseball bat',\n            40: 'baseball glove', 41: 'skateboard', 42: 'surfboard', 43: 'tennis racket',\n            44: 'bottle', 46: 'wine glass', 47: 'cup', 48: 'fork', 49: 'knife', 50: 'spoon',\n            51: 'bowl', 52: 'banana', 53: 'apple', 54: 'sandwich', 55: 'orange',\n            56: 'broccoli', 57: 'carrot', 58: 'hot dog', 59: 'pizza', 60: 'donut',\n            61: 'cake', 62: 'chair', 63: 'couch', 64: 'potted plant', 65: 'bed',\n            67: 'dining table', 70: 'toilet', 72: 'tv', 73: 'laptop', 74: 'mouse',\n            75: 'remote', 76: 'keyboard', 77: 'cell phone', 78: 'microwave', 79: 'oven',\n            80: 'toaster', 81: 'sink', 82: 'refrigerator', 84: 'book', 85: 'clock',\n            86: 'vase', 87: 'scissors', 88: 'teddy bear', 89: 'hair drier', 90: 'toothbrush'\n        }\n\n    def estimate_human_pose(self, frame: np.ndarray) -> Optional[Dict]:\n        \"\"\"\n        Estimate human pose in the frame.\n        \"\"\"\n        # This would use a pose estimation model\n        # For demonstration, return placeholder\n        return None\n\n    def estimate_gaze_direction(self, frame: np.ndarray, face_bbox: Tuple) -> Optional[Tuple[float, float]]:\n        \"\"\"\n        Estimate gaze direction from face region.\n        \"\"\"\n        # This would use a gaze estimation model\n        # For demonstration, return placeholder\n        return None\n\n    def process_frame(self) -> Optional[VisualEvent]:\n        \"\"\"\n        Process a single frame to extract visual information.\n        \"\"\"\n        frame = self.capture_frame()\n        if frame is None:\n            return None\n\n        # Detect objects\n        objects = self.detect_objects(frame)\n\n        # Estimate human pose (if people detected)\n        pose_info = self.estimate_human_pose(frame)\n\n        # Estimate gaze direction (if faces detected)\n        gaze_direction = None\n        if pose_info and 'face' in pose_info:\n            gaze_direction = self.estimate_gaze_direction(frame, pose_info['face'])\n\n        # Identify attention regions (based on detected objects and human pose)\n        attention_regions = self.identify_attention_regions(objects, pose_info)\n\n        # Create visual event\n        visual_event = VisualEvent(\n            objects=objects,\n            attention_regions=attention_regions,\n            gaze_direction=gaze_direction,\n            timestamp=time.time()\n        )\n\n        # Update attention history\n        self.attention_history.append(visual_event)\n\n        # Keep only recent history\n        if len(self.attention_history) > 10:\n            self.attention_history = self.attention_history[-10:]\n\n        return visual_event\n\n    def identify_attention_regions(self, objects: List[DetectedObject], pose_info: Optional[Dict]) -> List[Tuple[int, int, int, int]]:\n        \"\"\"\n        Identify regions of visual attention based on objects and human pose.\n        \"\"\"\n        attention_regions = []\n\n        # If human is detected and looking at specific object\n        if pose_info and 'gaze_target' in pose_info:\n            # Add region around gaze target\n            target_region = pose_info['gaze_target']\n            attention_regions.append(target_region)\n\n        # Add regions around detected objects that are likely to be attended\n        for obj in objects:\n            # Consider size, position, and other factors\n            if obj.confidence > 0.7:  # High confidence detection\n                attention_regions.append(obj.bbox)\n\n        return attention_regions\n\n    def get_attention_context(self) -> Dict:\n        \"\"\"\n        Get current attention context for multi-modal fusion.\n        \"\"\"\n        if not self.attention_history:\n            return {}\n\n        latest_event = self.attention_history[-1]\n\n        context = {\n            'visible_objects': [obj.class_name for obj in latest_event.objects],\n            'focused_object': self.get_focused_object(latest_event),\n            'attention_regions': latest_event.attention_regions,\n            'gaze_direction': latest_event.gaze_direction\n        }\n\n        return context\n\n    def get_focused_object(self, visual_event: VisualEvent) -> Optional[DetectedObject]:\n        \"\"\"\n        Determine which object is currently being focused on.\n        \"\"\"\n        # This would use attention models, gaze estimation, etc.\n        # For now, return the closest object\n        if visual_event.objects:\n            # Sort by y-coordinate (assuming closer objects are lower in frame)\n            closest_obj = min(visual_event.objects, key=lambda obj: obj.center[1])\n            return closest_obj\n\n        return None\n"})}),"\n",(0,i.jsx)(n.h2,{id:"gesture-recognition-and-processing",children:"Gesture Recognition and Processing"}),"\n",(0,i.jsx)(n.h3,{id:"hand-and-body-gesture-recognition",children:"Hand and Body Gesture Recognition"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import mediapipe as mp\nimport numpy as np\nfrom dataclasses import dataclass\nfrom typing import List, Tuple, Optional\n\n\n@dataclass\nclass Gesture:\n    \"\"\"\n    Data structure for recognized gestures.\n    \"\"\"\n    name: str\n    confidence: float\n    landmarks: List[Tuple[float, float, float]]  # x, y, z coordinates\n    gesture_type: str  # 'hand', 'body', 'head', etc.\n    timestamp: float = 0.0\n\n\n@dataclass\nclass GestureEvent:\n    \"\"\"\n    Data structure for gesture events.\n    \"\"\"\n    gesture: Gesture\n    context: Dict = None\n    timestamp: float = 0.0\n\n\nclass MultiModalGestureProcessor:\n    \"\"\"\n    Gesture processor designed for multi-modal interaction.\n    \"\"\"\n\n    def __init__(self):\n        # Initialize MediaPipe components\n        self.mp_hands = mp.solutions.hands\n        self.mp_pose = mp.solutions.pose\n        self.mp_drawing = mp.solutions.drawing_utils\n\n        # Initialize gesture recognizers\n        self.hands = self.mp_hands.Hands(\n            static_image_mode=False,\n            max_num_hands=2,\n            min_detection_confidence=0.5,\n            min_tracking_confidence=0.5\n        )\n\n        self.pose = self.mp_pose.Pose(\n            static_image_mode=False,\n            model_complexity=1,\n            enable_segmentation=False,\n            min_detection_confidence=0.5,\n            min_tracking_confidence=0.5\n        )\n\n        # Gesture vocabulary\n        self.gesture_vocabulary = self.define_gesture_vocabulary()\n\n        # Processing parameters\n        self.min_gesture_confidence = 0.6\n        self.gesture_history = []\n\n    def define_gesture_vocabulary(self) -> Dict[str, Dict]:\n        \"\"\"\n        Define the vocabulary of recognizable gestures.\n        \"\"\"\n        return {\n            'pointing': {\n                'description': 'Index finger extended, other fingers folded',\n                'key_landmarks': [8],  # Tip of index finger\n                'constraints': {\n                    'index_finger_extended': True,\n                    'other_fingers_folded': True\n                }\n            },\n            'thumbs_up': {\n                'description': 'Thumb up, other fingers folded',\n                'key_landmarks': [4, 8],  # Thumb tip and index finger MCP\n                'constraints': {\n                    'thumb_extended': True,\n                    'other_fingers_folded': True\n                }\n            },\n            'wave': {\n                'description': 'Hand waving motion',\n                'key_landmarks': [8, 12],  # Index and middle finger tips\n                'constraints': {\n                    'periodic_motion': True\n                }\n            },\n            'okay': {\n                'description': 'OK hand sign (thumb and index finger touching)',\n                'key_landmarks': [4, 8],  # Thumb tip and index finger tip\n                'constraints': {\n                    'thumb_index_touching': True\n                }\n            },\n            'stop': {\n                'description': 'Palm facing forward (stop sign)',\n                'key_landmarks': [8, 12, 16, 20],  # All finger tips\n                'constraints': {\n                    'fingers_extended': True,\n                    'palm_facing_forward': True\n                }\n            }\n        }\n\n    def recognize_hand_gestures(self, frame: np.ndarray) -> List[Gesture]:\n        \"\"\"\n        Recognize hand gestures from frame.\n        \"\"\"\n        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        results = self.hands.process(rgb_frame)\n\n        gestures = []\n\n        if results.multi_hand_landmarks:\n            for hand_landmarks in results.multi_hand_landmarks:\n                # Extract landmarks\n                landmarks = []\n                for landmark in hand_landmarks.landmark:\n                    landmarks.append((landmark.x, landmark.y, landmark.z))\n\n                # Recognize gesture\n                gesture_name, confidence = self.classify_gesture(landmarks, 'hand')\n\n                if confidence > self.min_gesture_confidence:\n                    gesture = Gesture(\n                        name=gesture_name,\n                        confidence=confidence,\n                        landmarks=landmarks,\n                        gesture_type='hand',\n                        timestamp=time.time()\n                    )\n                    gestures.append(gesture)\n\n                # Draw landmarks\n                self.mp_drawing.draw_landmarks(\n                    frame, hand_landmarks, self.mp_hands.HAND_CONNECTIONS\n                )\n\n        return gestures\n\n    def classify_gesture(self, landmarks: List[Tuple[float, float, float]], gesture_type: str) -> Tuple[str, float]:\n        \"\"\"\n        Classify gesture based on landmarks.\n        \"\"\"\n        if gesture_type == 'hand':\n            # Analyze finger positions and relationships\n            return self.classify_hand_gesture(landmarks)\n\n        return 'unknown', 0.0\n\n    def classify_hand_gesture(self, landmarks: List[Tuple[float, float, float]]) -> Tuple[str, float]:\n        \"\"\"\n        Classify hand gesture based on landmark positions.\n        \"\"\"\n        # Calculate distances between key points\n        if len(landmarks) < 21:  # Not enough landmarks\n            return 'unknown', 0.0\n\n        # Extract specific landmark positions\n        thumb_tip = landmarks[4]\n        index_tip = landmarks[8]\n        middle_tip = landmarks[12]\n        ring_tip = landmarks[16]\n        pinky_tip = landmarks[20]\n\n        index_mcp = landmarks[5]\n        middle_mcp = landmarks[9]\n        ring_mcp = landmarks[13]\n        pinky_mcp = landmarks[17]\n\n        # Calculate distances\n        thumb_index_dist = self.calculate_distance(thumb_tip, index_tip)\n        index_middle_dist = self.calculate_distance(index_tip, middle_tip)\n        middle_ring_dist = self.calculate_distance(middle_tip, ring_tip)\n        ring_pinky_dist = self.calculate_distance(ring_tip, pinky_tip)\n\n        # Classify based on distances and positions\n        if self.is_pointing_gesture(landmarks):\n            return 'pointing', 0.9\n        elif self.is_thumbs_up_gesture(landmarks):\n            return 'thumbs_up', 0.85\n        elif self.is_okay_gesture(landmarks):\n            return 'okay', 0.8\n        elif self.is_stop_gesture(landmarks):\n            return 'stop', 0.85\n        else:\n            return 'unknown', 0.0\n\n    def is_pointing_gesture(self, landmarks: List[Tuple[float, float, float]]) -> bool:\n        \"\"\"\n        Check if landmarks represent a pointing gesture.\n        \"\"\"\n        # Index finger extended, others folded\n        index_tip = landmarks[8]\n        middle_tip = landmarks[12]\n        ring_tip = landmarks[16]\n        pinky_tip = landmarks[20]\n\n        index_mcp = landmarks[5]\n        middle_mcp = landmarks[9]\n        ring_mcp = landmarks[13]\n        pinky_mcp = landmarks[17]\n\n        # Index finger should be extended (tip further from MCP than others)\n        index_extended = self.calculate_distance(index_tip, index_mcp) > 0.1\n        others_folded = (\n            self.calculate_distance(middle_tip, middle_mcp) < 0.08 and\n            self.calculate_distance(ring_tip, ring_mcp) < 0.08 and\n            self.calculate_distance(pinky_tip, pinky_mcp) < 0.08\n        )\n\n        return index_extended and others_folded\n\n    def is_thumbs_up_gesture(self, landmarks: List[Tuple[float, float, float]]) -> bool:\n        \"\"\"\n        Check if landmarks represent a thumbs up gesture.\n        \"\"\"\n        thumb_tip = landmarks[4]\n        index_tip = landmarks[8]\n        middle_tip = landmarks[12]\n        ring_tip = landmarks[16]\n        pinky_tip = landmarks[20]\n\n        thumb_mcp = landmarks[2]\n        index_mcp = landmarks[5]\n        middle_mcp = landmarks[9]\n        ring_mcp = landmarks[13]\n        pinky_mcp = landmarks[17]\n\n        # Thumb extended, others folded\n        thumb_extended = self.calculate_distance(thumb_tip, thumb_mcp) > 0.1\n        others_folded = (\n            self.calculate_distance(index_tip, index_mcp) < 0.08 and\n            self.calculate_distance(middle_tip, middle_mcp) < 0.08 and\n            self.calculate_distance(ring_tip, ring_mcp) < 0.08 and\n            self.calculate_distance(pinky_tip, pinky_mcp) < 0.08\n        )\n\n        return thumb_extended and others_folded\n\n    def is_okay_gesture(self, landmarks: List[Tuple[float, float, float]]) -> bool:\n        \"\"\"\n        Check if landmarks represent an OK gesture.\n        \"\"\"\n        thumb_tip = landmarks[4]\n        index_tip = landmarks[8]\n\n        # Thumb tip and index finger tip close together\n        distance = self.calculate_distance(thumb_tip, index_tip)\n        return distance < 0.05\n\n    def is_stop_gesture(self, landmarks: List[Tuple[float, float, float]]) -> bool:\n        \"\"\"\n        Check if landmarks represent a stop gesture (open palm).\n        \"\"\"\n        index_tip = landmarks[8]\n        middle_tip = landmarks[12]\n        ring_tip = landmarks[16]\n        pinky_tip = landmarks[20]\n\n        index_mcp = landmarks[5]\n        middle_mcp = landmarks[9]\n        ring_mcp = landmarks[13]\n        pinky_mcp = landmarks[17]\n\n        # All fingers extended\n        all_extended = (\n            self.calculate_distance(index_tip, index_mcp) > 0.1 and\n            self.calculate_distance(middle_tip, middle_mcp) > 0.1 and\n            self.calculate_distance(ring_tip, ring_mcp) > 0.1 and\n            self.calculate_distance(pinky_tip, pinky_mcp) > 0.1\n        )\n\n        return all_extended\n\n    def calculate_distance(self, point1: Tuple[float, float, float], point2: Tuple[float, float, float]) -> float:\n        \"\"\"\n        Calculate Euclidean distance between two 3D points.\n        \"\"\"\n        return np.sqrt(sum((a - b) ** 2 for a, b in zip(point1, point2)))\n\n    def recognize_body_gestures(self, frame: np.ndarray) -> List[Gesture]:\n        \"\"\"\n        Recognize body gestures from frame.\n        \"\"\"\n        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        results = self.pose.process(rgb_frame)\n\n        gestures = []\n\n        if results.pose_landmarks:\n            # Extract body landmarks\n            landmarks = []\n            for landmark in results.pose_landmarks.landmark:\n                landmarks.append((landmark.x, landmark.y, landmark.z))\n\n            # Recognize body gesture (simplified)\n            gesture_name, confidence = self.classify_body_gesture(landmarks)\n\n            if confidence > self.min_gesture_confidence:\n                gesture = Gesture(\n                    name=gesture_name,\n                    confidence=confidence,\n                    landmarks=landmarks,\n                    gesture_type='body',\n                    timestamp=time.time()\n                )\n                gestures.append(gesture)\n\n            # Draw pose landmarks\n            self.mp_drawing.draw_landmarks(\n                frame, results.pose_landmarks, self.mp_pose.POSE_CONNECTIONS\n            )\n\n        return gestures\n\n    def classify_body_gesture(self, landmarks: List[Tuple[float, float, float]]) -> Tuple[str, float]:\n        \"\"\"\n        Classify body gesture based on landmarks.\n        \"\"\"\n        # Simplified body gesture classification\n        # In practice, this would use more sophisticated analysis\n\n        # Example: Raise hand gesture\n        nose = landmarks[0]  # Nose landmark\n        left_wrist = landmarks[15]  # Left wrist\n        right_wrist = landmarks[16]  # Right wrist\n\n        # Check if either hand is raised above head\n        if left_wrist[1] < nose[1] or right_wrist[1] < nose[1]:  # Y-axis is inverted in MediaPipe\n            return 'raise_hand', 0.8\n\n        return 'unknown', 0.0\n\n    def process_frame(self, frame: np.ndarray) -> List[GestureEvent]:\n        \"\"\"\n        Process frame to extract gestures.\n        \"\"\"\n        # Recognize hand gestures\n        hand_gestures = self.recognize_hand_gestures(frame)\n\n        # Recognize body gestures\n        body_gestures = self.recognize_body_gestures(frame)\n\n        # Combine all gestures\n        all_gestures = hand_gestures + body_gestures\n\n        # Create gesture events\n        gesture_events = []\n        for gesture in all_gestures:\n            event = GestureEvent(\n                gesture=gesture,\n                timestamp=time.time()\n            )\n            gesture_events.append(event)\n\n            # Update gesture history\n            self.gesture_history.append(event)\n\n        # Keep only recent history\n        if len(self.gesture_history) > 20:\n            self.gesture_history = self.gesture_history[-20:]\n\n        return gesture_events\n\n    def get_gesture_context(self) -> Dict:\n        \"\"\"\n        Get current gesture context for multi-modal fusion.\n        \"\"\"\n        if not self.gesture_history:\n            return {}\n\n        recent_gestures = self.gesture_history[-5:]  # Last 5 gestures\n\n        context = {\n            'recent_gestures': [(g.gesture.name, g.gesture.confidence) for g in recent_gestures],\n            'dominant_gesture': self.get_dominant_gesture(recent_gestures),\n            'gesture_sequence': [g.gesture.name for g in recent_gestures]\n        }\n\n        return context\n\n    def get_dominant_gesture(self, recent_gestures: List[GestureEvent]) -> Optional[str]:\n        \"\"\"\n        Determine the dominant gesture from recent history.\n        \"\"\"\n        if not recent_gestures:\n            return None\n\n        # Group by gesture name and find most confident\n        gesture_counts = {}\n        gesture_confidences = {}\n\n        for event in recent_gestures:\n            name = event.gesture.name\n            if name not in gesture_counts:\n                gesture_counts[name] = 0\n                gesture_confidences[name] = 0.0\n\n            gesture_counts[name] += 1\n            gesture_confidences[name] = max(gesture_confidences[name], event.gesture.confidence)\n\n        # Find gesture with highest confidence\n        dominant_gesture = max(gesture_confidences.keys(), key=lambda x: gesture_confidences[x])\n        return dominant_gesture\n"})}),"\n",(0,i.jsx)(n.h2,{id:"multi-modal-fusion-and-intent-recognition",children:"Multi-Modal Fusion and Intent Recognition"}),"\n",(0,i.jsx)(n.h3,{id:"fusion-engine",children:"Fusion Engine"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import threading\nfrom typing import Dict, Any, Optional\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass MultiModalEvent:\n    \"\"\"\n    Combined event from multiple modalities.\n    \"\"\"\n    speech_event: Optional[SpeechEvent] = None\n    visual_event: Optional[VisualEvent] = None\n    gesture_event: Optional[GestureEvent] = None\n    timestamp: float = 0.0\n    fused_intent: Optional[str] = None\n    confidence: float = 0.0\n\n\nclass MultiModalFusionEngine:\n    \"\"\"\n    Fuses information from multiple modalities to recognize intent.\n    \"\"\"\n\n    def __init__(self):\n        # Queues for different modalities\n        self.speech_queue = queue.Queue()\n        self.visual_queue = queue.Queue()\n        self.gesture_queue = queue.Queue()\n\n        # Event synchronization\n        self.speech_event = None\n        self.visual_event = None\n        self.gesture_event = None\n\n        # Fusion parameters\n        self.temporal_window = 2.0  # seconds to consider for fusion\n        self.confidence_weights = {\n            'speech': 0.5,\n            'visual': 0.3,\n            'gesture': 0.2\n        }\n\n        # Intent vocabulary\n        self.intent_vocabulary = self.define_intent_vocabulary()\n\n    def define_intent_vocabulary(self) -> Dict[str, Dict]:\n        \"\"\"\n        Define the vocabulary of possible intents.\n        \"\"\"\n        return {\n            'navigation': {\n                'keywords': ['go', 'move', 'navigate', 'to', 'toward'],\n                'gestures': ['pointing'],\n                'visual_context': ['location', 'waypoint']\n            },\n            'manipulation': {\n                'keywords': ['pick', 'grasp', 'take', 'lift', 'put', 'place', 'drop'],\n                'gestures': ['pointing', 'grasping_motion'],\n                'visual_context': ['object', 'graspable']\n            },\n            'detection': {\n                'keywords': ['find', 'locate', 'look', 'see', 'where', 'is'],\n                'gestures': ['pointing', 'searching_motion'],\n                'visual_context': ['object', 'search_area']\n            },\n            'greeting': {\n                'keywords': ['hello', 'hi', 'hey', 'good morning', 'good evening'],\n                'gestures': ['wave', 'nod'],\n                'visual_context': ['person', 'face']\n            },\n            'affirmation': {\n                'keywords': ['yes', 'yeah', 'sure', 'ok', 'okay', 'alright'],\n                'gestures': ['thumbs_up', 'nod'],\n                'visual_context': ['interaction']\n            },\n            'negation': {\n                'keywords': ['no', 'nope', 'negative', 'stop', 'cancel'],\n                'gestures': ['stop', 'shake_head'],\n                'visual_context': ['ongoing_action']\n            }\n        }\n\n    def add_speech_event(self, event: SpeechEvent):\n        \"\"\"\n        Add speech event to fusion engine.\n        \"\"\"\n        self.speech_queue.put(event)\n        self.speech_event = event\n\n    def add_visual_event(self, event: VisualEvent):\n        \"\"\"\n        Add visual event to fusion engine.\n        \"\"\"\n        self.visual_queue.put(event)\n        self.visual_event = event\n\n    def add_gesture_event(self, event: GestureEvent):\n        \"\"\"\n        Add gesture event to fusion engine.\n        \"\"\"\n        self.gesture_queue.put(event)\n        self.gesture_event = event\n\n    def fuse_modalities(self) -> Optional[MultiModalEvent]:\n        \"\"\"\n        Fuse information from all modalities to recognize intent.\n        \"\"\"\n        # Get recent events within temporal window\n        speech_event = self.get_recent_speech_event()\n        visual_event = self.get_recent_visual_event()\n        gesture_event = self.get_recent_gesture_event()\n\n        if not any([speech_event, visual_event, gesture_event]):\n            return None\n\n        # Perform fusion\n        fused_intent, confidence = self.perform_fusion(\n            speech_event, visual_event, gesture_event\n        )\n\n        # Create multi-modal event\n        mm_event = MultiModalEvent(\n            speech_event=speech_event,\n            visual_event=visual_event,\n            gesture_event=gesture_event,\n            timestamp=time.time(),\n            fused_intent=fused_intent,\n            confidence=confidence\n        )\n\n        return mm_event\n\n    def get_recent_speech_event(self) -> Optional[SpeechEvent]:\n        \"\"\"\n        Get speech event within temporal window.\n        \"\"\"\n        recent_events = []\n        current_time = time.time()\n\n        # Get all events from queue\n        while not self.speech_queue.empty():\n            try:\n                event = self.speech_queue.get_nowait()\n                if current_time - event.timestamp <= self.temporal_window:\n                    recent_events.append(event)\n            except queue.Empty:\n                break\n\n        # Return most recent or combine if needed\n        if recent_events:\n            return recent_events[-1]\n\n        return self.speech_event if self.speech_event else None\n\n    def get_recent_visual_event(self) -> Optional[VisualEvent]:\n        \"\"\"\n        Get visual event within temporal window.\n        \"\"\"\n        recent_events = []\n        current_time = time.time()\n\n        # Get all events from queue\n        while not self.visual_queue.empty():\n            try:\n                event = self.visual_queue.get_nowait()\n                if current_time - event.timestamp <= self.temporal_window:\n                    recent_events.append(event)\n            except queue.Empty:\n                break\n\n        # Return most recent\n        if recent_events:\n            return recent_events[-1]\n\n        return self.visual_event if self.visual_event else None\n\n    def get_recent_gesture_event(self) -> Optional[GestureEvent]:\n        \"\"\"\n        Get gesture event within temporal window.\n        \"\"\"\n        recent_events = []\n        current_time = time.time()\n\n        # Get all events from queue\n        while not self.gesture_queue.empty():\n            try:\n                event = self.gesture_queue.get_nowait()\n                if current_time - event.timestamp <= self.temporal_window:\n                    recent_events.append(event)\n            except queue.Empty:\n                break\n\n        # Return most recent\n        if recent_events:\n            return recent_events[-1]\n\n        return self.gesture_event if self.gesture_event else None\n\n    def perform_fusion(self, speech_event: Optional[SpeechEvent],\n                      visual_event: Optional[VisualEvent],\n                      gesture_event: Optional[GestureEvent]) -> Tuple[str, float]:\n        \"\"\"\n        Perform multi-modal fusion to recognize intent.\n        \"\"\"\n        # Extract information from each modality\n        speech_info = self.extract_speech_info(speech_event)\n        visual_info = self.extract_visual_info(visual_event)\n        gesture_info = self.extract_gesture_info(gesture_event)\n\n        # Calculate confidence scores for each intent\n        intent_scores = {}\n\n        for intent, definition in self.intent_vocabulary.items():\n            score = 0.0\n            total_weight = 0.0\n\n            # Speech contribution\n            if speech_info:\n                speech_match = self.match_keywords(speech_info, definition['keywords'])\n                score += speech_match * self.confidence_weights['speech']\n                total_weight += self.confidence_weights['speech']\n\n            # Visual contribution\n            if visual_info:\n                visual_match = self.match_visual_context(visual_info, definition['visual_context'])\n                score += visual_match * self.confidence_weights['visual']\n                total_weight += self.confidence_weights['visual']\n\n            # Gesture contribution\n            if gesture_info:\n                gesture_match = self.match_gestures(gesture_info, definition['gestures'])\n                score += gesture_match * self.confidence_weights['gesture']\n                total_weight += self.confidence_weights['gesture']\n\n            # Normalize by total weight\n            if total_weight > 0:\n                intent_scores[intent] = score / total_weight\n            else:\n                intent_scores[intent] = 0.0\n\n        # Find intent with highest score\n        if intent_scores:\n            best_intent = max(intent_scores.keys(), key=lambda x: intent_scores[x])\n            best_confidence = intent_scores[best_intent]\n            return best_intent, best_confidence\n\n        return 'unknown', 0.0\n\n    def extract_speech_info(self, speech_event: Optional[SpeechEvent]) -> Dict[str, Any]:\n        \"\"\"\n        Extract relevant information from speech event.\n        \"\"\"\n        if not speech_event:\n            return {}\n\n        text = speech_event.text.lower()\n        words = text.split()\n\n        return {\n            'text': text,\n            'words': words,\n            'original_text': speech_event.text,\n            'confidence': speech_event.confidence\n        }\n\n    def extract_visual_info(self, visual_event: Optional[VisualEvent]) -> Dict[str, Any]:\n        \"\"\"\n        Extract relevant information from visual event.\n        \"\"\"\n        if not visual_event:\n            return {}\n\n        objects = [obj.class_name for obj in visual_event.objects]\n        object_names = list(set(objects))  # Unique object names\n\n        return {\n            'objects': object_names,\n            'object_count': len(visual_event.objects),\n            'attention_regions': visual_event.attention_regions,\n            'gaze_direction': visual_event.gaze_direction\n        }\n\n    def extract_gesture_info(self, gesture_event: Optional[GestureEvent]) -> Dict[str, Any]:\n        \"\"\"\n        Extract relevant information from gesture event.\n        \"\"\"\n        if not gesture_event:\n            return {}\n\n        return {\n            'gesture_name': gesture_event.gesture.name,\n            'gesture_confidence': gesture_event.gesture.confidence,\n            'gesture_type': gesture_event.gesture.gesture_type\n        }\n\n    def match_keywords(self, speech_info: Dict, keywords: List[str]) -> float:\n        \"\"\"\n        Match speech against keywords to determine relevance.\n        \"\"\"\n        if not speech_info or not keywords:\n            return 0.0\n\n        text = speech_info['text']\n        confidence = speech_info.get('confidence', 1.0)\n\n        matches = 0\n        total_keywords = len(keywords)\n\n        for keyword in keywords:\n            if keyword in text:\n                matches += 1\n\n        match_ratio = matches / total_keywords if total_keywords > 0 else 0.0\n        return match_ratio * confidence\n\n    def match_visual_context(self, visual_info: Dict, context_keywords: List[str]) -> float:\n        \"\"\"\n        Match visual information against context keywords.\n        \"\"\"\n        if not visual_info or not context_keywords:\n            return 0.0\n\n        objects = visual_info.get('objects', [])\n        attention_regions = visual_info.get('attention_regions', [])\n\n        matches = 0\n        total_keywords = len(context_keywords)\n\n        for keyword in context_keywords:\n            # Check if keyword matches any detected objects\n            for obj in objects:\n                if keyword in obj.lower():\n                    matches += 1\n                    break\n\n            # Check if keyword relates to context\n            if keyword in ['location', 'waypoint'] and attention_regions:\n                matches += 1\n\n        match_ratio = matches / total_keywords if total_keywords > 0 else 0.0\n        return match_ratio\n\n    def match_gestures(self, gesture_info: Dict, allowed_gestures: List[str]) -> float:\n        \"\"\"\n        Match gesture against allowed gestures.\n        \"\"\"\n        if not gesture_info or not allowed_gestures:\n            return 0.0\n\n        gesture_name = gesture_info.get('gesture_name', '')\n        gesture_confidence = gesture_info.get('gesture_confidence', 1.0)\n\n        if gesture_name in allowed_gestures:\n            return gesture_confidence\n\n        return 0.0\n\n\nclass MultiModalInteractionManager:\n    \"\"\"\n    Main manager for multi-modal interaction system.\n    \"\"\"\n\n    def __init__(self):\n        # Initialize processors\n        self.speech_processor = AdvancedSpeechProcessor()\n        self.vision_processor = MultiModalVisionProcessor()\n        self.gesture_processor = MultiModalGestureProcessor()\n        self.fusion_engine = MultiModalFusionEngine()\n\n        # Threading for concurrent processing\n        self.running = False\n        self.speech_thread = None\n        self.vision_thread = None\n        self.gesture_thread = None\n\n        # Callback for fused intents\n        self.intent_callback = None\n\n    def start_interaction(self):\n        \"\"\"\n        Start multi-modal interaction system.\n        \"\"\"\n        self.running = True\n\n        # Start speech processing\n        self.speech_thread = threading.Thread(target=self.speech_processing_loop, daemon=True)\n        self.speech_thread.start()\n\n        # Start vision processing\n        self.vision_thread = threading.Thread(target=self.vision_processing_loop, daemon=True)\n        self.vision_thread.start()\n\n        # Start gesture processing\n        self.gesture_thread = threading.Thread(target=self.gesture_processing_loop, daemon=True)\n        self.gesture_thread.start()\n\n        # Start fusion processing\n        self.fusion_thread = threading.Thread(target=self.fusion_processing_loop, daemon=True)\n        self.fusion_thread.start()\n\n        print(\"Multi-modal interaction system started\")\n\n    def stop_interaction(self):\n        \"\"\"\n        Stop multi-modal interaction system.\n        \"\"\"\n        self.running = False\n        print(\"Stopping multi-modal interaction system...\")\n\n    def set_intent_callback(self, callback):\n        \"\"\"\n        Set callback for when fused intents are recognized.\n        \"\"\"\n        self.intent_callback = callback\n\n    def speech_processing_loop(self):\n        \"\"\"\n        Continuously process speech input.\n        \"\"\"\n        self.speech_processor.start_listening()\n\n        while self.running:\n            speech_events = self.speech_processor.get_speech_events()\n            for event in speech_events:\n                self.fusion_engine.add_speech_event(event)\n                time.sleep(0.01)  # Small delay to prevent busy waiting\n\n        self.speech_processor.stop_listening()\n\n    def vision_processing_loop(self):\n        \"\"\"\n        Continuously process visual input.\n        \"\"\"\n        while self.running:\n            visual_event = self.vision_processor.process_frame()\n            if visual_event:\n                self.fusion_engine.add_visual_event(visual_event)\n\n            time.sleep(0.1)  # ~10 FPS processing\n\n    def gesture_processing_loop(self):\n        \"\"\"\n        Continuously process gesture input.\n        \"\"\"\n        cap = cv2.VideoCapture(0)\n\n        while self.running:\n            ret, frame = cap.read()\n            if ret:\n                gesture_events = self.gesture_processor.process_frame(frame)\n                for event in gesture_events:\n                    self.fusion_engine.add_gesture_event(event)\n\n            time.sleep(0.05)  # ~20 FPS processing\n\n        cap.release()\n\n    def fusion_processing_loop(self):\n        \"\"\"\n        Continuously perform multi-modal fusion.\n        \"\"\"\n        while self.running:\n            mm_event = self.fusion_engine.fuse_modalities()\n            if mm_event and mm_event.fused_intent:\n                print(f\"Fused intent: {mm_event.fused_intent} (confidence: {mm_event.confidence:.2f})\")\n\n                # Call intent callback if set\n                if self.intent_callback:\n                    self.intent_callback(mm_event.fused_intent, mm_event.confidence, mm_event)\n\n            time.sleep(0.05)  # ~20 fusion cycles per second\n\n    def get_current_context(self) -> Dict[str, Any]:\n        \"\"\"\n        Get current multi-modal context.\n        \"\"\"\n        return {\n            'speech_context': getattr(self.speech_processor, 'current_context', {}),\n            'visual_context': self.vision_processor.get_attention_context(),\n            'gesture_context': self.gesture_processor.get_gesture_context()\n        }\n\n\ndef intent_handler(intent: str, confidence: float, mm_event: MultiModalEvent):\n    \"\"\"\n    Example intent handler function.\n    \"\"\"\n    print(f\"Handling intent: {intent} with confidence {confidence}\")\n\n    # In a real system, this would translate the intent to robot actions\n    if intent == 'navigation':\n        print(\"Robot should navigate to specified location\")\n    elif intent == 'manipulation':\n        print(\"Robot should manipulate specified object\")\n    elif intent == 'detection':\n        print(\"Robot should detect specified object\")\n    elif intent == 'greeting':\n        print(\"Robot should greet the person\")\n    elif intent == 'affirmation':\n        print(\"Robot should acknowledge positively\")\n    elif intent == 'negation':\n        print(\"Robot should acknowledge negatively\")\n\n\ndef main():\n    \"\"\"\n    Main function to demonstrate multi-modal interaction.\n    \"\"\"\n    manager = MultiModalInteractionManager()\n\n    # Set intent handler\n    manager.set_intent_callback(intent_handler)\n\n    print(\"Starting multi-modal interaction system...\")\n    print(\"Speak, gesture, or show objects to the camera to interact.\")\n    print(\"Press Ctrl+C to stop.\")\n\n    try:\n        manager.start_interaction()\n\n        # Keep running until interrupted\n        while True:\n            time.sleep(1)\n\n    except KeyboardInterrupt:\n        print(\"\\nShutting down...\")\n        manager.stop_interaction()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.h2,{id:"integration-with-ros-2",children:"Integration with ROS 2"}),"\n",(0,i.jsx)(n.h3,{id:"ros-2-multi-modal-interface",children:"ROS 2 Multi-Modal Interface"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String, Float32\nfrom sensor_msgs.msg import Image\nfrom geometry_msgs.msg import Point\nfrom multi_modal_msgs.msg import MultiModalEvent, IntentRecognition  # Custom message types\n\n\nclass MultiModalROSInterface(Node):\n    """\n    ROS 2 interface for multi-modal interaction system.\n    """\n\n    def __init__(self):\n        super().__init__(\'multi_modal_interface\')\n\n        # Publishers\n        self.intent_pub = self.create_publisher(IntentRecognition, \'/multi_modal/intent\', 10)\n        self.event_pub = self.create_publisher(MultiModalEvent, \'/multi_modal/event\', 10)\n        self.feedback_pub = self.create_publisher(String, \'/multi_modal/feedback\', 10)\n\n        # Subscribers\n        self.speech_sub = self.create_subscription(\n            String,\n            \'/speech_recognition/text\',\n            self.speech_callback,\n            10\n        )\n\n        self.image_sub = self.create_subscription(\n            Image,\n            \'/camera/image_raw\',\n            self.image_callback,\n            10\n        )\n\n        # Initialize multi-modal manager\n        self.mm_manager = MultiModalInteractionManager()\n        self.mm_manager.set_intent_callback(self.ros_intent_handler)\n\n        # Start multi-modal system\n        self.mm_manager_thread = threading.Thread(target=self.mm_manager.start_interaction, daemon=True)\n        self.mm_manager_thread.start()\n\n        self.get_logger().info(\'Multi-Modal ROS Interface initialized\')\n\n    def speech_callback(self, msg):\n        """\n        Handle speech input from ROS 2 topic.\n        """\n        # Convert ROS String to SpeechEvent and add to fusion engine\n        speech_event = SpeechEvent(\n            text=msg.data,\n            confidence=0.9,  # Would come from speech recognition confidence\n            timestamp=self.get_clock().now().nanoseconds / 1e9\n        )\n        self.mm_manager.fusion_engine.add_speech_event(speech_event)\n\n    def image_callback(self, msg):\n        """\n        Handle image input from ROS 2 topic.\n        """\n        # Convert ROS Image to OpenCV format\n        # This would require cv_bridge in real implementation\n        pass\n\n    def ros_intent_handler(self, intent: str, confidence: float, mm_event: MultiModalEvent):\n        """\n        Handle fused intents in ROS 2 context.\n        """\n        # Publish intent recognition\n        intent_msg = IntentRecognition()\n        intent_msg.intent = intent\n        intent_msg.confidence = confidence\n        intent_msg.timestamp = self.get_clock().now().to_msg()\n\n        self.intent_pub.publish(intent_msg)\n\n        # Publish detailed multi-modal event\n        event_msg = MultiModalEvent()\n        event_msg.speech_text = mm_event.speech_event.text if mm_event.speech_event else ""\n        event_msg.visual_objects = [obj.class_name for obj in mm_event.visual_event.objects] if mm_event.visual_event else []\n        event_msg.gesture_name = mm_event.gesture_event.gesture.name if mm_event.gesture_event else ""\n        event_msg.fused_intent = mm_event.fused_intent\n        event_msg.confidence = mm_event.confidence\n        event_msg.header.stamp = self.get_clock().now().to_msg()\n\n        self.event_pub.publish(event_msg)\n\n        self.get_logger().info(f\'Fused intent: {intent} (confidence: {confidence:.2f})\')\n\n    def publish_feedback(self, feedback_text: str):\n        """\n        Publish feedback to user.\n        """\n        feedback_msg = String()\n        feedback_msg.data = feedback_text\n        self.feedback_pub.publish(feedback_msg)\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    node = MultiModalROSInterface()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info(\'Shutting down multi-modal interface...\')\n    finally:\n        node.mm_manager.stop_interaction()\n        node.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"best-practices-for-multi-modal-systems",children:"Best Practices for Multi-Modal Systems"}),"\n",(0,i.jsx)(n.h3,{id:"design-principles",children:"Design Principles"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Temporal Synchronization"}),": Align events from different modalities in time"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Confidence Integration"}),": Combine confidences from different modalities appropriately"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Context Awareness"}),": Use context to disambiguate inputs"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Robustness"}),": Handle failure of individual modalities gracefully"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Latency Management"}),": Optimize processing for real-time interaction"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"User Feedback"}),": Provide clear feedback about system understanding"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Privacy Considerations"}),": Respect privacy when processing audio/video"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Calibration"}),": Calibrate sensors and models regularly"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Multi-modal interaction systems enable robots to understand human intentions through natural communication channels, creating more intuitive and effective human-robot collaboration. By combining speech, gesture, and vision, robots can achieve a more human-like understanding of their environment and user commands."})]})}function u(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>a});var s=t(6540);const i={},o=s.createContext(i);function r(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);