"use strict";(self.webpackChunkdocusaurus=self.webpackChunkdocusaurus||[]).push([[583],{8453:(n,e,t)=>{t.d(e,{R:()=>o,x:()=>r});var a=t(6540);const s={},i=a.createContext(s);function o(n){const e=a.useContext(i);return a.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:o(n.components),a.createElement(i.Provider,{value:e},n.children)}},9906:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>o,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"module4/capstone_project","title":"Capstone Project: Autonomous Humanoid Robot","description":"Introduction","source":"@site/docs/module4/capstone_project.md","sourceDirName":"module4","slug":"/module4/capstone_project","permalink":"/humanoid-robotics-book/docs/module4/capstone_project","draft":false,"unlisted":false,"editUrl":"https://github.com/Amnaahmed798/humanoid-robotics-book/edit/main/docs/module4/capstone_project.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Visual SLAM (VSLAM) and Navigation using Isaac ROS","permalink":"/humanoid-robotics-book/docs/module3/vslam_navigation"},"next":{"title":"Module 4: Conclusion - Autonomous Humanoid Robotics","permalink":"/humanoid-robotics-book/docs/module4/conclusion"}}');var s=t(4848),i=t(8453);const o={},r="Capstone Project: Autonomous Humanoid Robot",l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Project Overview",id:"project-overview",level:2},{value:"Objectives",id:"objectives",level:3},{value:"Technical Requirements",id:"technical-requirements",level:3},{value:"System Architecture",id:"system-architecture",level:2},{value:"High-Level Architecture",id:"high-level-architecture",level:3},{value:"Implementation Plan",id:"implementation-plan",level:2},{value:"Phase 1: Environment Setup and Robot Configuration",id:"phase-1-environment-setup-and-robot-configuration",level:3},{value:"Phase 2: Perception System Integration",id:"phase-2-perception-system-integration",level:3},{value:"Phase 3: Cognitive Planning and Natural Language Integration",id:"phase-3-cognitive-planning-and-natural-language-integration",level:3},{value:"Phase 4: Integration and Testing",id:"phase-4-integration-and-testing",level:3},{value:"Testing and Validation",id:"testing-and-validation",level:2},{value:"Test Scenarios",id:"test-scenarios",level:3},{value:"Performance Metrics",id:"performance-metrics",level:3},{value:"Conclusion",id:"conclusion",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"capstone-project-autonomous-humanoid-robot",children:"Capstone Project: Autonomous Humanoid Robot"})}),"\n",(0,s.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsx)(e.p,{children:"The capstone project brings together all the concepts learned in this book to create a fully autonomous humanoid robot system. This project integrates ROS 2, simulation environments, AI perception and planning, and multi-modal interaction to create a robot that can understand natural language commands, navigate complex environments, manipulate objects, and interact naturally with humans."}),"\n",(0,s.jsx)(e.h2,{id:"project-overview",children:"Project Overview"}),"\n",(0,s.jsx)(e.h3,{id:"objectives",children:"Objectives"}),"\n",(0,s.jsx)(e.p,{children:"The autonomous humanoid robot will be capable of:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Understanding and responding to natural language commands"}),"\n",(0,s.jsx)(e.li,{children:"Navigating to specified locations in indoor environments"}),"\n",(0,s.jsx)(e.li,{children:"Detecting and manipulating objects using vision and manipulation"}),"\n",(0,s.jsx)(e.li,{children:"Engaging in multi-modal interaction (speech, gesture, vision)"}),"\n",(0,s.jsx)(e.li,{children:"Executing complex, multi-step tasks"}),"\n",(0,s.jsx)(e.li,{children:"Demonstrating autonomous behavior in realistic scenarios"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"technical-requirements",children:"Technical Requirements"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Hardware"}),": Simulated humanoid robot with 2 arms, mobile base, and sensors"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Software"}),": ROS 2 Humble with Isaac ROS packages"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Sensors"}),": RGB-D camera, IMU, LiDAR, joint encoders"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Actuators"}),": 7-DOF arms, mobile base, grippers"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"AI Components"}),": LLM for planning, VSLAM for navigation, object recognition"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"system-architecture",children:"System Architecture"}),"\n",(0,s.jsx)(e.h3,{id:"high-level-architecture",children:"High-Level Architecture"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        USER INTERFACE                           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Natural Language    \u2502  Multi-Modal    \u2502  Visual Interface    \u2502\n\u2502  Commands           \u2502  Interaction    \u2502  (RViz, Dashboard)   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2502                      \u2502                  \u2502\n              \u25bc                      \u25bc                  \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     COGNITIVE PLANNING                          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  LLM-Based         \u2502  Task Planning    \u2502  Motion Planning     \u2502\n\u2502  Command Parser    \u2502  & Sequencing    \u2502  & Trajectory Gen    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2502                      \u2502                  \u2502\n              \u25bc                      \u25bc                  \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   EXECUTION SYSTEM                              \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Navigation        \u2502  Manipulation    \u2502  Perception          \u2502\n\u2502  Stack (Nav2)     \u2502  Stack          \u2502  Stack               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2502                      \u2502                  \u2502\n              \u25bc                      \u25bc                  \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   ROBOT PLATFORM                                \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Humanoid Robot   \u2502  Sensors        \u2502  Actuators           \u2502\n\u2502  (Isaac Sim)      \u2502  (Cameras,      \u2502  (Motors, Grippers)  \u2502\n\u2502                   \u2502   LiDAR, IMU)   \u2502                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,s.jsx)(e.h2,{id:"implementation-plan",children:"Implementation Plan"}),"\n",(0,s.jsx)(e.h3,{id:"phase-1-environment-setup-and-robot-configuration",children:"Phase 1: Environment Setup and Robot Configuration"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"# capstone_project/launch/humanoid_robot.launch.py\nfrom launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument, IncludeLaunchDescription\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom launch.substitutions import PathJoinSubstitution, LaunchConfiguration\nfrom launch_ros.actions import Node\nfrom launch_ros.substitutions import FindPackageShare\n\n\ndef generate_launch_description():\n    # Declare launch arguments\n    world = DeclareLaunchArgument(\n        'world',\n        default_value='autonomous_house',\n        description='Choose one of the world files from `/worlds`'\n    )\n\n    # Launch Gazebo with humanoid robot\n    gazebo = IncludeLaunchDescription(\n        PythonLaunchDescriptionSource([\n            PathJoinSubstitution([\n                FindPackageShare('gazebo_ros'),\n                'launch',\n                'gazebo.launch.py'\n            ])\n        ]),\n        launch_arguments={\n            'world': PathJoinSubstitution([\n                FindPackageShare('capstone_project'),\n                'worlds',\n                LaunchConfiguration('world') + '.world'\n            ])\n        }.items()\n    )\n\n    # Launch humanoid robot\n    robot_spawn = Node(\n        package='gazebo_ros',\n        executable='spawn_entity.py',\n        arguments=[\n            '-entity', 'humanoid_robot',\n            '-file', PathJoinSubstitution([\n                FindPackageShare('capstone_project'),\n                'models',\n                'humanoid_robot.urdf'\n            ]),\n            '-x', '0', '-y', '0', '-z', '1'\n        ],\n        output='screen'\n    )\n\n    # Launch robot state publisher\n    robot_state_publisher = Node(\n        package='robot_state_publisher',\n        executable='robot_state_publisher',\n        parameters=[{\n            'robot_description': PathJoinSubstitution([\n                FindPackageShare('capstone_project'),\n                'urdf',\n                'humanoid_robot.urdf'\n            ])\n        }]\n    )\n\n    return LaunchDescription([\n        world,\n        gazebo,\n        robot_spawn,\n        robot_state_publisher\n    ])\n"})}),"\n",(0,s.jsx)(e.h3,{id:"phase-2-perception-system-integration",children:"Phase 2: Perception System Integration"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"# capstone_project/src/perception_system.py\n#!/usr/bin/env python3\n\n\"\"\"\nPerception system for the autonomous humanoid robot.\nIntegrates Isaac ROS perception nodes with custom processing.\n\"\"\"\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo, PointCloud2, LaserScan\nfrom vision_msgs.msg import Detection2DArray, Detection2D\nfrom geometry_msgs.msg import Point\nfrom std_msgs.msg import Header\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport cv2\nfrom scipy.spatial import distance\nimport tf2_ros\nfrom tf2_ros import TransformListener\nfrom tf2_geometry_msgs import do_transform_point\nimport message_filters\n\n\nclass HumanoidPerceptionSystem(Node):\n    \"\"\"\n    Integrated perception system for the humanoid robot.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__('humanoid_perception_system')\n\n        # CV Bridge for image processing\n        self.cv_bridge = CvBridge()\n\n        # TF2 for coordinate transformations\n        self.tf_buffer = tf2_ros.Buffer()\n        self.tf_listener = tf2_ros.TransformListener(self.tf_buffer, self)\n\n        # QoS for sensor data\n        sensor_qos = rclpy.qos.QoSProfile(\n            depth=10,\n            reliability=rclpy.qos.ReliabilityPolicy.BEST_EFFORT,\n            durability=rclpy.qos.DurabilityPolicy.VOLATILE\n        )\n\n        # Synchronized subscribers for RGB-D data\n        rgb_sub = message_filters.Subscriber(self, Image, '/camera/rgb/image_raw', qos_profile=sensor_qos)\n        depth_sub = message_filters.Subscriber(self, Image, '/camera/depth/image_raw', qos_profile=sensor_qos)\n        camera_info_sub = message_filters.Subscriber(self, CameraInfo, '/camera/rgb/camera_info', qos_profile=sensor_qos)\n\n        # Synchronize RGB, depth, and camera info\n        self.sync = message_filters.ApproximateTimeSynchronizer(\n            [rgb_sub, depth_sub, camera_info_sub],\n            queue_size=10,\n            slop=0.1\n        )\n        self.sync.registerCallback(self.rgb_depth_callback)\n\n        # Other sensor subscriptions\n        self.lidar_sub = self.create_subscription(\n            LaserScan,\n            '/lidar/scan',\n            self.lidar_callback,\n            10\n        )\n\n        # Publishers\n        self.object_detection_pub = self.create_publisher(\n            Detection2DArray,\n            '/perception/objects',\n            10\n        )\n\n        self.navigation_map_pub = self.create_publisher(\n            OccupancyGrid,\n            '/perception/navigation_map',\n            10\n        )\n\n        self.visualization_pub = self.create_publisher(\n            MarkerArray,\n            '/perception/visualization',\n            10\n        )\n\n        # Object detection model\n        self.object_detector = self.initialize_object_detector()\n\n        # Perception parameters\n        self.detection_confidence_threshold = 0.7\n        self.min_object_size = 0.05  # meters\n        self.max_detection_distance = 3.0  # meters\n\n        # Internal state\n        self.latest_rgb = None\n        self.latest_depth = None\n        self.camera_intrinsics = None\n        self.camera_info = None\n\n        # Object tracking\n        self.tracked_objects = {}\n        self.next_object_id = 0\n\n        self.get_logger().info('Humanoid Perception System initialized')\n\n    def initialize_object_detector(self):\n        \"\"\"\n        Initialize object detection model (using Isaac ROS or custom).\n        \"\"\"\n        # In real implementation, this would load a trained object detection model\n        # For now, we'll use a placeholder\n        return None\n\n    def rgb_depth_callback(self, rgb_msg, depth_msg, camera_info_msg):\n        \"\"\"\n        Process synchronized RGB-D data.\n        \"\"\"\n        try:\n            # Convert ROS messages to OpenCV\n            rgb_image = self.cv_bridge.imgmsg_to_cv2(rgb_msg, desired_encoding='bgr8')\n            depth_image = self.cv_bridge.imgmsg_to_cv2(depth_msg, desired_encoding='32FC1')\n\n            # Store camera info\n            self.camera_info = camera_info_msg\n            self.camera_intrinsics = np.array(camera_info_msg.k).reshape(3, 3)\n\n            # Store images\n            self.latest_rgb = rgb_image\n            self.latest_depth = depth_image\n\n            # Process perception pipeline\n            self.perception_pipeline()\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing RGB-D data: {e}')\n\n    def lidar_callback(self, msg):\n        \"\"\"\n        Process LiDAR data for navigation mapping.\n        \"\"\"\n        # Process LiDAR data to create navigation map\n        navigation_map = self.process_lidar_for_navigation(msg)\n\n        if navigation_map is not None:\n            self.navigation_map_pub.publish(navigation_map)\n\n    def perception_pipeline(self):\n        \"\"\"\n        Main perception pipeline processing.\n        \"\"\"\n        if self.latest_rgb is None or self.latest_depth is None:\n            return\n\n        # Object detection\n        detections = self.detect_objects(self.latest_rgb)\n\n        # 3D object localization using depth\n        localized_detections = self.localize_detections_in_3d(detections, self.latest_depth)\n\n        # Track objects over time\n        tracked_detections = self.track_objects(localized_detections)\n\n        # Publish detections\n        self.publish_detections(tracked_detections)\n\n        # Update visualization\n        self.update_visualization(tracked_detections)\n\n    def detect_objects(self, image):\n        \"\"\"\n        Detect objects in RGB image.\n        \"\"\"\n        # In real implementation, use Isaac ROS object detection or custom model\n        # For demonstration, use a simple color-based detection\n        detections = []\n\n        # Convert to HSV for color-based detection\n        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n        # Define colors to detect (red, blue, green)\n        colors = {\n            'red': ([0, 50, 50], [10, 255, 255]),\n            'blue': ([100, 50, 50], [130, 255, 255]),\n            'green': ([40, 50, 50], [80, 255, 255])\n        }\n\n        for color_name, (lower, upper) in colors.items():\n            lower = np.array(lower, dtype=\"uint8\")\n            upper = np.array(upper, dtype=\"uint8\")\n\n            mask = cv2.inRange(hsv, lower, upper)\n            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n            for contour in contours:\n                if cv2.contourArea(contour) > 500:  # Minimum area threshold\n                    x, y, w, h = cv2.boundingRect(contour)\n\n                    detection = Detection2D()\n                    detection.header.frame_id = 'camera_link'\n                    detection.header.stamp = self.get_clock().now().to_msg()\n\n                    # Bounding box\n                    detection.bbox.center.x = x + w/2\n                    detection.bbox.center.y = y + h/2\n                    detection.bbox.size_x = w\n                    detection.bbox.size_y = h\n\n                    # Classification\n                    class_hypothesis = ObjectHypothesisWithPose()\n                    class_hypothesis.hypothesis.class_id = color_name\n                    class_hypothesis.hypothesis.score = 0.8\n                    detection.results.append(class_hypothesis)\n\n                    detections.append(detection)\n\n        return detections\n\n    def localize_detections_in_3d(self, detections, depth_image):\n        \"\"\"\n        Localize 2D detections in 3D space using depth information.\n        \"\"\"\n        localized_detections = []\n\n        for detection in detections:\n            # Get center of bounding box\n            center_x = int(detection.bbox.center.x)\n            center_y = int(detection.bbox.center.y)\n\n            # Get depth at center (average in a small region for robustness)\n            depth_region = depth_image[\n                max(0, center_y-5):min(depth_image.shape[0], center_y+5),\n                max(0, center_x-5):min(depth_image.shape[1], center_x+5)\n            ]\n\n            valid_depths = depth_region[np.isfinite(depth_region)]\n            if len(valid_depths) > 0:\n                avg_depth = np.mean(valid_depths)\n\n                # Convert pixel coordinates to 3D world coordinates\n                if self.camera_intrinsics is not None:\n                    x_3d = (center_x - self.camera_intrinsics[0, 2]) * avg_depth / self.camera_intrinsics[0, 0]\n                    y_3d = (center_y - self.camera_intrinsics[1, 2]) * avg_depth / self.camera_intrinsics[1, 1]\n                    z_3d = avg_depth\n\n                    # Transform from camera frame to robot base frame\n                    try:\n                        transform = self.tf_buffer.lookup_transform(\n                            'base_link', 'camera_link',\n                            rclpy.time.Time(seconds=0),\n                            timeout_sec=0.1\n                        )\n\n                        # Apply transformation\n                        point_in_camera = Point(x=x_3d, y=y_3d, z=z_3d)\n                        point_in_base = do_transform_point(point_in_camera, transform)\n\n                        detection.position_3d = point_in_base\n                        localized_detections.append(detection)\n\n                    except tf2_ros.TransformException as ex:\n                        self.get_logger().warn(f'Could not transform point: {ex}')\n                        # Use camera frame coordinates as fallback\n                        detection.position_3d = Point(x=x_3d, y=y_3d, z=z_3d)\n                        localized_detections.append(detection)\n            else:\n                # No valid depth, skip this detection\n                continue\n\n        return localized_detections\n\n    def track_objects(self, detections):\n        \"\"\"\n        Track objects over time using simple association.\n        \"\"\"\n        current_time = self.get_clock().now()\n\n        for detection in detections:\n            # Simple nearest neighbor association\n            best_match = None\n            min_distance = float('inf')\n\n            for obj_id, obj_info in self.tracked_objects.items():\n                # Calculate distance between current detection and existing object\n                dist = distance.euclidean(\n                    [detection.position_3d.x, detection.position_3d.y, detection.position_3d.z],\n                    [obj_info['position'].x, obj_info['position'].y, obj_info['position'].z]\n                )\n\n                if dist < min_distance and dist < 0.5:  # 50cm threshold\n                    min_distance = dist\n                    best_match = obj_id\n\n            if best_match is not None:\n                # Update existing track\n                self.tracked_objects[best_match]['position'] = detection.position_3d\n                self.tracked_objects[best_match]['last_seen'] = current_time\n                self.tracked_objects[best_match]['detection'] = detection\n            else:\n                # Create new track\n                new_id = self.next_object_id\n                self.tracked_objects[new_id] = {\n                    'id': new_id,\n                    'position': detection.position_3d,\n                    'detection': detection,\n                    'last_seen': current_time,\n                    'first_seen': current_time,\n                    'class': detection.results[0].hypothesis.class_id if detection.results else 'unknown'\n                }\n                self.next_object_id += 1\n\n        # Remove old tracks (not seen for more than 5 seconds)\n        current_time = self.get_clock().now()\n        ids_to_remove = []\n        for obj_id, obj_info in self.tracked_objects.items():\n            time_since_seen = (current_time - obj_info['last_seen']).nanoseconds / 1e9\n            if time_since_seen > 5.0:\n                ids_to_remove.append(obj_id)\n\n        for obj_id in ids_to_remove:\n            del self.tracked_objects[obj_id]\n\n        # Return tracked detections\n        return [obj_info['detection'] for obj_info in self.tracked_objects.values()]\n\n    def publish_detections(self, detections):\n        \"\"\"\n        Publish object detections.\n        \"\"\"\n        detection_array = Detection2DArray()\n        detection_array.header.frame_id = 'base_link'\n        detection_array.header.stamp = self.get_clock().now().to_msg()\n        detection_array.detections = detections\n\n        self.object_detection_pub.publish(detection_array)\n\n    def update_visualization(self, detections):\n        \"\"\"\n        Update visualization markers for detected objects.\n        \"\"\"\n        marker_array = MarkerArray()\n\n        for i, detection in enumerate(detections):\n            # Create marker for each detection\n            marker = Marker()\n            marker.header.frame_id = 'base_link'\n            marker.header.stamp = self.get_clock().now().to_msg()\n            marker.ns = 'detected_objects'\n            marker.id = i\n            marker.type = Marker.SPHERE\n            marker.action = Marker.ADD\n\n            # Position from 3D localization\n            marker.pose.position = detection.position_3d\n            marker.pose.orientation.w = 1.0\n\n            # Scale based on object size\n            marker.scale.x = 0.1  # 10cm diameter\n            marker.scale.y = 0.1\n            marker.scale.z = 0.1\n\n            # Color based on class\n            class_name = detection.results[0].hypothesis.class_id if detection.results else 'unknown'\n            if class_name == 'red':\n                marker.color.r = 1.0\n                marker.color.g = 0.0\n                marker.color.b = 0.0\n            elif class_name == 'blue':\n                marker.color.r = 0.0\n                marker.color.g = 0.0\n                marker.color.b = 1.0\n            elif class_name == 'green':\n                marker.color.r = 0.0\n                marker.color.g = 1.0\n                marker.color.b = 0.0\n            else:\n                marker.color.r = 1.0\n                marker.color.g = 1.0\n                marker.color.b = 0.0  # Yellow for unknown\n\n            marker.color.a = 0.8\n\n            # Add label\n            label_marker = Marker()\n            label_marker.header = marker.header\n            label_marker.ns = 'object_labels'\n            label_marker.id = i + 1000  # Separate ID space\n            label_marker.type = Marker.TEXT_VIEW_FACING\n            label_marker.action = Marker.ADD\n            label_marker.pose.position = detection.position_3d\n            label_marker.pose.position.z += 0.2  # Raise label above object\n            label_marker.pose.orientation.w = 1.0\n            label_marker.text = f\"{class_name}\"\n            label_marker.scale.z = 0.1\n            label_marker.color.r = 1.0\n            label_marker.color.g = 1.0\n            label_marker.color.b = 1.0\n            label_marker.color.a = 1.0\n\n            marker_array.markers.append(marker)\n            marker_array.markers.append(label_marker)\n\n        self.visualization_pub.publish(marker_array)\n\n    def process_lidar_for_navigation(self, lidar_msg):\n        \"\"\"\n        Process LiDAR data to create navigation map.\n        \"\"\"\n        # This would create an occupancy grid for navigation\n        # For simplicity, we'll create a placeholder\n        occupancy_grid = OccupancyGrid()\n        occupancy_grid.header.frame_id = 'map'\n        occupancy_grid.header.stamp = lidar_msg.header.stamp\n\n        # Set map parameters (simplified)\n        occupancy_grid.info.resolution = 0.05  # 5cm resolution\n        occupancy_grid.info.width = 400  # 20m x 20m map\n        occupancy_grid.info.height = 400\n        occupancy_grid.info.origin.position.x = -10.0\n        occupancy_grid.info.origin.position.y = -10.0\n\n        # Create empty map initially\n        occupancy_grid.data = [-1] * (occupancy_grid.info.width * occupancy_grid.info.height)  # Unknown\n\n        return occupancy_grid\n\n\nclass HumanoidNavigationSystem(Node):\n    \"\"\"\n    Navigation system for the humanoid robot.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__('humanoid_navigation_system')\n\n        # Navigation action client\n        self.nav_client = ActionClient(self, NavigateToPose, 'navigate_to_pose')\n\n        # Publishers\n        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\n\n        # Subscribers\n        self.odom_sub = self.create_subscription(\n            Odometry,\n            '/odom',\n            self.odom_callback,\n            10\n        )\n\n        self.imu_sub = self.create_subscription(\n            Imu,\n            '/imu/data',\n            self.imu_callback,\n            10\n        )\n\n        # Internal state\n        self.current_pose = None\n        self.current_twist = None\n        self.current_orientation = None\n\n        # Navigation parameters\n        self.linear_speed = 0.3  # m/s\n        self.angular_speed = 0.5  # rad/s\n        self.arrival_threshold = 0.3  # meters\n        self.rotation_threshold = 0.1  # radians\n\n        self.get_logger().info('Humanoid Navigation System initialized')\n\n    def navigate_to_pose(self, target_pose):\n        \"\"\"\n        Navigate to target pose using navigation2.\n        \"\"\"\n        if not self.nav_client.wait_for_server(timeout_sec=5.0):\n            self.get_logger().error('Navigation action server not available')\n            return False\n\n        goal_msg = NavigateToPose.Goal()\n        goal_msg.pose = target_pose\n\n        future = self.nav_client.send_goal_async(goal_msg)\n        future.add_done_callback(self.navigation_done_callback)\n\n        return True\n\n    def navigation_done_callback(self, future):\n        \"\"\"\n        Callback when navigation goal is completed.\n        \"\"\"\n        goal_handle = future.result()\n\n        if goal_handle.status == GoalStatus.STATUS_SUCCEEDED:\n            self.get_logger().info('Navigation succeeded')\n        elif goal_handle.status == GoalStatus.STATUS_CANCELED:\n            self.get_logger().info('Navigation was canceled')\n        elif goal_handle.status == GoalStatus.STATUS_ABORTED:\n            self.get_logger().error('Navigation failed')\n        else:\n            self.get_logger().info(f'Navigation finished with status: {goal_handle.status}')\n\n    def odom_callback(self, msg):\n        \"\"\"\n        Update robot pose from odometry.\n        \"\"\"\n        self.current_pose = msg.pose.pose\n        self.current_twist = msg.twist.twist\n\n    def imucallback(self, msg):\n        \"\"\"\n        Update robot orientation from IMU.\n        \"\"\"\n        self.current_orientation = msg.orientation\n\n\nclass HumanoidManipulationSystem(Node):\n    \"\"\"\n    Manipulation system for the humanoid robot.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__('humanoid_manipulation_system')\n\n        # Publishers for arm control\n        self.left_arm_pub = self.create_publisher(JointTrajectory, '/left_arm_controller/joint_trajectory', 10)\n        self.right_arm_pub = self.create_publisher(JointTrajectory, '/right_arm_controller/joint_trajectory', 10)\n        self.gripper_pub = self.create_publisher(Float64MultiArray, '/gripper_controller/commands', 10)\n\n        # Subscribers\n        self.joint_state_sub = self.create_subscription(\n            JointState,\n            '/joint_states',\n            self.joint_state_callback,\n            10\n        )\n\n        # Internal state\n        self.current_joint_positions = {}\n        self.left_arm_joints = ['left_shoulder_pan', 'left_shoulder_lift', 'left_elbow_flex',\n                               'left_wrist_flex', 'left_wrist_roll', 'left_forearm_roll', 'left_gripper']\n        self.right_arm_joints = ['right_shoulder_pan', 'right_shoulder_lift', 'right_elbow_flex',\n                                'right_wrist_flex', 'right_wrist_roll', 'right_forearm_roll', 'right_gripper']\n\n        self.get_logger().info('Humanoid Manipulation System initialized')\n\n    def joint_state_callback(self, msg):\n        \"\"\"\n        Update current joint positions.\n        \"\"\"\n        for i, name in enumerate(msg.name):\n            if i < len(msg.position):\n                self.current_joint_positions[name] = msg.position[i]\n\n    def move_arm_to_pose(self, arm_side, target_pose, duration=5.0):\n        \"\"\"\n        Move specified arm to target pose.\n        \"\"\"\n        if arm_side not in ['left', 'right']:\n            self.get_logger().error(f'Invalid arm side: {arm_side}')\n            return False\n\n        # Calculate joint trajectory for target pose\n        # This would use inverse kinematics in real implementation\n        joint_trajectory = self.calculate_arm_trajectory(arm_side, target_pose)\n\n        # Publish trajectory\n        if arm_side == 'left':\n            self.left_arm_pub.publish(joint_trajectory)\n        else:\n            self.right_arm_pub.publish(joint_trajectory)\n\n        return True\n\n    def calculate_arm_trajectory(self, arm_side, target_pose):\n        \"\"\"\n        Calculate joint trajectory for arm movement (simplified).\n        \"\"\"\n        # This would use inverse kinematics in real implementation\n        # For now, return a simple trajectory\n        trajectory = JointTrajectory()\n        trajectory.joint_names = self.left_arm_joints if arm_side == 'left' else self.right_arm_joints\n\n        # Create trajectory point\n        point = JointTrajectoryPoint()\n        # In real implementation, calculate actual joint positions using IK\n        point.positions = [0.0] * len(trajectory.joint_names)  # Placeholder\n        point.time_from_start.sec = int(duration)\n        point.time_from_start.nanosec = int((duration % 1) * 1e9)\n\n        trajectory.points = [point]\n        return trajectory\n\n    def grasp_object(self, arm_side):\n        \"\"\"\n        Close gripper to grasp object.\n        \"\"\"\n        commands = Float64MultiArray()\n        if arm_side == 'left':\n            commands.data = [0.0]  # Close left gripper\n        else:\n            commands.data = [0.0]  # Close right gripper\n\n        self.gripper_pub.publish(commands)\n        self.get_logger().info(f'{arm_side.capitalize()} gripper closed')\n\n    def release_object(self, arm_side):\n        \"\"\"\n        Open gripper to release object.\n        \"\"\"\n        commands = Float64MultiArray()\n        if arm_side == 'left':\n            commands.data = [1.0]  # Open left gripper\n        else:\n            commands.data = [1.0]  # Open right gripper\n\n        self.gripper_pub.publish(commands)\n        self.get_logger().info(f'{arm_side.capitalize()} gripper opened')\n"})}),"\n",(0,s.jsx)(e.h3,{id:"phase-3-cognitive-planning-and-natural-language-integration",children:"Phase 3: Cognitive Planning and Natural Language Integration"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"# capstone_project/src/cognitive_planner.py\n#!/usr/bin/env python3\n\n\"\"\"\nCognitive planning system for the autonomous humanoid robot.\nIntegrates LLM-based planning with multi-modal interaction.\n\"\"\"\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import PoseStamped, Point\nfrom action_msgs.msg import GoalStatus\nfrom builtin_interfaces.msg import Time\nimport asyncio\nimport json\nimport openai\nimport time\nfrom typing import Dict, List, Optional, Tuple\nfrom dataclasses import dataclass\nfrom enum import Enum\n\n\nclass TaskType(Enum):\n    NAVIGATION = \"navigation\"\n    MANIPULATION = \"manipulation\"\n    PERCEPTION = \"perception\"\n    INTERACTION = \"interaction\"\n\n\n@dataclass\nclass Task:\n    \"\"\"\n    Data structure for tasks in the planning system.\n    \"\"\"\n    id: str\n    type: TaskType\n    description: str\n    parameters: Dict\n    priority: int = 0\n    status: str = \"pending\"  # pending, executing, completed, failed\n    dependencies: List[str] = None\n    created_at: float = 0.0\n\n\nclass AutonomousHumanoidPlanner(Node):\n    \"\"\"\n    Cognitive planning system for the humanoid robot.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__('autonomous_humanoid_planner')\n\n        # Initialize LLM client\n        self.llm_client = self.initialize_llm_client()\n\n        # Publishers\n        self.task_pub = self.create_publisher(String, '/planning/tasks', 10)\n        self.status_pub = self.create_publisher(String, '/planning/status', 10)\n\n        # Subscribers\n        self.command_sub = self.create_subscription(\n            String,\n            '/natural_language_command',\n            self.command_callback,\n            10\n        )\n\n        self.perception_sub = self.create_subscription(\n            String,\n            '/perception/events',\n            self.perception_callback,\n            10\n        )\n\n        # Internal state\n        self.current_tasks = []\n        self.completed_tasks = []\n        self.failed_tasks = []\n        self.robot_capabilities = self.define_robot_capabilities()\n        self.location_map = self.define_location_map()\n\n        # Task execution\n        self.current_task = None\n        self.task_executor = TaskExecutor(self)\n\n        self.get_logger().info('Autonomous Humanoid Planner initialized')\n\n    def initialize_llm_client(self):\n        \"\"\"\n        Initialize LLM client for cognitive planning.\n        \"\"\"\n        # In practice, set your API key securely\n        # openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n        return openai\n\n    def define_robot_capabilities(self):\n        \"\"\"\n        Define robot's capabilities for planning.\n        \"\"\"\n        return {\n            'locomotion': {\n                'abilities': ['navigate', 'move_base', 'turn'],\n                'constraints': ['max_speed: 0.5 m/s', 'indoor_only']\n            },\n            'manipulation': {\n                'abilities': ['grasp', 'release', 'carry', 'place'],\n                'constraints': ['max_payload: 2.0 kg', 'reachable_distance: 1.5m']\n            },\n            'perception': {\n                'abilities': ['detect_objects', 'recognize_people', 'measure_distances'],\n                'constraints': ['fov: 60 degrees', 'range: 0.1-3.0m']\n            },\n            'communication': {\n                'abilities': ['speak', 'listen', 'gesture'],\n                'constraints': ['language: English']\n            }\n        }\n\n    def define_location_map(self):\n        \"\"\"\n        Define known locations in the environment.\n        \"\"\"\n        return {\n            'kitchen': {'x': -2.0, 'y': 1.0, 'description': 'Kitchen area with appliances'},\n            'living_room': {'x': 1.0, 'y': -1.0, 'description': 'Main living area'},\n            'bedroom': {'x': 2.0, 'y': 2.0, 'description': 'Sleeping area'},\n            'office': {'x': -1.0, 'y': -2.0, 'description': 'Work area'},\n            'entrance': {'x': 0.0, 'y': 0.0, 'description': 'Main entrance'},\n            'dining_room': {'x': -1.5, 'y': 0.5, 'description': 'Dining area'}\n        }\n\n    def command_callback(self, msg):\n        \"\"\"\n        Process natural language command.\n        \"\"\"\n        command_text = msg.data\n        self.get_logger().info(f'Received command: {command_text}')\n\n        # Plan and execute command\n        asyncio.create_task(self.process_command_async(command_text))\n\n    def perception_callback(self, msg):\n        \"\"\"\n        Process perception events for context-aware planning.\n        \"\"\"\n        try:\n            perception_data = json.loads(msg.data)\n            self.update_context_with_perception(perception_data)\n        except json.JSONDecodeError:\n            self.get_logger().error('Could not parse perception data')\n\n    def update_context_with_perception(self, perception_data):\n        \"\"\"\n        Update planning context with perception information.\n        \"\"\"\n        # Update known objects and locations based on perception\n        if 'objects' in perception_data:\n            for obj in perception_data['objects']:\n                # Add object to context if not already known\n                pass\n\n    async def process_command_async(self, command_text):\n        \"\"\"\n        Process command asynchronously using LLM.\n        \"\"\"\n        try:\n            # Generate plan using LLM\n            plan = await self.generate_plan_with_llm(command_text)\n\n            # Execute plan\n            await self.execute_plan_async(plan)\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing command: {e}')\n            self.publish_status(f'Error: {str(e)}')\n\n    async def generate_plan_with_llm(self, command_text):\n        \"\"\"\n        Generate plan using LLM based on command.\n        \"\"\"\n        system_prompt = f\"\"\"\n        You are a cognitive planning assistant for a humanoid robot. The robot has these capabilities:\n\n        {json.dumps(self.robot_capabilities, indent=2)}\n\n        Known locations in the environment:\n        {json.dumps(self.location_map, indent=2)}\n\n        Your task is to break down the following command into executable steps:\n        \"{command_text}\"\n\n        Respond with a JSON array of tasks, where each task has:\n        - 'type': task type (navigation, manipulation, perception, interaction)\n        - 'action': specific action to take\n        - 'parameters': any required parameters\n        - 'description': human-readable description\n        - 'dependencies': list of task IDs that must be completed first\n\n        Tasks should be atomic and executable by the robot.\n        \"\"\"\n\n        try:\n            response = await asyncio.get_event_loop().run_in_executor(\n                None,\n                lambda: self.llm_client.ChatCompletion.create(\n                    model=\"gpt-4\",\n                    messages=[\n                        {\"role\": \"system\", \"content\": system_prompt},\n                        {\"role\": \"user\", \"content\": command_text}\n                    ],\n                    temperature=0.1\n                )\n            )\n\n            plan_text = response.choices[0].message.content\n\n            # Parse plan from response\n            plan = self.parse_plan_from_response(plan_text)\n\n            # Validate and prioritize plan\n            validated_plan = self.validate_and_prioritize_plan(plan)\n\n            self.get_logger().info(f'Generated plan: {validated_plan}')\n\n            return validated_plan\n\n        except Exception as e:\n            self.get_logger().error(f'Error generating plan: {e}')\n            return []\n\n    def parse_plan_from_response(self, response_text):\n        \"\"\"\n        Parse plan from LLM response.\n        \"\"\"\n        try:\n            # Look for JSON in the response\n            start_idx = response_text.find('[')\n            end_idx = response_text.rfind(']') + 1\n\n            if start_idx != -1 and end_idx != 0:\n                json_str = response_text[start_idx:end_idx]\n                plan = json.loads(json_str)\n                return plan\n            else:\n                # If no JSON found, create simple plan based on command\n                return self.create_simple_plan_from_command(response_text)\n        except json.JSONDecodeError:\n            self.get_logger().error('Could not parse LLM response as JSON')\n            return []\n\n    def create_simple_plan_from_command(self, command_text):\n        \"\"\"\n        Create simple plan if LLM response couldn't be parsed.\n        \"\"\"\n        plan = []\n\n        if 'go to' in command_text.lower() or 'navigate to' in command_text.lower():\n            # Extract location\n            for loc_name in self.location_map.keys():\n                if loc_name in command_text.lower():\n                    plan.append({\n                        'type': 'navigation',\n                        'action': 'go_to_location',\n                        'parameters': {'location': loc_name},\n                        'description': f'Navigate to {loc_name}',\n                        'dependencies': []\n                    })\n                    break\n\n        elif 'pick up' in command_text.lower() or 'grasp' in command_text.lower():\n            plan.append({\n                'type': 'manipulation',\n                'action': 'grasp_object',\n                'parameters': {'object': 'unknown'},  # Would be extracted from command\n                'description': 'Grasp specified object',\n                'dependencies': [{'type': 'navigation', 'action': 'go_to_location'}]  # Navigate first\n            })\n\n        elif 'find' in command_text.lower() or 'look for' in command_text.lower():\n            plan.append({\n                'type': 'perception',\n                'action': 'detect_objects',\n                'parameters': {'object_type': 'unknown'},\n                'description': 'Detect specified object',\n                'dependencies': []\n            })\n\n        return plan\n\n    def validate_and_prioritize_plan(self, plan):\n        \"\"\"\n        Validate and prioritize the generated plan.\n        \"\"\"\n        validated_plan = []\n\n        for i, task_data in enumerate(plan):\n            try:\n                # Validate task data\n                task_type = task_data.get('type')\n                action = task_data.get('action')\n                parameters = task_data.get('parameters', {})\n                description = task_data.get('description', f'Task {i}')\n                dependencies = task_data.get('dependencies', [])\n\n                if not task_type or not action:\n                    continue  # Skip invalid tasks\n\n                # Create task object\n                task = Task(\n                    id=f'task_{i}',\n                    type=TaskType(task_type),\n                    description=description,\n                    parameters=parameters,\n                    dependencies=dependencies,\n                    created_at=time.time()\n                )\n\n                # Validate against robot capabilities\n                if self.is_task_feasible(task):\n                    validated_plan.append(task)\n\n            except (KeyError, ValueError) as e:\n                self.get_logger().warn(f'Invalid task data: {task_data}, error: {e}')\n                continue\n\n        return validated_plan\n\n    def is_task_feasible(self, task: Task) -> bool:\n        \"\"\"\n        Check if a task is feasible given robot capabilities.\n        \"\"\"\n        # Check if action is supported by robot\n        for capability_category in self.robot_capabilities.values():\n            if task.description in capability_category['abilities']:\n                return True\n\n        # For more complex checks, examine action and parameters\n        if task.type == TaskType.NAVIGATION:\n            # Check navigation constraints\n            return True\n        elif task.type == TaskType.MANIPULATION:\n            # Check manipulation constraints\n            return True\n        elif task.type == TaskType.PERCEPTION:\n            # Check perception constraints\n            return True\n        elif task.type == TaskType.INTERACTION:\n            # Check interaction constraints\n            return True\n\n        return False\n\n    async def execute_plan_async(self, plan):\n        \"\"\"\n        Execute the plan asynchronously.\n        \"\"\"\n        if not plan:\n            self.get_logger().warn('No plan to execute')\n            return\n\n        self.get_logger().info(f'Executing plan with {len(plan)} tasks')\n\n        # Add tasks to queue\n        for task in plan:\n            self.current_tasks.append(task)\n\n        # Execute tasks in order respecting dependencies\n        await self.execute_task_queue()\n\n    async def execute_task_queue(self):\n        \"\"\"\n        Execute tasks in the queue respecting dependencies.\n        \"\"\"\n        completed_ids = set()\n\n        while self.current_tasks:\n            # Find tasks whose dependencies are satisfied\n            ready_tasks = []\n            for task in self.current_tasks:\n                deps_satisfied = True\n                for dep in task.dependencies:\n                    if isinstance(dep, dict) and 'id' in dep:\n                        if dep['id'] not in completed_ids:\n                            deps_satisfied = False\n                            break\n                    elif isinstance(dep, str):\n                        if dep not in completed_ids:\n                            deps_satisfied = False\n                            break\n\n                if deps_satisfied:\n                    ready_tasks.append(task)\n\n            if not ready_tasks:\n                self.get_logger().error('Deadlock detected in task dependencies')\n                break\n\n            # Execute ready tasks (in parallel or sequentially based on type)\n            for task in ready_tasks:\n                self.current_tasks.remove(task)\n                success = await self.execute_single_task(task)\n\n                if success:\n                    completed_ids.add(task.id)\n                    self.completed_tasks.append(task)\n                    self.publish_status(f'Task completed: {task.description}')\n                else:\n                    self.failed_tasks.append(task)\n                    self.publish_status(f'Task failed: {task.description}')\n                    # Continue with other tasks\n\n    async def execute_single_task(self, task: Task) -> bool:\n        \"\"\"\n        Execute a single task.\n        \"\"\"\n        self.get_logger().info(f'Executing task: {task.description}')\n\n        # Update task status\n        task.status = 'executing'\n        self.publish_task_status(task)\n\n        try:\n            if task.type == TaskType.NAVIGATION:\n                return await self.execute_navigation_task(task)\n            elif task.type == TaskType.MANIPULATION:\n                return await self.execute_manipulation_task(task)\n            elif task.type == TaskType.PERCEPTION:\n                return await self.execute_perception_task(task)\n            elif task.type == TaskType.INTERACTION:\n                return await self.execute_interaction_task(task)\n            else:\n                self.get_logger().error(f'Unknown task type: {task.type}')\n                return False\n\n        except Exception as e:\n            self.get_logger().error(f'Error executing task {task.id}: {e}')\n            return False\n\n    async def execute_navigation_task(self, task: Task) -> bool:\n        \"\"\"\n        Execute navigation task.\n        \"\"\"\n        location_name = task.parameters.get('location')\n\n        if not location_name:\n            self.get_logger().error('No location specified for navigation task')\n            return False\n\n        # Get coordinates for location\n        if location_name not in self.location_map:\n            self.get_logger().error(f'Unknown location: {location_name}')\n            return False\n\n        location_data = self.location_map[location_name]\n        target_pose = PoseStamped()\n        target_pose.header.stamp = self.get_clock().now().to_msg()\n        target_pose.header.frame_id = 'map'\n        target_pose.pose.position.x = location_data['x']\n        target_pose.pose.position.y = location_data['y']\n        target_pose.pose.position.z = 0.0\n\n        # Call navigation system\n        # This would integrate with the navigation system created earlier\n        self.get_logger().info(f'Navigating to {location_name} at ({location_data[\"x\"]}, {location_data[\"y\"]})')\n\n        # Simulate navigation execution\n        await asyncio.sleep(2.0)  # Simulated navigation time\n\n        return True\n\n    async def execute_manipulation_task(self, task: Task) -> bool:\n        \"\"\"\n        Execute manipulation task.\n        \"\"\"\n        action = task.parameters.get('action')\n        object_name = task.parameters.get('object')\n\n        if not action:\n            self.get_logger().error('No action specified for manipulation task')\n            return False\n\n        self.get_logger().info(f'Performing manipulation: {action} on {object_name}')\n\n        # Call manipulation system\n        # This would integrate with the manipulation system created earlier\n        if action == 'grasp_object':\n            # Simulate grasping\n            await asyncio.sleep(1.0)\n        elif action == 'release_object':\n            # Simulate releasing\n            await asyncio.sleep(1.0)\n\n        return True\n\n    async def execute_perception_task(self, task: Task) -> bool:\n        \"\"\"\n        Execute perception task.\n        \"\"\"\n        object_type = task.parameters.get('object_type')\n\n        self.get_logger().info(f'Detecting objects of type: {object_type}')\n\n        # Call perception system\n        # This would integrate with the perception system created earlier\n\n        # Simulate perception execution\n        await asyncio.sleep(1.0)\n\n        return True\n\n    async def execute_interaction_task(self, task: Task) -> bool:\n        \"\"\"\n        Execute interaction task.\n        \"\"\"\n        message = task.parameters.get('message', '')\n\n        self.get_logger().info(f'Interacting: {message}')\n\n        # Call speech system\n        # This would integrate with speech system\n\n        # Simulate interaction execution\n        await asyncio.sleep(0.5)\n\n        return True\n\n    def publish_task_status(self, task: Task):\n        \"\"\"\n        Publish task status update.\n        \"\"\"\n        status_msg = String()\n        status_msg.data = f'TASK_STATUS: {task.id} - {task.status} - {task.description}'\n        self.status_pub.publish(status_msg)\n\n    def publish_status(self, status_text: str):\n        \"\"\"\n        Publish general status update.\n        \"\"\"\n        status_msg = String()\n        status_msg.data = status_text\n        self.status_pub.publish(status_msg)\n\n\nclass TaskExecutor:\n    \"\"\"\n    Component responsible for executing individual tasks.\n    \"\"\"\n\n    def __init__(self, planner_node):\n        self.planner_node = planner_node\n        self.execution_history = []\n\n    async def execute_task(self, task: Task) -> bool:\n        \"\"\"\n        Execute a task and return success status.\n        \"\"\"\n        start_time = time.time()\n\n        success = await self.planner_node.execute_single_task(task)\n\n        execution_time = time.time() - start_time\n\n        # Record execution\n        self.execution_history.append({\n            'task_id': task.id,\n            'success': success,\n            'execution_time': execution_time,\n            'timestamp': time.time()\n        })\n\n        return success\n"})}),"\n",(0,s.jsx)(e.h3,{id:"phase-4-integration-and-testing",children:"Phase 4: Integration and Testing"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'# capstone_project/src/main_system.py\n#!/usr/bin/env python3\n\n"""\nMain system integration for the autonomous humanoid robot.\n"""\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nimport asyncio\nimport threading\nimport time\n\n\nclass AutonomousHumanoidSystem(Node):\n    """\n    Main system node that integrates all components.\n    """\n\n    def __init__(self):\n        super().__init__(\'autonomous_humanoid_system\')\n\n        # Initialize subsystems\n        self.perception_system = HumanoidPerceptionSystem()\n        self.navigation_system = HumanoidNavigationSystem()\n        self.manipulation_system = HumanoidManipulationSystem()\n        self.cognitive_planner = AutonomousHumanoidPlanner()\n\n        # Publishers\n        self.system_status_pub = self.create_publisher(String, \'/system/status\', 10)\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n\n        # Subscribers\n        self.command_sub = self.create_subscription(\n            String,\n            \'/user/command\',\n            self.user_command_callback,\n            10\n        )\n\n        # System state\n        self.system_state = \'idle\'  # idle, active, paused, error\n        self.active_components = []\n\n        # Timers\n        self.status_timer = self.create_timer(1.0, self.publish_system_status)\n\n        self.get_logger().info(\'Autonomous Humanoid System initialized\')\n\n    def user_command_callback(self, msg):\n        """\n        Handle user commands that may involve multiple subsystems.\n        """\n        command = msg.data\n        self.get_logger().info(f\'Received user command: {command}\')\n\n        # Route command to appropriate subsystem\n        if any(word in command.lower() for word in [\'go\', \'navigate\', \'move\', \'walk\']):\n            # Route to cognitive planner for high-level navigation\n            cmd_msg = String()\n            cmd_msg.data = command\n            self.cognitive_planner.command_pub.publish(cmd_msg)\n        elif any(word in command.lower() for word in [\'pick\', \'grasp\', \'take\', \'lift\']):\n            # Route to cognitive planner for manipulation\n            cmd_msg = String()\n            cmd_msg.data = command\n            self.cognitive_planner.command_pub.publish(cmd_msg)\n        elif any(word in command.lower() for word in [\'find\', \'look\', \'detect\', \'see\']):\n            # Route to cognitive planner for perception\n            cmd_msg = String()\n            cmd_msg.data = command\n            self.cognitive_planner.command_pub.publish(cmd_msg)\n        else:\n            # Default to cognitive planner for complex commands\n            cmd_msg = String()\n            cmd_msg.data = command\n            self.cognitive_planner.command_pub.publish(cmd_msg)\n\n    def publish_system_status(self):\n        """\n        Publish overall system status.\n        """\n        status_msg = String()\n        status_msg.data = f\'SYSTEM_STATUS: {self.system_state} - Components: {len(self.active_components)}\'\n        self.system_status_pub.publish(status_msg)\n\n    def start_system(self):\n        """\n        Start all subsystems.\n        """\n        self.system_state = \'active\'\n        self.get_logger().info(\'Starting autonomous humanoid system...\')\n\n        # Start each subsystem in separate threads\n        self.perception_thread = threading.Thread(target=self.run_perception, daemon=True)\n        self.navigation_thread = threading.Thread(target=self.run_navigation, daemon=True)\n        self.manipulation_thread = threading.Thread(target=self.run_manipulation, daemon=True)\n        self.planning_thread = threading.Thread(target=self.run_planning, daemon=True)\n\n        self.perception_thread.start()\n        self.navigation_thread.start()\n        self.manipulation_thread.start()\n        self.planning_thread.start()\n\n        self.active_components = [\'perception\', \'navigation\', \'manipulation\', \'planning\']\n        self.get_logger().info(\'All subsystems started\')\n\n    def run_perception(self):\n        """\n        Run perception system continuously.\n        """\n        # In real implementation, this would run the perception system\n        pass\n\n    def run_navigation(self):\n        """\n        Run navigation system continuously.\n        """\n        # In real implementation, this would run the navigation system\n        pass\n\n    def run_manipulation(self):\n        """\n        Run manipulation system continuously.\n        """\n        # In real implementation, this would run the manipulation system\n        pass\n\n    def run_planning(self):\n        """\n        Run planning system continuously.\n        """\n        # In real implementation, this would run the planning system\n        pass\n\n    def stop_system(self):\n        """\n        Stop all subsystems.\n        """\n        self.system_state = \'idle\'\n        self.get_logger().info(\'Stopping autonomous humanoid system...\')\n\n        # Stop all threads\n        # In real implementation, set flags to stop each subsystem\n        pass\n\n\ndef main(args=None):\n    """\n    Main function to run the autonomous humanoid system.\n    """\n    rclpy.init(args=args)\n\n    system = AutonomousHumanoidSystem()\n\n    try:\n        system.start_system()\n        rclpy.spin(system)\n    except KeyboardInterrupt:\n        system.get_logger().info(\'Shutting down autonomous humanoid system...\')\n    finally:\n        system.stop_system()\n        system.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(e.h2,{id:"testing-and-validation",children:"Testing and Validation"}),"\n",(0,s.jsx)(e.h3,{id:"test-scenarios",children:"Test Scenarios"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Basic Navigation Test"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:'Command: "Go to the kitchen"'}),"\n",(0,s.jsx)(e.li,{children:"Expected: Robot navigates to kitchen location"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Object Manipulation Test"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:'Command: "Pick up the red ball from the living room"'}),"\n",(0,s.jsx)(e.li,{children:"Expected: Robot navigates to living room, detects red ball, grasps it"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Multi-step Task Test"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:'Command: "Go to the kitchen, find a cup, bring it to me"'}),"\n",(0,s.jsx)(e.li,{children:"Expected: Robot performs sequence of navigation, detection, manipulation"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Social Interaction Test"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:'Command: "Say hello to everyone in the room"'}),"\n",(0,s.jsx)(e.li,{children:"Expected: Robot detects people and greets them"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"performance-metrics",children:"Performance Metrics"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Success Rate"}),": Percentage of tasks completed successfully"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Response Time"}),": Time from command to action initiation"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Accuracy"}),": Precision of navigation and manipulation"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Robustness"}),": Performance under various conditions"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"User Satisfaction"}),": Subjective measure of naturalness"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,s.jsx)(e.p,{children:"The autonomous humanoid robot capstone project demonstrates the integration of all the concepts covered in this book. It showcases how ROS 2, simulation environments, AI perception and planning, and multi-modal interaction can work together to create an intelligent, autonomous robot system capable of understanding natural language commands and performing complex tasks in real-world environments."}),"\n",(0,s.jsx)(e.p,{children:"The system architecture is modular and extensible, allowing for future enhancements and adaptations. Through this project, you have gained hands-on experience with the entire pipeline of humanoid robotics development, from low-level control to high-level cognitive planning."})]})}function p(n={}){const{wrapper:e}={...(0,i.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(d,{...n})}):d(n)}}}]);