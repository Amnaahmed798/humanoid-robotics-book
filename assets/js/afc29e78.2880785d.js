"use strict";(self.webpackChunkdocusaurus=self.webpackChunkdocusaurus||[]).push([[14],{2545:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>t,default:()=>m,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module2/sensor_simulation","title":"Sensor Simulation: LiDAR, Depth Cameras, IMUs","description":"Introduction","source":"@site/docs/module2/sensor_simulation.md","sourceDirName":"module2","slug":"/module2/sensor_simulation","permalink":"/humanoid-robotics-book/docs/module2/sensor_simulation","draft":false,"unlisted":false,"editUrl":"https://github.com/Amnaahmed798/humanoid-robotics-book/edit/main/docs/module2/sensor_simulation.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Integration of ROS 2 with Simulated Environments","permalink":"/humanoid-robotics-book/docs/module2/ros2_sim_integration"},"next":{"title":"Unity Visualization for Robots","permalink":"/humanoid-robotics-book/docs/module2/unity_visualization"}}');var a=i(4848),r=i(8453);const o={},t="Sensor Simulation: LiDAR, Depth Cameras, IMUs",l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"LiDAR Simulation",id:"lidar-simulation",level:2},{value:"Types of LiDAR Sensors",id:"types-of-lidar-sensors",level:3},{value:"2D LiDAR Configuration",id:"2d-lidar-configuration",level:3},{value:"3D LiDAR Configuration",id:"3d-lidar-configuration",level:3},{value:"Depth Camera Simulation",id:"depth-camera-simulation",level:2},{value:"Depth Camera Configuration",id:"depth-camera-configuration",level:3},{value:"RGB-D Camera with Point Cloud Output",id:"rgb-d-camera-with-point-cloud-output",level:3},{value:"IMU Simulation",id:"imu-simulation",level:2},{value:"IMU Sensor Configuration",id:"imu-sensor-configuration",level:3},{value:"Working with Sensor Data in ROS 2",id:"working-with-sensor-data-in-ros-2",level:2},{value:"LiDAR Data Processing",id:"lidar-data-processing",level:3},{value:"Depth Camera Data Processing",id:"depth-camera-data-processing",level:3},{value:"IMU Data Processing",id:"imu-data-processing",level:3},{value:"Sensor Fusion Example",id:"sensor-fusion-example",level:2},{value:"Best Practices for Sensor Simulation",id:"best-practices-for-sensor-simulation",level:2},{value:"LiDAR Best Practices",id:"lidar-best-practices",level:3},{value:"Camera Best Practices",id:"camera-best-practices",level:3},{value:"IMU Best Practices",id:"imu-best-practices",level:3},{value:"General Best Practices",id:"general-best-practices",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"sensor-simulation-lidar-depth-cameras-imus",children:"Sensor Simulation: LiDAR, Depth Cameras, IMUs"})}),"\n",(0,a.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,a.jsx)(n.p,{children:"Sensor simulation is a critical aspect of robotics development, allowing developers to test perception algorithms, navigation systems, and control strategies without physical hardware. This chapter covers the simulation of three key sensor types in Gazebo: LiDAR, depth cameras, and IMUs, which are essential for robot perception and navigation."}),"\n",(0,a.jsx)(n.h2,{id:"lidar-simulation",children:"LiDAR Simulation"}),"\n",(0,a.jsx)(n.h3,{id:"types-of-lidar-sensors",children:"Types of LiDAR Sensors"}),"\n",(0,a.jsx)(n.p,{children:"Gazebo supports several types of LiDAR sensors:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Ray sensors"}),": Basic 2D laser range finders"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"GPU ray sensors"}),": GPU-accelerated 2D laser range finders"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"3D ray sensors"}),": Multi-line LiDAR (e.g., Velodyne-style)"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"2d-lidar-configuration",children:"2D LiDAR Configuration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'<gazebo reference="laser_link">\n  <sensor name="laser_sensor" type="ray">\n    <pose>0 0 0 0 0 0</pose>\n    <visualize>true</visualize>\n    <update_rate>40</update_rate>\n    <ray>\n      <scan>\n        <horizontal>\n          <samples>720</samples>\n          <resolution>1</resolution>\n          <min_angle>-1.570796</min_angle>  \x3c!-- -90 degrees --\x3e\n          <max_angle>1.570796</max_angle>   \x3c!-- 90 degrees --\x3e\n        </horizontal>\n      </scan>\n      <range>\n        <min>0.1</min>\n        <max>30.0</max>\n        <resolution>0.01</resolution>\n      </range>\n    </ray>\n    <plugin name="laser_controller" filename="libgazebo_ros_ray_sensor.so">\n      <ros>\n        <namespace>/laser</namespace>\n        <remapping>~/out:=scan</remapping>\n      </ros>\n      <output_type>sensor_msgs/LaserScan</output_type>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,a.jsx)(n.h3,{id:"3d-lidar-configuration",children:"3D LiDAR Configuration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'<gazebo reference="velodyne_link">\n  <sensor name="velodyne_sensor" type="ray">\n    <pose>0 0 0 0 0 0</pose>\n    <visualize>false</visualize>\n    <update_rate>10</update_rate>\n    <ray>\n      <scan>\n        <horizontal>\n          <samples>1800</samples>\n          <resolution>1</resolution>\n          <min_angle>-3.14159</min_angle>\n          <max_angle>3.14159</max_angle>\n        </horizontal>\n        <vertical>\n          <samples>32</samples>\n          <resolution>1</resolution>\n          <min_angle>-0.436332</min_angle>  \x3c!-- -25 degrees --\x3e\n          <max_angle>0.279253</max_angle>   \x3c!-- 16 degrees --\x3e\n        </vertical>\n      </scan>\n      <range>\n        <min>0.2</min>\n        <max>100.0</max>\n        <resolution>0.01</resolution>\n      </range>\n    </ray>\n    <plugin name="velodyne_controller" filename="libgazebo_ros_ray_sensor.so">\n      <ros>\n        <namespace>/velodyne</namespace>\n        <remapping>~/out:=scan</remapping>\n      </ros>\n      <output_type>sensor_msgs/PointCloud2</output_type>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,a.jsx)(n.h2,{id:"depth-camera-simulation",children:"Depth Camera Simulation"}),"\n",(0,a.jsx)(n.h3,{id:"depth-camera-configuration",children:"Depth Camera Configuration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'<gazebo reference="camera_link">\n  <sensor name="depth_camera" type="depth">\n    <pose>0 0 0 0 0 0</pose>\n    <visualize>true</visualize>\n    <update_rate>30</update_rate>\n    <camera name="depth_camera">\n      <horizontal_fov>1.047</horizontal_fov>  \x3c!-- 60 degrees --\x3e\n      <image>\n        <width>640</width>\n        <height>480</height>\n        <format>R8G8B8</format>\n      </image>\n      <clip>\n        <near>0.1</near>\n        <far>10.0</far>\n      </clip>\n      <noise>\n        <type>gaussian</type>\n        <mean>0.0</mean>\n        <stddev>0.007</stddev>\n      </noise>\n    </camera>\n    <plugin name="camera_controller" filename="libgazebo_ros_camera.so">\n      <ros>\n        <namespace>/camera</namespace>\n        <remapping>image_raw:=image</remapping>\n        <remapping>camera_info:=camera_info</remapping>\n      </ros>\n      <camera_name>depth_camera</camera_name>\n      <frame_name>camera_optical_frame</frame_name>\n      <min_depth>0.1</min_depth>\n      <max_depth>10.0</max_depth>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,a.jsx)(n.h3,{id:"rgb-d-camera-with-point-cloud-output",children:"RGB-D Camera with Point Cloud Output"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'<gazebo reference="rgbd_camera_link">\n  <sensor name="rgbd_camera" type="depth">\n    <always_on>true</always_on>\n    <visualize>true</visualize>\n    <update_rate>30</update_rate>\n    <camera name="head">\n      <horizontal_fov>1.047</horizontal_fov>\n      <image>\n        <width>640</width>\n        <height>480</height>\n        <format>R8G8B8</format>\n      </image>\n      <clip>\n        <near>0.1</near>\n        <far>100</far>\n      </clip>\n    </camera>\n    <plugin name="camera_controller" filename="libgazebo_ros_openni_kinect.so">\n      <baseline>0.2</baseline>\n      <alwaysOn>true</alwaysOn>\n      <updateRate>30.0</updateRate>\n      <cameraName>rgbd_camera</cameraName>\n      <imageTopicName>/rgb/image_raw</imageTopicName>\n      <depthImageTopicName>/depth/image_raw</depthImageTopicName>\n      <pointCloudTopicName>/depth/points</pointCloudTopicName>\n      <cameraInfoTopicName>/rgb/camera_info</cameraInfoTopicName>\n      <depthImageCameraInfoTopicName>/depth/camera_info</depthImageCameraInfoTopicName>\n      <frameName>rgbd_camera_optical_frame</frameName>\n      <pointCloudCutoff>0.1</pointCloudCutoff>\n      <pointCloudCutoffMax>10.0</pointCloudCutoffMax>\n      <distortion_k1>0.0</distortion_k1>\n      <distortion_k2>0.0</distortion_k2>\n      <distortion_k3>0.0</distortion_k3>\n      <distortion_t1>0.0</distortion_t1>\n      <distortion_t2>0.0</distortion_t2>\n      <CxPrime>0.0</CxPrime>\n      <Cx>0.0</Cx>\n      <Cy>0.0</Cy>\n      <focalLength>0.0</focalLength>\n      <hackBaseline>0.0</hackBaseline>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,a.jsx)(n.h2,{id:"imu-simulation",children:"IMU Simulation"}),"\n",(0,a.jsx)(n.h3,{id:"imu-sensor-configuration",children:"IMU Sensor Configuration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'<gazebo reference="imu_link">\n  <sensor name="imu_sensor" type="imu">\n    <always_on>true</always_on>\n    <update_rate>100</update_rate>\n    <visualize>false</visualize>\n    <imu>\n      <angular_velocity>\n        <x>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.0017</stddev>  \x3c!-- ~0.1 deg/s (1-sigma) --\x3e\n            <bias_mean>0.005</bias_mean>\n            <bias_stddev>0.0005</bias_stddev>\n          </noise>\n        </x>\n        <y>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.0017</stddev>\n            <bias_mean>0.005</bias_mean>\n            <bias_stddev>0.0005</bias_stddev>\n          </noise>\n        </y>\n        <z>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.0017</stddev>\n            <bias_mean>0.005</bias_mean>\n            <bias_stddev>0.0005</bias_stddev>\n          </noise>\n        </z>\n      </angular_velocity>\n      <linear_acceleration>\n        <x>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>1.7e-2</stddev>  \x3c!-- 1-sigma accel noise: 17 mg */\n            <bias_mean>0.1</bias_mean>\n            <bias_stddev>0.001</bias_stddev>\n          </noise>\n        </x>\n        <y>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>1.7e-2</stddev>\n            <bias_mean>0.1</bias_mean>\n            <bias_stddev>0.001</bias_stddev>\n          </noise>\n        </y>\n        <z>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>1.7e-2</stddev>\n            <bias_mean>0.1</bias_mean>\n            <bias_stddev>0.001</bias_stddev>\n          </noise>\n        </z>\n      </linear_acceleration>\n    </imu>\n    <plugin name="imu_plugin" filename="libgazebo_ros_imu.so">\n      <ros>\n        <namespace>/imu</namespace>\n        <remapping>~/out:=data</remapping>\n      </ros>\n      <frame_name>imu_link</frame_name>\n      <body_name>imu_link</body_name>\n      <update_rate>100</update_rate>\n      <gaussian_noise>0.0017</gaussian_noise>\n      <accel_gaussian_noise>0.017</accel_gaussian_noise>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,a.jsx)(n.h2,{id:"working-with-sensor-data-in-ros-2",children:"Working with Sensor Data in ROS 2"}),"\n",(0,a.jsx)(n.h3,{id:"lidar-data-processing",children:"LiDAR Data Processing"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan\nimport numpy as np\n\nclass LIDARProcessor(Node):\n    def __init__(self):\n        super().__init__('lidar_processor')\n\n        self.subscription = self.create_subscription(\n            LaserScan,\n            '/laser/scan',\n            self.lidar_callback,\n            10\n        )\n\n        self.get_logger().info('LiDAR Processor initialized')\n\n    def lidar_callback(self, msg):\n        # Convert to numpy array for processing\n        ranges = np.array(msg.ranges)\n\n        # Filter out invalid ranges (inf, nan)\n        valid_ranges = ranges[np.isfinite(ranges)]\n\n        # Find minimum distance (obstacle detection)\n        if len(valid_ranges) > 0:\n            min_distance = np.min(valid_ranges)\n            min_idx = np.argmin(ranges)\n            angle_of_min = msg.angle_min + min_idx * msg.angle_increment\n\n            self.get_logger().info(\n                f'Min distance: {min_distance:.2f}m at angle: {angle_of_min:.2f}rad'\n            )\n\n        # Calculate statistics\n        if len(valid_ranges) > 0:\n            avg_distance = np.mean(valid_ranges)\n            self.get_logger().info(f'Average distance: {avg_distance:.2f}m')\n"})}),"\n",(0,a.jsx)(n.h3,{id:"depth-camera-data-processing",children:"Depth Camera Data Processing"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\n\nclass DepthCameraProcessor(Node):\n    def __init__(self):\n        super().__init__('depth_camera_processor')\n\n        self.bridge = CvBridge()\n\n        # Subscribe to depth image\n        self.depth_sub = self.create_subscription(\n            Image,\n            '/camera/depth/image_raw',\n            self.depth_callback,\n            10\n        )\n\n        # Subscribe to camera info for intrinsics\n        self.info_sub = self.create_subscription(\n            CameraInfo,\n            '/camera/depth/camera_info',\n            self.info_callback,\n            10\n        )\n\n        self.camera_info = None\n        self.get_logger().info('Depth Camera Processor initialized')\n\n    def info_callback(self, msg):\n        self.camera_info = msg\n\n    def depth_callback(self, msg):\n        try:\n            # Convert ROS Image message to OpenCV image\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='passthrough')\n\n            # Process depth data\n            # Find distance to center pixel\n            height, width = cv_image.shape\n            center_x, center_y = width // 2, height // 2\n            center_depth = cv_image[center_y, center_x]\n\n            if np.isfinite(center_depth):\n                self.get_logger().info(f'Depth at center: {center_depth:.2f}m')\n\n            # Find valid depth pixels (not inf or nan)\n            valid_depths = cv_image[np.isfinite(cv_image)]\n            if len(valid_depths) > 0:\n                avg_depth = np.mean(valid_depths)\n                self.get_logger().info(f'Average depth: {avg_depth:.2f}m')\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing depth image: {e}')\n"})}),"\n",(0,a.jsx)(n.h3,{id:"imu-data-processing",children:"IMU Data Processing"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Imu\nfrom geometry_msgs.msg import Vector3\nimport math\n\nclass IMUProcessor(Node):\n    def __init__(self):\n        super().__init__('imu_processor')\n\n        self.subscription = self.create_subscription(\n            Imu,\n            '/imu/data',\n            self.imu_callback,\n            10\n        )\n\n        self.get_logger().info('IMU Processor initialized')\n\n    def imu_callback(self, msg):\n        # Extract orientation (quaternion)\n        orientation = msg.orientation\n        # Convert quaternion to Euler angles\n        euler = self.quaternion_to_euler(orientation)\n\n        # Extract angular velocity\n        angular_vel = msg.angular_velocity\n\n        # Extract linear acceleration\n        linear_acc = msg.linear_acceleration\n\n        self.get_logger().info(\n            f'Roll: {euler.x:.2f}, Pitch: {euler.y:.2f}, Yaw: {euler.z:.2f}'\n        )\n        self.get_logger().info(\n            f'Angular Vel - X: {angular_vel.x:.2f}, Y: {angular_vel.y:.2f}, Z: {angular_vel.z:.2f}'\n        )\n        self.get_logger().info(\n            f'Linear Acc - X: {linear_acc.x:.2f}, Y: {linear_acc.y:.2f}, Z: {linear_acc.z:.2f}'\n        )\n\n    def quaternion_to_euler(self, quaternion):\n        \"\"\"Convert quaternion to Euler angles (roll, pitch, yaw).\"\"\"\n        q = quaternion\n        sinr_cosp = 2 * (q.w * q.x + q.y * q.z)\n        cosr_cosp = 1 - 2 * (q.x * q.x + q.y * q.y)\n        roll = math.atan2(sinr_cosp, cosr_cosp)\n\n        sinp = 2 * (q.w * q.y - q.z * q.x)\n        pitch = math.asin(sinp)\n\n        siny_cosp = 2 * (q.w * q.z + q.x * q.y)\n        cosy_cosp = 1 - 2 * (q.y * q.y + q.z * q.z)\n        yaw = math.atan2(siny_cosp, cosy_cosp)\n\n        return Vector3(x=roll, y=pitch, z=yaw)\n"})}),"\n",(0,a.jsx)(n.h2,{id:"sensor-fusion-example",children:"Sensor Fusion Example"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan, Imu\nfrom nav_msgs.msg import Odometry\nfrom geometry_msgs.msg import PoseWithCovarianceStamped\nimport numpy as np\n\nclass SensorFusionNode(Node):\n    def __init__(self):\n        super().__init__('sensor_fusion')\n\n        # Subscribers for different sensors\n        self.lidar_sub = self.create_subscription(\n            LaserScan, '/laser/scan', self.lidar_callback, 10\n        )\n        self.imu_sub = self.create_subscription(\n            Imu, '/imu/data', self.imu_callback, 10\n        )\n\n        # Publisher for fused pose\n        self.pose_pub = self.create_publisher(\n            PoseWithCovarianceStamped, '/fused_pose', 10\n        )\n\n        # Initialize state variables\n        self.orientation = [0.0, 0.0, 0.0, 1.0]  # x, y, z, w\n        self.position = [0.0, 0.0, 0.0]\n\n        self.get_logger().info('Sensor Fusion Node initialized')\n\n    def lidar_callback(self, msg):\n        # Process LiDAR data for position estimation\n        # This is a simplified example - real fusion would be more complex\n        pass\n\n    def imu_callback(self, msg):\n        # Update orientation from IMU\n        self.orientation = [\n            msg.orientation.x,\n            msg.orientation.y,\n            msg.orientation.z,\n            msg.orientation.w\n        ]\n\n    def publish_fused_pose(self):\n        # Publish the fused pose estimate\n        pose_msg = PoseWithCovarianceStamped()\n        pose_msg.header.stamp = self.get_clock().now().to_msg()\n        pose_msg.header.frame_id = 'map'\n\n        # Set position and orientation\n        pose_msg.pose.pose.position.x = self.position[0]\n        pose_msg.pose.pose.position.y = self.position[1]\n        pose_msg.pose.pose.position.z = self.position[2]\n\n        pose_msg.pose.pose.orientation.x = self.orientation[0]\n        pose_msg.pose.pose.orientation.y = self.orientation[1]\n        pose_msg.pose.pose.orientation.z = self.orientation[2]\n        pose_msg.pose.pose.orientation.w = self.orientation[3]\n\n        # Set covariance (simplified)\n        pose_msg.pose.covariance = [0.1] * 36  # Diagonal elements = 0.1\n\n        self.pose_pub.publish(pose_msg)\n"})}),"\n",(0,a.jsx)(n.h2,{id:"best-practices-for-sensor-simulation",children:"Best Practices for Sensor Simulation"}),"\n",(0,a.jsx)(n.h3,{id:"lidar-best-practices",children:"LiDAR Best Practices"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Resolution"}),": Balance between detail and performance"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Range"}),": Set appropriate min/max values for your application"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Update rate"}),": Match the rate to your algorithm requirements"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Noise"}),": Include realistic noise models for robust algorithms"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Field of view"}),": Match the physical sensor specifications"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"camera-best-practices",children:"Camera Best Practices"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Intrinsics"}),": Use accurate camera calibration parameters"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Distortion"}),": Include distortion coefficients if present in real sensor"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Frame rate"}),": Match the physical camera capabilities"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Resolution"}),": Use appropriate resolution for your algorithms"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Noise"}),": Add realistic noise models for robust processing"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"imu-best-practices",children:"IMU Best Practices"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Bias"}),": Include realistic bias values"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Noise"}),": Use appropriate noise models based on sensor specifications"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Update rate"}),": Higher rates for dynamic applications"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Alignment"}),": Ensure proper frame alignment with robot coordinate system"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Calibration"}),": Simulate calibration procedures in your algorithms"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"general-best-practices",children:"General Best Practices"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Validate with real data"}),": Compare simulation output with real sensor data"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Document parameters"}),": Keep detailed records of sensor configurations"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Test edge cases"}),": Include scenarios with sensor limitations"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Performance monitoring"}),": Monitor simulation performance with multiple sensors"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Cross-validation"}),": Test algorithms with different sensor combinations"]}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>t});var s=i(6540);const a={},r=s.createContext(a);function o(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);